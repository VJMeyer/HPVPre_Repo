{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VJMeyer/HPVPre_Repo/blob/main/PRUT_Transcriber6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81lArfxtubjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_6uANjjubWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 ULTIMATE - MAXIMIZING GPU USAGE FOR QUALITY\n",
        "# Targets 14GB GPU RAM usage with quality-focused optimizations\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION - QUALITY MAXIMIZED\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "WHISPER_MODEL = \"large-v3\"\n",
        "\n",
        "# Enhanced parameters for maximum quality\n",
        "TRANSCRIPTION_PARAMS = {\n",
        "    # Quality settings\n",
        "    'best_of': 5,  # Generate 5 candidates and pick the best (uses more memory)\n",
        "    'beam_size': 10,  # Beam search instead of greedy (much better quality, more memory)\n",
        "    'patience': 2.0,  # Beam search patience\n",
        "    'length_penalty': 1.0,  # Favor longer sequences\n",
        "\n",
        "    # Temperature strategy\n",
        "    'temperature': 0.0,  # Start with 0 for consistency\n",
        "    'temperature_increment_on_fallback': 0.2,  # Only increase if needed\n",
        "\n",
        "    # Thresholds\n",
        "    'compression_ratio_threshold': 2.4,\n",
        "    'logprob_threshold': -1.0,\n",
        "    'no_speech_threshold': 0.6,\n",
        "\n",
        "    # Context handling - SMART approach\n",
        "    'condition_on_previous_text': True,  # Keep context by default\n",
        "    'initial_prompt': \"This is a professional interview or call recording with clear speech.\",\n",
        "\n",
        "    # Advanced options\n",
        "    'word_timestamps': True,\n",
        "    'prepend_punctuations': '\"\\'\"¬ø([{-',\n",
        "    'append_punctuations': '\"\\'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ',\n",
        "\n",
        "    # Longer audio segments for better context\n",
        "    'chunk_length': 60,  # Process 60-second chunks instead of default 30\n",
        "}\n",
        "\n",
        "# Memory-intensive options for 14GB target\n",
        "ADVANCED_OPTIONS = {\n",
        "    'n_mel': 128,  # Increase mel bands (default 80)\n",
        "    'sample_len': 1500,  # Longer sample length\n",
        "    'best_of_temperatures': [0.0, 0.1, 0.2, 0.4, 0.8],  # More temperature attempts\n",
        "}\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Whisper V3 Ultimate - Quality Maximized Edition\")\n",
        "print(\"   Target GPU usage: 14GB for maximum accuracy\")\n",
        "print(\"   Features: Beam search, extended context, smart hallucination prevention\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES (SIMPLIFIED)\n",
        "# ============================================\n",
        "print(\"\\nüì¶ Checking dependencies...\")\n",
        "\n",
        "try:\n",
        "    import whisper\n",
        "    import torch\n",
        "    print(\"‚úì Core packages ready\")\n",
        "except ImportError:\n",
        "    print(\"Installing Whisper...\")\n",
        "    subprocess.run(['pip', 'install', '--quiet', 'openai-whisper'], check=True)\n",
        "    import whisper\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# GPU CONFIGURATION\n",
        "# ============================================\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    gpu_props = torch.cuda.get_device_properties(0)\n",
        "    total_memory_gb = gpu_props.total_memory / 1e9\n",
        "\n",
        "    print(f\"\\nüéÆ GPU Configuration:\")\n",
        "    print(f\"   Device: {gpu_props.name}\")\n",
        "    print(f\"   Total Memory: {total_memory_gb:.1f} GB\")\n",
        "\n",
        "    # Set PyTorch to use more memory\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "    print(f\"   Allocated for processing: {total_memory_gb * 0.95:.1f} GB\")\n",
        "\n",
        "    # Enable TF32 for better performance on Ampere GPUs\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    print(\"   TF32 acceleration: Enabled\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"‚ö†Ô∏è  No GPU detected - this will be extremely slow\")\n",
        "\n",
        "# ============================================\n",
        "# ENHANCED TRANSCRIPTION FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def load_audio_extended(file_path, sr=16000):\n",
        "    \"\"\"Load audio with extended precision\"\"\"\n",
        "    import librosa\n",
        "    audio, _ = librosa.load(file_path, sr=sr, mono=True, dtype=np.float32)\n",
        "    return audio\n",
        "\n",
        "def smart_transcribe(model, audio_path, initial_params):\n",
        "    \"\"\"\n",
        "    Intelligent transcription with adaptive parameters\n",
        "    \"\"\"\n",
        "    print(\"   üß† Smart transcription mode engaged...\")\n",
        "\n",
        "    # First attempt with context\n",
        "    params = initial_params.copy()\n",
        "    attempt = 1\n",
        "    max_attempts = 3\n",
        "\n",
        "    while attempt <= max_attempts:\n",
        "        try:\n",
        "            print(f\"   Attempt {attempt}/3 (context={'on' if params['condition_on_previous_text'] else 'off'})...\", end='', flush=True)\n",
        "\n",
        "            # Load audio in high quality\n",
        "            audio = load_audio_extended(audio_path)\n",
        "\n",
        "            # Transcribe with current parameters\n",
        "            result = model.transcribe(\n",
        "                audio,\n",
        "                language=None,  # Auto-detect\n",
        "                task='transcribe',\n",
        "                verbose=False,\n",
        "                fp16=(device == \"cuda\"),\n",
        "                **params\n",
        "            )\n",
        "\n",
        "            # Check for hallucinations\n",
        "            segments = result.get('segments', [])\n",
        "            hallucination_score = calculate_hallucination_score(segments)\n",
        "\n",
        "            if hallucination_score < 0.1:  # Good transcription\n",
        "                print(\" ‚úì Success (quality score: {:.2f})\".format(1 - hallucination_score))\n",
        "                return result\n",
        "            else:\n",
        "                print(f\" ‚ö†Ô∏è  Hallucination detected (score: {hallucination_score:.2f})\")\n",
        "\n",
        "                # Adaptive strategy\n",
        "                if attempt == 1:\n",
        "                    # Try without context\n",
        "                    params['condition_on_previous_text'] = False\n",
        "                    params['initial_prompt'] = None\n",
        "                elif attempt == 2:\n",
        "                    # Increase temperature and reduce beam search\n",
        "                    params['temperature'] = 0.2\n",
        "                    params['beam_size'] = 5\n",
        "                    params['best_of'] = 3\n",
        "\n",
        "                attempt += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" ‚ùå Error: {e}\")\n",
        "            attempt += 1\n",
        "\n",
        "    # Return best effort\n",
        "    return result\n",
        "\n",
        "def calculate_hallucination_score(segments):\n",
        "    \"\"\"\n",
        "    Calculate a score from 0-1 indicating likelihood of hallucination\n",
        "    \"\"\"\n",
        "    if len(segments) < 5:\n",
        "        return 0.0\n",
        "\n",
        "    # Check for repetitions\n",
        "    texts = [seg.get('text', '').strip().lower() for seg in segments]\n",
        "\n",
        "    repetition_count = 0\n",
        "    for i in range(len(texts) - 3):\n",
        "        # Check for exact repetitions in sliding window\n",
        "        window = texts[i:i+4]\n",
        "        if len(set(window)) == 1 and window[0]:\n",
        "            repetition_count += 1\n",
        "\n",
        "    # Check for suspiciously regular timestamps\n",
        "    timestamp_regularity = 0\n",
        "    for i in range(len(segments) - 1):\n",
        "        duration = segments[i+1]['start'] - segments[i]['end']\n",
        "        if duration < 0.1:  # Segments too close together\n",
        "            timestamp_regularity += 1\n",
        "\n",
        "    # Combined score\n",
        "    repetition_score = repetition_count / max(1, len(segments) - 3)\n",
        "    timestamp_score = timestamp_regularity / max(1, len(segments) - 1)\n",
        "\n",
        "    return min(1.0, repetition_score + timestamp_score)\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Format seconds to MM:SS.ss\"\"\"\n",
        "    minutes = int(seconds // 60)\n",
        "    secs = seconds % 60\n",
        "    return f\"{minutes:02d}:{secs:05.2f}\"\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING PIPELINE\n",
        "# ============================================\n",
        "\n",
        "# Find WAV files\n",
        "wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "remaining_files = []\n",
        "\n",
        "print(f\"\\nüìä Scanning for files...\")\n",
        "for wav_file in wav_files:\n",
        "    base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "\n",
        "    # Check multiple possible output names\n",
        "    transcript_exists = any(\n",
        "        os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "        for suffix in ['_ultimate.txt', '_enhanced.txt', '_large-v3.txt']\n",
        "    )\n",
        "\n",
        "    if not transcript_exists:\n",
        "        remaining_files.append(wav_file)\n",
        "        size_mb = os.path.getsize(wav_file) / (1024**2)\n",
        "        print(f\"   ‚è≥ {os.path.basename(wav_file)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "print(f\"\\nFiles to process: {len(remaining_files)} of {len(wav_files)}\")\n",
        "\n",
        "if remaining_files:\n",
        "    # Process 2 files per session with large-v3\n",
        "    files_to_process = remaining_files[:2]\n",
        "\n",
        "    # Load model with maximum quality settings\n",
        "    print(f\"\\n‚è≥ Loading {WHISPER_MODEL} for maximum quality...\")\n",
        "    print(\"   This optimizes for accuracy over speed\")\n",
        "\n",
        "    start_load = time.time()\n",
        "    model = whisper.load_model(WHISPER_MODEL, device=device)\n",
        "    load_time = time.time() - start_load\n",
        "\n",
        "    # Check memory usage after model load\n",
        "    if torch.cuda.is_available():\n",
        "        allocated_gb = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved_gb = torch.cuda.memory_reserved() / 1e9\n",
        "        print(f\"\\n‚úì Model loaded in {load_time:.1f}s\")\n",
        "        print(f\"   GPU Memory: {allocated_gb:.1f}GB used, {reserved_gb:.1f}GB reserved\")\n",
        "        print(f\"   Free for processing: {total_memory_gb - reserved_gb:.1f}GB\")\n",
        "\n",
        "    # Process files\n",
        "    for idx, audio_file in enumerate(files_to_process):\n",
        "        base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
        "        output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_ultimate.txt\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"[{idx+1}/{len(files_to_process)}] {os.path.basename(audio_file)}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            file_size_mb = os.path.getsize(audio_file) / (1024**2)\n",
        "\n",
        "            # Estimate duration from file size (rough approximation)\n",
        "            estimated_duration = file_size_mb * 12  # ~12 seconds per MB for WAV\n",
        "\n",
        "            print(f\"   File size: {file_size_mb:.1f} MB\")\n",
        "            print(f\"   Estimated duration: {format_time(estimated_duration)}\")\n",
        "            print(f\"   Using: Beam search (size={TRANSCRIPTION_PARAMS['beam_size']}), \"\n",
        "                  f\"Best of {TRANSCRIPTION_PARAMS['best_of']}\")\n",
        "\n",
        "            # Perform smart transcription\n",
        "            result = smart_transcribe(model, audio_file, TRANSCRIPTION_PARAMS)\n",
        "\n",
        "            # Extract results\n",
        "            segments = result.get('segments', [])\n",
        "            detected_language = result.get('language', 'unknown')\n",
        "            actual_duration = segments[-1]['end'] if segments else 0\n",
        "            process_time = time.time() - start_time\n",
        "\n",
        "            # Calculate statistics\n",
        "            speed_factor = actual_duration / process_time if process_time > 0 else 0\n",
        "            words_count = sum(len(seg.get('text', '').split()) for seg in segments)\n",
        "\n",
        "            print(f\"\\n‚úÖ Transcription complete!\")\n",
        "            print(f\"   Language: {detected_language}\")\n",
        "            print(f\"   Duration: {format_time(actual_duration)}\")\n",
        "            print(f\"   Process time: {format_time(process_time)}\")\n",
        "            print(f\"   Speed: {speed_factor:.1f}x realtime\")\n",
        "            print(f\"   Total words: {words_count:,}\")\n",
        "\n",
        "            # Memory check\n",
        "            if torch.cuda.is_available():\n",
        "                peak_memory_gb = torch.cuda.max_memory_allocated() / 1e9\n",
        "                print(f\"   Peak GPU memory: {peak_memory_gb:.1f}GB\")\n",
        "\n",
        "            # Save high-quality transcript\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                # Header with metadata\n",
        "                f.write(f\"# Whisper V3 Ultimate Transcription\\n\")\n",
        "                f.write(f\"# Model: {WHISPER_MODEL}\\n\")\n",
        "                f.write(f\"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "                f.write(f\"# File: {os.path.basename(audio_file)}\\n\")\n",
        "                f.write(f\"# Duration: {format_time(actual_duration)}\\n\")\n",
        "                f.write(f\"# Language: {detected_language}\\n\")\n",
        "                f.write(f\"# Words: {words_count:,}\\n\")\n",
        "                f.write(f\"# Processing: Beam search (size={TRANSCRIPTION_PARAMS['beam_size']}), \"\n",
        "                       f\"Best of {TRANSCRIPTION_PARAMS['best_of']}\\n\")\n",
        "                f.write(\"#\" + \"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "                # Write segments with improved formatting\n",
        "                for i, segment in enumerate(segments):\n",
        "                    start = segment['start']\n",
        "                    end = segment['end']\n",
        "                    text = segment.get('text', '').strip()\n",
        "\n",
        "                    if text:  # Skip empty segments\n",
        "                        # Format: [MM:SS.ss ‚Üí MM:SS.ss] Text\n",
        "                        f.write(f\"[{format_time(start)} ‚Üí {format_time(end)}] {text}\\n\")\n",
        "\n",
        "                        # Add paragraph breaks for long pauses\n",
        "                        if i < len(segments) - 1:\n",
        "                            next_start = segments[i + 1]['start']\n",
        "                            pause_duration = next_start - end\n",
        "                            if pause_duration > 3.0:  # 3+ second pause\n",
        "                                f.write(\"\\n\")\n",
        "\n",
        "            print(f\"\\n‚úì Saved: {os.path.basename(output_file)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Save error log\n",
        "            with open(f\"{output_file}.error\", 'w') as f:\n",
        "                f.write(f\"Error processing: {audio_file}\\n\")\n",
        "                f.write(f\"Error: {str(e)}\\n\")\n",
        "                f.write(f\"Traceback:\\n{traceback.format_exc()}\\n\")\n",
        "\n",
        "        # Cleanup after each file\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Show memory stats\n",
        "        if torch.cuda.is_available():\n",
        "            current_memory_gb = torch.cuda.memory_allocated() / 1e9\n",
        "            print(f\"\\nüíæ Memory after cleanup: {current_memory_gb:.1f}GB\")\n",
        "\n",
        "        # Cool down between files\n",
        "        if idx < len(files_to_process) - 1:\n",
        "            print(\"\\n‚è≥ Cooling down for 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "    # Final cleanup\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print(f\"   Processed: {len(files_to_process)} files\")\n",
        "    print(f\"   Remaining: {len(remaining_files) - len(files_to_process)} files\")\n",
        "    print(f\"   Output format: *_ultimate.txt\")\n",
        "    print(\"\\nüí° Quality optimizations used:\")\n",
        "    print(\"   - Beam search for better accuracy\")\n",
        "    print(\"   - Best-of-N sampling\")\n",
        "    print(\"   - Extended context windows\")\n",
        "    print(\"   - Smart hallucination prevention\")\n",
        "    print(\"   - 95% GPU memory utilization\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚úÖ All files have been transcribed!\")\n",
        "    print(f\"üìÅ Transcripts location: {OUTPUT_PATH}\")\n",
        "\n",
        "# ============================================\n",
        "# OPTIMIZATION TIPS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° FURTHER OPTIMIZATION OPTIONS\")\n",
        "print(\"=\"*70)\n",
        "print(\"To push quality even higher, you can manually adjust:\")\n",
        "print(\"\")\n",
        "print(\"1. Increase beam_size to 20 (slower but more accurate):\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['beam_size'] = 20\")\n",
        "print(\"\")\n",
        "print(\"2. Increase best_of to 10 (much slower):\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['best_of'] = 10\")\n",
        "print(\"\")\n",
        "print(\"3. For interviews with technical terms, add a prompt:\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['initial_prompt'] = 'Technical interview about...'\")\n",
        "print(\"\")\n",
        "print(\"4. For very long files, increase chunk_length:\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['chunk_length'] = 120  # 2-minute chunks\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "rDlbJ4p-rFJa",
        "outputId": "bcdb31a9-f77f-4a78-ea9c-11d2e6c11868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üöÄ Whisper V3 Ultimate - Quality Maximized Edition\n",
            "   Target GPU usage: 14GB for maximum accuracy\n",
            "   Features: Beam search, extended context, smart hallucination prevention\n",
            "============================================================\n",
            "\n",
            "üì¶ Checking dependencies...\n",
            "Installing Whisper...\n",
            "\n",
            "üéÆ GPU Configuration:\n",
            "   Device: Tesla T4\n",
            "   Total Memory: 15.8 GB\n",
            "   Allocated for processing: 15.0 GB\n",
            "   TF32 acceleration: Enabled\n",
            "\n",
            "üìä Scanning for files...\n",
            "   ‚è≥ Call Recording - 19Mar2025 0800 JD.wav (125.2 MB)\n",
            "   ‚è≥ Call Recording - 19Mar25 0900 - AJ.wav (77.2 MB)\n",
            "   ‚è≥ Call Recording - 19Mar25 1730 - MO.wav (87.2 MB)\n",
            "   ‚è≥ Call Recording - 20Mar2025 1200 LN.wav (179.8 MB)\n",
            "   ‚è≥ Call Recording - 26Mar2025 0830 SA.wav (94.2 MB)\n",
            "\n",
            "Files to process: 5 of 8\n",
            "\n",
            "‚è≥ Loading large-v3 for maximum quality...\n",
            "   This optimizes for accuracy over speed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [02:12<00:00, 23.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Model loaded in 164.4s\n",
            "   GPU Memory: 6.3GB used, 9.8GB reserved\n",
            "   Free for processing: 6.0GB\n",
            "\n",
            "======================================================================\n",
            "[1/2] Call Recording - 19Mar2025 0800 JD.wav\n",
            "======================================================================\n",
            "   File size: 125.2 MB\n",
            "   Estimated duration: 25:02.52\n",
            "   Using: Beam search (size=10), Best of 5\n",
            "   üß† Smart transcription mode engaged...\n",
            "   Attempt 1/3 (context=on)...Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/148857 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 2/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/148857 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 3/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/148857 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "\n",
            "‚ùå Failed: cannot access local variable 'result' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 278, in <cell line: 0>\n",
            "    result = smart_transcribe(model, audio_file, TRANSCRIPTION_PARAMS)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 173, in smart_transcribe\n",
            "    return result\n",
            "           ^^^^^^\n",
            "UnboundLocalError: cannot access local variable 'result' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Memory after cleanup: 6.3GB\n",
            "\n",
            "‚è≥ Cooling down for 10 seconds...\n",
            "\n",
            "======================================================================\n",
            "[2/2] Call Recording - 19Mar25 0900 - AJ.wav\n",
            "======================================================================\n",
            "   File size: 77.2 MB\n",
            "   Estimated duration: 15:25.97\n",
            "   Using: Beam search (size=10), Best of 5\n",
            "   üß† Smart transcription mode engaged...\n",
            "   Attempt 1/3 (context=on)...Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/91737 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 2/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/91737 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 3/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/91737 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "\n",
            "‚ùå Failed: cannot access local variable 'result' where it is not associated with a value\n",
            "\n",
            "üíæ Memory after cleanup: 6.3GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 278, in <cell line: 0>\n",
            "    result = smart_transcribe(model, audio_file, TRANSCRIPTION_PARAMS)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 173, in smart_transcribe\n",
            "    return result\n",
            "           ^^^^^^\n",
            "UnboundLocalError: cannot access local variable 'result' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "‚úÖ SESSION COMPLETE\n",
            "   Processed: 2 files\n",
            "   Remaining: 3 files\n",
            "   Output format: *_ultimate.txt\n",
            "\n",
            "üí° Quality optimizations used:\n",
            "   - Beam search for better accuracy\n",
            "   - Best-of-N sampling\n",
            "   - Extended context windows\n",
            "   - Smart hallucination prevention\n",
            "   - 95% GPU memory utilization\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üí° FURTHER OPTIMIZATION OPTIONS\n",
            "======================================================================\n",
            "To push quality even higher, you can manually adjust:\n",
            "\n",
            "1. Increase beam_size to 20 (slower but more accurate):\n",
            "   TRANSCRIPTION_PARAMS['beam_size'] = 20\n",
            "\n",
            "2. Increase best_of to 10 (much slower):\n",
            "   TRANSCRIPTION_PARAMS['best_of'] = 10\n",
            "\n",
            "3. For interviews with technical terms, add a prompt:\n",
            "   TRANSCRIPTION_PARAMS['initial_prompt'] = 'Technical interview about...'\n",
            "\n",
            "4. For very long files, increase chunk_length:\n",
            "   TRANSCRIPTION_PARAMS['chunk_length'] = 120  # 2-minute chunks\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YlO8LjFzrFFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 SIMPLE & STABLE - NO MEMORY ISSUES\n",
        "# Clean implementation without parallel processing or aggressive allocation\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Checking dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION - SIMPLE & STABLE\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Simple transcription parameters\n",
        "TRANSCRIPTION_PARAMS = {\n",
        "    'temperature': 0.2,\n",
        "    'compression_ratio_threshold': 2.4,\n",
        "    'logprob_threshold': -1.0,\n",
        "    'no_speech_threshold': 0.6,\n",
        "    'condition_on_previous_text': True,\n",
        "    'word_timestamps': True,\n",
        "    'prepend_punctuations': '\"\\'\"¬ø([{-',\n",
        "    'append_punctuations': '\"\\'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ',\n",
        "    'beam_size': 5,  # Standard beam size\n",
        "    'best_of': 5,    # Standard best_of\n",
        "    'fp16': False,    # Use FP16 for efficiency\n",
        "}\n",
        "\n",
        "print(\"\\nüöÄ Whisper V3 Simple Transcription System\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MONITORING\n",
        "# ============================================\n",
        "def print_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "    else:\n",
        "        print(\"No GPU available\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL (SIMPLE)\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Clear any existing memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Load model normally\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "print(\"‚úì Model loaded successfully\")\n",
        "print_gpu_memory()\n",
        "\n",
        "# ============================================\n",
        "# POST-PROCESSING UTILITIES\n",
        "# ============================================\n",
        "def clean_text(text):\n",
        "    \"\"\"Basic text cleanup\"\"\"\n",
        "    # Fix common acronym patterns\n",
        "    text = text.replace(' h m h c ', ' HMHC ')\n",
        "    text = text.replace(' h n h m ', ' HMHC ')\n",
        "    text = text.replace(' c a ', ' CA ')\n",
        "    text = text.replace(' c o m ', '.com ')\n",
        "    text = text.replace(' . ', '.')\n",
        "\n",
        "    # Fix spacing around punctuation\n",
        "    text = text.replace(' ,', ',')\n",
        "    text = text.replace(' .', '.')\n",
        "    text = text.replace(' ?', '?')\n",
        "    text = text.replace(' !', '!')\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def is_quality_issue(segments):\n",
        "    \"\"\"Check for repetition or quality issues\"\"\"\n",
        "    if len(segments) < 10:\n",
        "        return False\n",
        "\n",
        "    # Check for excessive repetition\n",
        "    texts = [seg.get('text', '').strip().lower() for seg in segments[-10:]]\n",
        "    unique_texts = set(texts)\n",
        "\n",
        "    # If last 10 segments have less than 3 unique texts, there's likely repetition\n",
        "    return len(unique_texts) < 3\n",
        "\n",
        "# ============================================\n",
        "# MAIN TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file(audio_path):\n",
        "    \"\"\"Transcribe a single file with quality checks\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_transcript.txt\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {os.path.basename(audio_path)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        file_size_mb = os.path.getsize(audio_path) / (1024**2)\n",
        "        print(f\"File size: {file_size_mb:.1f} MB\")\n",
        "\n",
        "        # Transcribe with standard parameters\n",
        "        print(\"Transcribing...\")\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            language=None,  # Auto-detect language\n",
        "            task='transcribe',\n",
        "            verbose=False,\n",
        "            **TRANSCRIPTION_PARAMS\n",
        "        )\n",
        "\n",
        "        # Check for quality issues\n",
        "        segments = result.get('segments', [])\n",
        "        if is_quality_issue(segments):\n",
        "            print(\"‚ö†Ô∏è  Quality issue detected, retrying with adjusted parameters...\")\n",
        "\n",
        "            # Retry with different parameters\n",
        "            retry_params = TRANSCRIPTION_PARAMS.copy()\n",
        "            retry_params['temperature'] = 0.8\n",
        "            retry_params['condition_on_previous_text'] = False\n",
        "\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                task='transcribe',\n",
        "                verbose=False,\n",
        "                **retry_params\n",
        "            )\n",
        "            segments = result.get('segments', [])\n",
        "\n",
        "        # Process results\n",
        "        duration = segments[-1]['end'] if segments else 0\n",
        "        process_time = time.time() - start_time\n",
        "        speed_factor = duration / process_time if process_time > 0 else 0\n",
        "\n",
        "        print(f\"\\n‚úÖ Transcription complete!\")\n",
        "        print(f\"   Language: {result.get('language', 'unknown')}\")\n",
        "        print(f\"   Duration: {duration/60:.1f} minutes\")\n",
        "        print(f\"   Process time: {process_time:.1f} seconds\")\n",
        "        print(f\"   Speed: {speed_factor:.1f}x realtime\")\n",
        "        print_gpu_memory()\n",
        "\n",
        "        # Save transcript\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            # Header\n",
        "            f.write(f\"# Whisper Large-v3 Transcript\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Duration: {duration/60:.1f} minutes\\n\")\n",
        "            f.write(\"#\" + \"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "            # Write segments\n",
        "            for segment in segments:\n",
        "                start = segment['start']\n",
        "                end = segment['end']\n",
        "                text = clean_text(segment.get('text', ''))\n",
        "\n",
        "                if text:  # Only write non-empty segments\n",
        "                    f.write(f\"[{start:06.2f} ‚Üí {end:06.2f}] {text}\\n\")\n",
        "\n",
        "        print(f\"üíæ Saved: {os.path.basename(output_file)}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Clear GPU cache after each file\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining_files = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "\n",
        "        # Check if already processed\n",
        "        transcript_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_transcript.txt', '_final.txt', '_enhanced.txt', '_gpu_optimized.txt']\n",
        "        )\n",
        "\n",
        "        if not transcript_exists:\n",
        "            remaining_files.append(wav_file)\n",
        "\n",
        "    print(f\"\\nüìä Status:\")\n",
        "    print(f\"   Total files: {len(wav_files)}\")\n",
        "    print(f\"   Already processed: {len(wav_files) - len(remaining_files)}\")\n",
        "    print(f\"   To process: {len(remaining_files)}\")\n",
        "\n",
        "    if not remaining_files:\n",
        "        print(\"\\n‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files one by one (no parallel processing)\n",
        "    files_to_process = remaining_files[:5]  # Process up to 5 files per session\n",
        "    successful = 0\n",
        "\n",
        "    for idx, audio_file in enumerate(files_to_process):\n",
        "        if transcribe_file(audio_file):\n",
        "            successful += 1\n",
        "\n",
        "        # Cool down between files\n",
        "        if idx < len(files_to_process) - 1:\n",
        "            print(\"\\n‚è≥ Cooling down for 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print(f\"   Processed successfully: {successful}/{len(files_to_process)}\")\n",
        "    print(f\"   Remaining files: {len(remaining_files) - len(files_to_process)}\")\n",
        "\n",
        "    if len(remaining_files) > len(files_to_process):\n",
        "        print(\"\\nüí° Run again to process remaining files\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# NOTES\n",
        "# ============================================\n",
        "\"\"\"\n",
        "This simplified version:\n",
        "1. NO parallel processing - one file at a time\n",
        "2. NO aggressive memory allocation\n",
        "3. NO complex GPU optimization\n",
        "4. Just reliable, quality transcription\n",
        "\n",
        "GPU usage will be around 7-10GB, which is normal for large-v3.\n",
        "\n",
        "To increase speed slightly, you can adjust:\n",
        "- beam_size: Lower to 3 for faster processing\n",
        "- best_of: Lower to 3 for faster processing\n",
        "\n",
        "But the defaults (5/5) provide the best quality.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZapWLQM5lBhv",
        "outputId": "22188457-bb12-4970-fb7a-b79ccf4896ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Checking dependencies...\n",
            "‚úì Whisper already installed\n",
            "\n",
            "üöÄ Whisper V3 Simple Transcription System\n",
            "============================================================\n",
            "\n",
            "Loading Whisper large-v3...\n",
            "‚úì Model loaded successfully\n",
            "GPU Memory: 6.3GB allocated, 9.8GB reserved / 15.8GB total\n",
            "\n",
            "üìä Status:\n",
            "   Total files: 8\n",
            "   Already processed: 2\n",
            "   To process: 6\n",
            "\n",
            "============================================================\n",
            "Processing: Call Recording - 13Mar25 1300 HB.wav\n",
            "============================================================\n",
            "File size: 70.3 MB\n",
            "Transcribing...\n",
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83628/83628 [09:15<00:00, 150.50frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Transcription complete!\n",
            "   Language: en\n",
            "   Duration: 13.9 minutes\n",
            "   Process time: 560.5 seconds\n",
            "   Speed: 1.5x realtime\n",
            "GPU Memory: 6.3GB allocated, 10.8GB reserved / 15.8GB total\n",
            "üíæ Saved: Call Recording - 13Mar25 1300 HB_transcript.txt\n",
            "\n",
            "‚è≥ Cooling down for 5 seconds...\n",
            "\n",
            "============================================================\n",
            "Processing: Call Recording - 19Mar2025 0800 JD.wav\n",
            "============================================================\n",
            "File size: 125.2 MB\n",
            "Transcribing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3633120119.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;31m# ============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;31m# ============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-3633120119.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_to_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtranscribe_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0msuccessful\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-3633120119.py\u001b[0m in \u001b[0;36mtranscribe_file\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Transcribe with standard parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transcribing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         result = model.transcribe(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Auto-detect language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Pad 30-seconds of silence to the input audio, for slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mcontent_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mN_FRAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mcontent_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_frames\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHOP_LENGTH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gqqu21JYlBUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 GPU-OPTIMIZED - MAXIMUM MEMORY UTILIZATION\n",
        "# Aggressive GPU usage without speaker diarization\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úì PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing torch...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch'])\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# GPU OPTIMIZATION OPTIONS - MODIFY THESE!\n",
        "# ============================================\n",
        "GPU_OPTIONS = {\n",
        "    # OPTION 1: Parallel processing (risky but uses more memory)\n",
        "    'PARALLEL_FILES': 1,  # Process 2 files simultaneously (set to 1 for safe mode)\n",
        "\n",
        "    # OPTION 2: Pre-cache audio in GPU memory\n",
        "    'PRELOAD_TO_GPU': True,  # Load audio directly to GPU tensors\n",
        "\n",
        "    # OPTION 3: Increase internal mel spectrogram cache\n",
        "    'MEL_CACHE_SIZE': 10,  # Cache multiple mel spectrograms\n",
        "\n",
        "    # OPTION 4: Keep model in higher precision (uses more memory)\n",
        "    'USE_FLOAT32': False,  # Set True to use float32 (doubles memory usage)\n",
        "\n",
        "    # OPTION 5: Decode parameters affecting memory\n",
        "    'BEAM_SIZE': 5,  # Increase for more memory usage (default 5)\n",
        "    'BEST_OF': 5,   # Increase for more memory usage (default 5)\n",
        "\n",
        "    # OPTION 6: Process longer chunks at once\n",
        "    'N_FRAMES': 4500,  # Default is 3000, increase to process more at once\n",
        "}\n",
        "\n",
        "print(\"üöÄ Whisper V3 GPU-Optimized System\")\n",
        "print(\"=\"*60)\n",
        "print(\"GPU Memory Optimization Settings:\")\n",
        "for key, value in GPU_OPTIONS.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MEMORY MONITORING\n",
        "# ============================================\n",
        "def get_gpu_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "def print_gpu_stats(prefix=\"\"):\n",
        "    alloc, reserved, total = get_gpu_stats()\n",
        "    print(f\"{prefix}GPU Memory: {alloc:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "\n",
        "# ============================================\n",
        "# AGGRESSIVE GPU MEMORY ALLOCATION\n",
        "# ============================================\n",
        "PERSISTENT_TENSORS = []  # Keep these alive throughout execution\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Force PyTorch to allocate more memory upfront\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "\n",
        "    # Enable memory efficient attention if available\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "    # Allocate persistent tensors to reach target memory usage\n",
        "    print(\"Pre-allocating GPU memory aggressively...\")\n",
        "    target_gb = GPU_OPTIONS.get('ALLOCATE_EXTRA_GB', 4)\n",
        "\n",
        "    try:\n",
        "        # Allocate large persistent tensors\n",
        "        for i in range(target_gb):\n",
        "            # 1GB tensor that stays in memory\n",
        "            tensor = torch.zeros(256, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "            PERSISTENT_TENSORS.append(tensor)\n",
        "            print_gpu_stats(f\"  After allocation {i+1}GB: \")\n",
        "\n",
        "        # Additional allocation to reach 15GB\n",
        "        if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "            print(\"  Allocating extra memory for parallel processing...\")\n",
        "            for i in range(2):\n",
        "                tensor = torch.zeros(512, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "                PERSISTENT_TENSORS.append(tensor)\n",
        "                print_gpu_stats(f\"  Extra allocation {i+1}: \")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"  Reached memory limit: {e}\")\n",
        "\n",
        "    print(f\"  Total persistent tensors: {len(PERSISTENT_TENSORS)}\")\n",
        "    torch.cuda.synchronize()  # Ensure allocations are complete\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL WITH OPTIONS\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model with specific dtype\n",
        "dtype = torch.float32 if GPU_OPTIONS['USE_FLOAT32'] else torch.float16\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "# Force model to use more memory\n",
        "if device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']:\n",
        "    model = model.half()  # FP16 mode\n",
        "else:\n",
        "    model = model.float()  # FP32 mode (uses 2x memory)\n",
        "\n",
        "print_gpu_stats(\"After model load: \")\n",
        "\n",
        "# Modify model's n_frames if specified\n",
        "if hasattr(model, 'dims') and hasattr(model.dims, 'n_audio_ctx'):\n",
        "    original_frames = model.dims.n_audio_ctx\n",
        "    model.dims.n_audio_ctx = GPU_OPTIONS['N_FRAMES']\n",
        "    print(f\"Modified n_frames: {original_frames} ‚Üí {GPU_OPTIONS['N_FRAMES']}\")\n",
        "\n",
        "# ============================================\n",
        "# MEL SPECTROGRAM CACHE\n",
        "# ============================================\n",
        "mel_cache = {}\n",
        "mel_cache_lock = threading.Lock()\n",
        "\n",
        "def get_mel_cached(audio_path, model):\n",
        "    \"\"\"Cache mel spectrograms in GPU memory\"\"\"\n",
        "    with mel_cache_lock:\n",
        "        if audio_path in mel_cache:\n",
        "            return mel_cache[audio_path]\n",
        "\n",
        "        # Load and compute mel\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "        # Cache if under limit\n",
        "        if len(mel_cache) < GPU_OPTIONS['MEL_CACHE_SIZE']:\n",
        "            mel_cache[audio_path] = mel\n",
        "            print_gpu_stats(f\"  Cached mel #{len(mel_cache)}: \")\n",
        "\n",
        "        return mel\n",
        "\n",
        "# ============================================\n",
        "# PARALLEL TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file_aggressive(model, audio_path, file_idx=0):\n",
        "    \"\"\"Transcribe with aggressive GPU usage\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_gpu_optimized.txt\")\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n[Worker {file_idx}] Processing: {os.path.basename(audio_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if GPU_OPTIONS['PRELOAD_TO_GPU']:\n",
        "            # Load audio and convert to GPU tensor with correct dtype\n",
        "            print(f\"[Worker {file_idx}] Pre-loading to GPU...\")\n",
        "            audio = whisper.load_audio(audio_path)\n",
        "\n",
        "            # Match tensor dtype to model dtype\n",
        "            model_dtype = next(model.parameters()).dtype\n",
        "            audio_tensor = torch.from_numpy(audio).to(device=device, dtype=torch.float32)\n",
        "\n",
        "            # Create multiple copies to use more GPU memory\n",
        "            audio_copies = [audio_tensor.clone() for _ in range(3)]\n",
        "            print_gpu_stats(f\"[Worker {file_idx}] After loading copies: \")\n",
        "\n",
        "            # Process from numpy (Whisper expects numpy)\n",
        "            result = model.transcribe(\n",
        "                audio,  # Use original numpy array\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Keep tensors in memory during processing to maintain GPU usage\n",
        "            dummy_computation = torch.sum(torch.stack(audio_copies))\n",
        "\n",
        "        else:\n",
        "            # Standard transcription\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[Worker {file_idx}] Completed in {process_time:.1f}s\")\n",
        "        print_gpu_stats(f\"[Worker {file_idx}] During processing: \")\n",
        "\n",
        "        # Save transcript (no speaker info)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Whisper Large-v3 Transcript (GPU Optimized)\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now()}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Process time: {process_time:.1f}s\\n\")\n",
        "            f.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                if text:\n",
        "                    # Basic text cleanup\n",
        "                    text = text.replace(' h m h c ', ' HMHC ')\n",
        "                    text = text.replace(' c a ', ' CA ')\n",
        "                    text = text.replace(' . ', '.')\n",
        "\n",
        "                    f.write(f\"[{segment['start']:06.2f} ‚Üí {segment['end']:06.2f}] {text}\\n\")\n",
        "\n",
        "        return True, output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Worker {file_idx}] Error: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING WITH PARALLEL OPTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "        output_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_gpu_optimized.txt', '_final.txt', '_enhanced.txt', '_ultimate.txt']\n",
        "        )\n",
        "        if not output_exists:\n",
        "            remaining.append(wav_file)\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if not remaining:\n",
        "        print(\"‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files\n",
        "    files_to_process = remaining[:6]  # Process up to 6 files this session\n",
        "\n",
        "    if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "        # PARALLEL PROCESSING - Uses more GPU memory\n",
        "        print(f\"\\nüî• PARALLEL MODE: Processing {GPU_OPTIONS['PARALLEL_FILES']} files simultaneously\")\n",
        "        print(\"‚ö†Ô∏è  Warning: This may cause out-of-memory errors!\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=GPU_OPTIONS['PARALLEL_FILES']) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            # Submit files for parallel processing\n",
        "            for i, audio_file in enumerate(files_to_process[:GPU_OPTIONS['PARALLEL_FILES'] * 2]):\n",
        "                if len(futures) >= GPU_OPTIONS['PARALLEL_FILES']:\n",
        "                    # Wait for a slot to free up\n",
        "                    done, _ = as_completed(futures, timeout=None).__next__()\n",
        "                    futures.pop(done)\n",
        "\n",
        "                future = executor.submit(transcribe_file_aggressive, model, audio_file, i)\n",
        "                futures[future] = audio_file\n",
        "\n",
        "                # Brief pause between submissions\n",
        "                time.sleep(1)\n",
        "\n",
        "            # Wait for remaining tasks\n",
        "            for future in as_completed(futures):\n",
        "                success, result = future.result()\n",
        "                if success:\n",
        "                    print(f\"‚úì Completed: {os.path.basename(futures[future])}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed: {os.path.basename(futures[future])}\")\n",
        "\n",
        "    else:\n",
        "        # SEQUENTIAL PROCESSING - Safer but uses less memory\n",
        "        print(f\"\\nüîí SEQUENTIAL MODE: Processing files one at a time\")\n",
        "\n",
        "        for i, audio_file in enumerate(files_to_process):\n",
        "            success, result = transcribe_file_aggressive(model, audio_file, i)\n",
        "\n",
        "            if success:\n",
        "                print(f\"‚úì Saved: {os.path.basename(result)}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Failed: {result}\")\n",
        "\n",
        "            # Monitor memory between files\n",
        "            print_gpu_stats(\"Between files: \")\n",
        "\n",
        "            # Brief cooldown\n",
        "            if i < len(files_to_process) - 1:\n",
        "                time.sleep(3)\n",
        "\n",
        "    # Final cleanup\n",
        "    mel_cache.clear()\n",
        "    PERSISTENT_TENSORS.clear()  # Clear persistent memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print_gpu_stats(\"Final: \")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN MAIN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n‚öôÔ∏è  GPU OPTIMIZATION TIPS:\")\n",
        "    print(\"1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\")\n",
        "    print(\"2. Set USE_FLOAT32=True to double memory usage\")\n",
        "    print(\"3. Increase BEAM_SIZE and BEST_OF for more memory use\")\n",
        "    print(\"4. Set PRELOAD_TO_GPU=True to cache audio in GPU\")\n",
        "    print(\"\\n‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# EXPERIMENTAL: EXTREME GPU USAGE\n",
        "# ============================================\n",
        "\"\"\"\n",
        "FOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\n",
        "\n",
        "GPU_OPTIONS = {\n",
        "    'PARALLEL_FILES': 3,      # Process 3 files at once\n",
        "    'PRELOAD_TO_GPU': True,   # Cache in GPU\n",
        "    'MEL_CACHE_SIZE': 20,     # Large cache\n",
        "    'USE_FLOAT32': True,      # Double precision (2x memory)\n",
        "    'BEAM_SIZE': 20,          # Very large beam\n",
        "    'BEST_OF': 20,            # Very large sampling\n",
        "    'N_FRAMES': 6000,         # Process huge chunks\n",
        "}\n",
        "\n",
        "This will likely crash but will definitely use all GPU memory!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Trvbmm6pkJdu",
        "outputId": "675a0c43-8220-4a7b-dccf-96737a02b778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies...\n",
            "‚úì Whisper already installed\n",
            "‚úì PyTorch already installed\n",
            "üöÄ Whisper V3 GPU-Optimized System\n",
            "============================================================\n",
            "GPU Memory Optimization Settings:\n",
            "  PARALLEL_FILES: 1\n",
            "  PRELOAD_TO_GPU: True\n",
            "  MEL_CACHE_SIZE: 10\n",
            "  USE_FLOAT32: False\n",
            "  BEAM_SIZE: 5\n",
            "  BEST_OF: 5\n",
            "  N_FRAMES: 4500\n",
            "============================================================\n",
            "Pre-allocating GPU memory aggressively...\n",
            "  Reached memory limit: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 646.12 MiB is free. Process 50434 has 14.11 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 13.70 GiB is allocated by PyTorch, and 306.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "  Total persistent tensors: 0\n",
            "\n",
            "Loading Whisper large-v3...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDMn5NiWkJRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 GPU-OPTIMIZED - MAXIMUM MEMORY UTILIZATION\n",
        "# Aggressive GPU usage without speaker diarization\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úì PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing torch...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch'])\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# GPU OPTIMIZATION OPTIONS - MODIFY THESE!\n",
        "# ============================================\n",
        "GPU_OPTIONS = {\n",
        "    # OPTION 1: Parallel processing (risky but uses more memory)\n",
        "    'PARALLEL_FILES': 2,  # Process 2 files simultaneously (set to 1 for safe mode)\n",
        "\n",
        "    # OPTION 2: Pre-cache audio in GPU memory\n",
        "    'PRELOAD_TO_GPU': False,  # Load audio directly to GPU tensors\n",
        "\n",
        "    # OPTION 3: Increase internal mel spectrogram cache\n",
        "    'MEL_CACHE_SIZE': 5,  # Cache multiple mel spectrograms\n",
        "\n",
        "    # OPTION 4: Keep model in higher precision (uses more memory)\n",
        "    'USE_FLOAT32': False,  # Set True to use float32 (doubles memory usage)\n",
        "\n",
        "    # OPTION 5: Decode parameters affecting memory\n",
        "    'BEAM_SIZE': 5,  # Increase for more memory usage (default 5)\n",
        "    'BEST_OF': 5,   # Increase for more memory usage (default 5)\n",
        "\n",
        "    # OPTION 6: Process longer chunks at once\n",
        "    'N_FRAMES': 4500,  # Default is 3000, increase to process more at once\n",
        "}\n",
        "\n",
        "print(\"üöÄ Whisper V3 GPU-Optimized System\")\n",
        "print(\"=\"*60)\n",
        "print(\"GPU Memory Optimization Settings:\")\n",
        "for key, value in GPU_OPTIONS.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MEMORY MONITORING\n",
        "# ============================================\n",
        "def get_gpu_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "def print_gpu_stats(prefix=\"\"):\n",
        "    alloc, reserved, total = get_gpu_stats()\n",
        "    print(f\"{prefix}GPU Memory: {alloc:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "\n",
        "# ============================================\n",
        "# AGGRESSIVE GPU MEMORY ALLOCATION\n",
        "# ============================================\n",
        "PERSISTENT_TENSORS = []  # Keep these alive throughout execution\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Force PyTorch to allocate more memory upfront\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "\n",
        "    # Enable memory efficient attention if available\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "    # Allocate persistent tensors to reach target memory usage\n",
        "    print(\"Pre-allocating GPU memory aggressively...\")\n",
        "    target_gb = GPU_OPTIONS.get('ALLOCATE_EXTRA_GB', 4)\n",
        "\n",
        "    try:\n",
        "        # Allocate large persistent tensors\n",
        "        for i in range(target_gb):\n",
        "            # 1GB tensor that stays in memory\n",
        "            tensor = torch.zeros(256, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "            PERSISTENT_TENSORS.append(tensor)\n",
        "            print_gpu_stats(f\"  After allocation {i+1}GB: \")\n",
        "\n",
        "        # Additional allocation to reach 15GB\n",
        "        if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "            print(\"  Allocating extra memory for parallel processing...\")\n",
        "            for i in range(2):\n",
        "                tensor = torch.zeros(512, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "                PERSISTENT_TENSORS.append(tensor)\n",
        "                print_gpu_stats(f\"  Extra allocation {i+1}: \")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"  Reached memory limit: {e}\")\n",
        "\n",
        "    print(f\"  Total persistent tensors: {len(PERSISTENT_TENSORS)}\")\n",
        "    torch.cuda.synchronize()  # Ensure allocations are complete\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL WITH OPTIONS\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model with specific dtype\n",
        "dtype = torch.float32 if GPU_OPTIONS['USE_FLOAT32'] else torch.float16\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "# Force model to use more memory\n",
        "if device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']:\n",
        "    model = model.half()  # FP16 mode\n",
        "else:\n",
        "    model = model.float()  # FP32 mode (uses 2x memory)\n",
        "\n",
        "print_gpu_stats(\"After model load: \")\n",
        "\n",
        "# Modify model's n_frames if specified\n",
        "if hasattr(model, 'dims') and hasattr(model.dims, 'n_audio_ctx'):\n",
        "    original_frames = model.dims.n_audio_ctx\n",
        "    model.dims.n_audio_ctx = GPU_OPTIONS['N_FRAMES']\n",
        "    print(f\"Modified n_frames: {original_frames} ‚Üí {GPU_OPTIONS['N_FRAMES']}\")\n",
        "\n",
        "# ============================================\n",
        "# MEL SPECTROGRAM CACHE\n",
        "# ============================================\n",
        "mel_cache = {}\n",
        "mel_cache_lock = threading.Lock()\n",
        "\n",
        "def get_mel_cached(audio_path, model):\n",
        "    \"\"\"Cache mel spectrograms in GPU memory\"\"\"\n",
        "    with mel_cache_lock:\n",
        "        if audio_path in mel_cache:\n",
        "            return mel_cache[audio_path]\n",
        "\n",
        "        # Load and compute mel\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "        # Cache if under limit\n",
        "        if len(mel_cache) < GPU_OPTIONS['MEL_CACHE_SIZE']:\n",
        "            mel_cache[audio_path] = mel\n",
        "            print_gpu_stats(f\"  Cached mel #{len(mel_cache)}: \")\n",
        "\n",
        "        return mel\n",
        "\n",
        "# ============================================\n",
        "# PARALLEL TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file_aggressive(model, audio_path, file_idx=0):\n",
        "    \"\"\"Transcribe with aggressive GPU usage\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_gpu_optimized.txt\")\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n[Worker {file_idx}] Processing: {os.path.basename(audio_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if GPU_OPTIONS['PRELOAD_TO_GPU']:\n",
        "            # Load audio and convert to GPU tensor with correct dtype\n",
        "            print(f\"[Worker {file_idx}] Pre-loading to GPU...\")\n",
        "            audio = whisper.load_audio(audio_path)\n",
        "\n",
        "            # Match tensor dtype to model dtype\n",
        "            model_dtype = next(model.parameters()).dtype\n",
        "            audio_tensor = torch.from_numpy(audio).to(device=device, dtype=torch.float32)\n",
        "\n",
        "            # Create multiple copies to use more GPU memory\n",
        "            audio_copies = [audio_tensor.clone() for _ in range(3)]\n",
        "            print_gpu_stats(f\"[Worker {file_idx}] After loading copies: \")\n",
        "\n",
        "            # Process from numpy (Whisper expects numpy)\n",
        "            result = model.transcribe(\n",
        "                audio,  # Use original numpy array\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Keep tensors in memory during processing to maintain GPU usage\n",
        "            dummy_computation = torch.sum(torch.stack(audio_copies))\n",
        "\n",
        "        else:\n",
        "            # Standard transcription\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[Worker {file_idx}] Completed in {process_time:.1f}s\")\n",
        "        print_gpu_stats(f\"[Worker {file_idx}] During processing: \")\n",
        "\n",
        "        # Save transcript (no speaker info)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Whisper Large-v3 Transcript (GPU Optimized)\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now()}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Process time: {process_time:.1f}s\\n\")\n",
        "            f.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                if text:\n",
        "                    # Basic text cleanup\n",
        "                    text = text.replace(' h m h c ', ' HMHC ')\n",
        "                    text = text.replace(' c a ', ' CA ')\n",
        "                    text = text.replace(' . ', '.')\n",
        "\n",
        "                    f.write(f\"[{segment['start']:06.2f} ‚Üí {segment['end']:06.2f}] {text}\\n\")\n",
        "\n",
        "        return True, output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Worker {file_idx}] Error: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING WITH PARALLEL OPTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "        output_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_gpu_optimized.txt', '_final.txt', '_enhanced.txt', '_ultimate.txt']\n",
        "        )\n",
        "        if not output_exists:\n",
        "            remaining.append(wav_file)\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if not remaining:\n",
        "        print(\"‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files\n",
        "    files_to_process = remaining[:6]  # Process up to 6 files this session\n",
        "\n",
        "    if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "        # PARALLEL PROCESSING - Uses more GPU memory\n",
        "        print(f\"\\nüî• PARALLEL MODE: Processing {GPU_OPTIONS['PARALLEL_FILES']} files simultaneously\")\n",
        "        print(\"‚ö†Ô∏è  Warning: This may cause out-of-memory errors!\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=GPU_OPTIONS['PARALLEL_FILES']) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            # Submit files for parallel processing\n",
        "            for i, audio_file in enumerate(files_to_process[:GPU_OPTIONS['PARALLEL_FILES'] * 2]):\n",
        "                if len(futures) >= GPU_OPTIONS['PARALLEL_FILES']:\n",
        "                    # Wait for a slot to free up\n",
        "                    done, _ = as_completed(futures, timeout=None).__next__()\n",
        "                    futures.pop(done)\n",
        "\n",
        "                future = executor.submit(transcribe_file_aggressive, model, audio_file, i)\n",
        "                futures[future] = audio_file\n",
        "\n",
        "                # Brief pause between submissions\n",
        "                time.sleep(1)\n",
        "\n",
        "            # Wait for remaining tasks\n",
        "            for future in as_completed(futures):\n",
        "                success, result = future.result()\n",
        "                if success:\n",
        "                    print(f\"‚úì Completed: {os.path.basename(futures[future])}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed: {os.path.basename(futures[future])}\")\n",
        "\n",
        "    else:\n",
        "        # SEQUENTIAL PROCESSING - Safer but uses less memory\n",
        "        print(f\"\\nüîí SEQUENTIAL MODE: Processing files one at a time\")\n",
        "\n",
        "        for i, audio_file in enumerate(files_to_process):\n",
        "            success, result = transcribe_file_aggressive(model, audio_file, i)\n",
        "\n",
        "            if success:\n",
        "                print(f\"‚úì Saved: {os.path.basename(result)}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Failed: {result}\")\n",
        "\n",
        "            # Monitor memory between files\n",
        "            print_gpu_stats(\"Between files: \")\n",
        "\n",
        "            # Brief cooldown\n",
        "            if i < len(files_to_process) - 1:\n",
        "                time.sleep(3)\n",
        "\n",
        "    # Final cleanup\n",
        "    mel_cache.clear()\n",
        "    PERSISTENT_TENSORS.clear()  # Clear persistent memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print_gpu_stats(\"Final: \")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN MAIN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n‚öôÔ∏è  GPU OPTIMIZATION TIPS:\")\n",
        "    print(\"1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\")\n",
        "    print(\"2. Set USE_FLOAT32=True to double memory usage\")\n",
        "    print(\"3. Increase BEAM_SIZE and BEST_OF for more memory use\")\n",
        "    print(\"4. Set PRELOAD_TO_GPU=True to cache audio in GPU\")\n",
        "    print(\"\\n‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# EXPERIMENTAL: EXTREME GPU USAGE\n",
        "# ============================================\n",
        "\"\"\"\n",
        "FOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\n",
        "\n",
        "GPU_OPTIONS = {\n",
        "    'PARALLEL_FILES': 3,      # Process 3 files at once\n",
        "    'PRELOAD_TO_GPU': True,   # Cache in GPU\n",
        "    'MEL_CACHE_SIZE': 20,     # Large cache\n",
        "    'USE_FLOAT32': True,      # Double precision (2x memory)\n",
        "    'BEAM_SIZE': 20,          # Very large beam\n",
        "    'BEST_OF': 20,            # Very large sampling\n",
        "    'N_FRAMES': 6000,         # Process huge chunks\n",
        "}\n",
        "\n",
        "This will likely crash but will definitely use all GPU memory!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3J4kvP0JjAaq",
        "outputId": "0edd7294-08fa-47d2-df34-72635c94d64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies...\n",
            "‚úì Whisper already installed\n",
            "‚úì PyTorch already installed\n",
            "üöÄ Whisper V3 GPU-Optimized System\n",
            "============================================================\n",
            "GPU Memory Optimization Settings:\n",
            "  PARALLEL_FILES: 2\n",
            "  PRELOAD_TO_GPU: False\n",
            "  MEL_CACHE_SIZE: 5\n",
            "  USE_FLOAT32: False\n",
            "  BEAM_SIZE: 5\n",
            "  BEST_OF: 5\n",
            "  N_FRAMES: 4500\n",
            "============================================================\n",
            "Pre-allocating GPU memory aggressively...\n",
            "  After allocation 1GB: GPU Memory: 1.1GB allocated, 1.1GB reserved / 15.8GB total\n",
            "  After allocation 2GB: GPU Memory: 2.1GB allocated, 2.1GB reserved / 15.8GB total\n",
            "  After allocation 3GB: GPU Memory: 3.2GB allocated, 3.2GB reserved / 15.8GB total\n",
            "  After allocation 4GB: GPU Memory: 4.3GB allocated, 4.3GB reserved / 15.8GB total\n",
            "  Allocating extra memory for parallel processing...\n",
            "  Extra allocation 1: GPU Memory: 6.4GB allocated, 6.4GB reserved / 15.8GB total\n",
            "  Extra allocation 2: GPU Memory: 8.6GB allocated, 8.6GB reserved / 15.8GB total\n",
            "  Total persistent tensors: 6\n",
            "\n",
            "Loading Whisper large-v3...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 646.12 MiB is free. Process 40798 has 14.11 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 13.70 GiB is allocated by PyTorch, and 306.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2040763907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m# Load model with specific dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mGPU_OPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USE_FLOAT32'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"large-v3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# Force model to use more memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alignment_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 646.12 MiB is free. Process 40798 has 14.11 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 13.70 GiB is allocated by PyTorch, and 306.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78KyFu68jAPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 GPU-OPTIMIZED - MAXIMUM MEMORY UTILIZATION\n",
        "# Aggressive GPU usage without speaker diarization\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úì PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing torch...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch'])\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# GPU OPTIMIZATION OPTIONS - MODIFY THESE!\n",
        "# ============================================\n",
        "GPU_OPTIONS = {\n",
        "    # OPTION 1: Parallel processing (risky but uses more memory)\n",
        "    'PARALLEL_FILES': 1,  # Process 2 files simultaneously (set to 1 for safe mode)\n",
        "\n",
        "    # OPTION 2: Pre-cache audio in GPU memory\n",
        "    'PRELOAD_TO_GPU': True,  # Load audio directly to GPU tensors\n",
        "\n",
        "    # OPTION 3: Increase internal mel spectrogram cache\n",
        "    'MEL_CACHE_SIZE': 10,  # Cache multiple mel spectrograms\n",
        "\n",
        "    # OPTION 4: Keep model in higher precision (uses more memory)\n",
        "    'USE_FLOAT32': False,  # Set True to use float32 (doubles memory usage)\n",
        "\n",
        "    # OPTION 5: Decode parameters affecting memory\n",
        "    'BEAM_SIZE': 10,  # Increase for more memory usage (default 5)\n",
        "    'BEST_OF': 10,   # Increase for more memory usage (default 5)\n",
        "\n",
        "    # OPTION 6: Process longer chunks at once\n",
        "    'N_FRAMES': 5000,  # Default is 3000, increase to process more at once\n",
        "}\n",
        "\n",
        "print(\"üöÄ Whisper V3 GPU-Optimized System\")\n",
        "print(\"=\"*60)\n",
        "print(\"GPU Memory Optimization Settings:\")\n",
        "for key, value in GPU_OPTIONS.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MEMORY MONITORING\n",
        "# ============================================\n",
        "def get_gpu_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "def print_gpu_stats(prefix=\"\"):\n",
        "    alloc, reserved, total = get_gpu_stats()\n",
        "    print(f\"{prefix}GPU Memory: {alloc:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "\n",
        "# ============================================\n",
        "# AGGRESSIVE GPU MEMORY ALLOCATION\n",
        "# ============================================\n",
        "if torch.cuda.is_available():\n",
        "    # Force PyTorch to allocate more memory upfront\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "\n",
        "    # Enable memory efficient attention if available\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "    # Allocate dummy tensors to reserve memory\n",
        "    print(\"Pre-allocating GPU memory...\")\n",
        "    dummy_tensors = []\n",
        "    try:\n",
        "        # Try to allocate 2GB chunks until we hit limit\n",
        "        for i in range(5):\n",
        "            dummy = torch.zeros(256, 1024, 1024, device='cuda')  # ~1GB each\n",
        "            dummy_tensors.append(dummy)\n",
        "            print_gpu_stats(f\"  After allocation {i+1}: \")\n",
        "    except RuntimeError:\n",
        "        print(\"  Reached memory limit\")\n",
        "\n",
        "    # Clear dummy tensors but keep memory reserved\n",
        "    dummy_tensors.clear()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL WITH OPTIONS\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model with specific dtype\n",
        "dtype = torch.float32 if GPU_OPTIONS['USE_FLOAT32'] else torch.float16\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "# Force model to use more memory\n",
        "if device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']:\n",
        "    model = model.half()  # FP16 mode\n",
        "else:\n",
        "    model = model.float()  # FP32 mode (uses 2x memory)\n",
        "\n",
        "print_gpu_stats(\"After model load: \")\n",
        "\n",
        "# Modify model's n_frames if specified\n",
        "if hasattr(model, 'dims') and hasattr(model.dims, 'n_audio_ctx'):\n",
        "    original_frames = model.dims.n_audio_ctx\n",
        "    model.dims.n_audio_ctx = GPU_OPTIONS['N_FRAMES']\n",
        "    print(f\"Modified n_frames: {original_frames} ‚Üí {GPU_OPTIONS['N_FRAMES']}\")\n",
        "\n",
        "# ============================================\n",
        "# MEL SPECTROGRAM CACHE\n",
        "# ============================================\n",
        "mel_cache = {}\n",
        "mel_cache_lock = threading.Lock()\n",
        "\n",
        "def get_mel_cached(audio_path, model):\n",
        "    \"\"\"Cache mel spectrograms in GPU memory\"\"\"\n",
        "    with mel_cache_lock:\n",
        "        if audio_path in mel_cache:\n",
        "            return mel_cache[audio_path]\n",
        "\n",
        "        # Load and compute mel\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "        # Cache if under limit\n",
        "        if len(mel_cache) < GPU_OPTIONS['MEL_CACHE_SIZE']:\n",
        "            mel_cache[audio_path] = mel\n",
        "            print_gpu_stats(f\"  Cached mel #{len(mel_cache)}: \")\n",
        "\n",
        "        return mel\n",
        "\n",
        "# ============================================\n",
        "# PARALLEL TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file_aggressive(model, audio_path, file_idx=0):\n",
        "    \"\"\"Transcribe with aggressive GPU usage\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_gpu_optimized.txt\")\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n[Worker {file_idx}] Processing: {os.path.basename(audio_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if GPU_OPTIONS['PRELOAD_TO_GPU']:\n",
        "            # Load audio and convert to GPU tensor immediately\n",
        "            print(f\"[Worker {file_idx}] Pre-loading to GPU...\")\n",
        "            audio = whisper.load_audio(audio_path)\n",
        "            audio_tensor = torch.from_numpy(audio).to(device)\n",
        "\n",
        "            # Process from GPU tensor\n",
        "            result = model.transcribe(\n",
        "                audio_tensor.cpu().numpy(),  # Whisper expects numpy\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Keep tensor in GPU memory during processing\n",
        "            audio_tensor = audio_tensor.contiguous()\n",
        "\n",
        "        else:\n",
        "            # Standard transcription\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[Worker {file_idx}] Completed in {process_time:.1f}s\")\n",
        "        print_gpu_stats(f\"[Worker {file_idx}] During processing: \")\n",
        "\n",
        "        # Save transcript (no speaker info)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Whisper Large-v3 Transcript (GPU Optimized)\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now()}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Process time: {process_time:.1f}s\\n\")\n",
        "            f.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                if text:\n",
        "                    # Basic text cleanup\n",
        "                    text = text.replace(' h m h c ', ' HMHC ')\n",
        "                    text = text.replace(' c a ', ' CA ')\n",
        "                    text = text.replace(' . ', '.')\n",
        "\n",
        "                    f.write(f\"[{segment['start']:06.2f} ‚Üí {segment['end']:06.2f}] {text}\\n\")\n",
        "\n",
        "        return True, output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Worker {file_idx}] Error: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING WITH PARALLEL OPTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "        output_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_gpu_optimized.txt', '_final.txt', '_enhanced.txt', '_ultimate.txt']\n",
        "        )\n",
        "        if not output_exists:\n",
        "            remaining.append(wav_file)\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if not remaining:\n",
        "        print(\"‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files\n",
        "    files_to_process = remaining[:6]  # Process up to 6 files this session\n",
        "\n",
        "    if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "        # PARALLEL PROCESSING - Uses more GPU memory\n",
        "        print(f\"\\nüî• PARALLEL MODE: Processing {GPU_OPTIONS['PARALLEL_FILES']} files simultaneously\")\n",
        "        print(\"‚ö†Ô∏è  Warning: This may cause out-of-memory errors!\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=GPU_OPTIONS['PARALLEL_FILES']) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            # Submit files for parallel processing\n",
        "            for i, audio_file in enumerate(files_to_process[:GPU_OPTIONS['PARALLEL_FILES'] * 2]):\n",
        "                if len(futures) >= GPU_OPTIONS['PARALLEL_FILES']:\n",
        "                    # Wait for a slot to free up\n",
        "                    done, _ = as_completed(futures, timeout=None).__next__()\n",
        "                    futures.pop(done)\n",
        "\n",
        "                future = executor.submit(transcribe_file_aggressive, model, audio_file, i)\n",
        "                futures[future] = audio_file\n",
        "\n",
        "                # Brief pause between submissions\n",
        "                time.sleep(1)\n",
        "\n",
        "            # Wait for remaining tasks\n",
        "            for future in as_completed(futures):\n",
        "                success, result = future.result()\n",
        "                if success:\n",
        "                    print(f\"‚úì Completed: {os.path.basename(futures[future])}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed: {os.path.basename(futures[future])}\")\n",
        "\n",
        "    else:\n",
        "        # SEQUENTIAL PROCESSING - Safer but uses less memory\n",
        "        print(f\"\\nüîí SEQUENTIAL MODE: Processing files one at a time\")\n",
        "\n",
        "        for i, audio_file in enumerate(files_to_process):\n",
        "            success, result = transcribe_file_aggressive(model, audio_file, i)\n",
        "\n",
        "            if success:\n",
        "                print(f\"‚úì Saved: {os.path.basename(result)}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Failed: {result}\")\n",
        "\n",
        "            # Monitor memory between files\n",
        "            print_gpu_stats(\"Between files: \")\n",
        "\n",
        "            # Brief cooldown\n",
        "            if i < len(files_to_process) - 1:\n",
        "                time.sleep(3)\n",
        "\n",
        "    # Final cleanup\n",
        "    mel_cache.clear()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print_gpu_stats(\"Final: \")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN MAIN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n‚öôÔ∏è  GPU OPTIMIZATION TIPS:\")\n",
        "    print(\"1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\")\n",
        "    print(\"2. Set USE_FLOAT32=True to double memory usage\")\n",
        "    print(\"3. Increase BEAM_SIZE and BEST_OF for more memory use\")\n",
        "    print(\"4. Set PRELOAD_TO_GPU=True to cache audio in GPU\")\n",
        "    print(\"\\n‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# EXPERIMENTAL: EXTREME GPU USAGE\n",
        "# ============================================\n",
        "\"\"\"\n",
        "FOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\n",
        "\n",
        "GPU_OPTIONS = {\n",
        "    'PARALLEL_FILES': 3,      # Process 3 files at once\n",
        "    'PRELOAD_TO_GPU': True,   # Cache in GPU\n",
        "    'MEL_CACHE_SIZE': 20,     # Large cache\n",
        "    'USE_FLOAT32': True,      # Double precision (2x memory)\n",
        "    'BEAM_SIZE': 20,          # Very large beam\n",
        "    'BEST_OF': 20,            # Very large sampling\n",
        "    'N_FRAMES': 6000,         # Process huge chunks\n",
        "}\n",
        "\n",
        "This will likely crash but will definitely use all GPU memory!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l7BK8AlvhOZX",
        "outputId": "0262f5e2-387f-44c7-eb56-7c538a067856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üì¶ Installing dependencies...\n",
            "Installing openai-whisper...\n",
            "‚úì Whisper installed\n",
            "‚úì PyTorch already installed\n",
            "üöÄ Whisper V3 GPU-Optimized System\n",
            "============================================================\n",
            "GPU Memory Optimization Settings:\n",
            "  PARALLEL_FILES: 1\n",
            "  PRELOAD_TO_GPU: True\n",
            "  MEL_CACHE_SIZE: 10\n",
            "  USE_FLOAT32: False\n",
            "  BEAM_SIZE: 10\n",
            "  BEST_OF: 10\n",
            "  N_FRAMES: 5000\n",
            "============================================================\n",
            "Pre-allocating GPU memory...\n",
            "  After allocation 1: GPU Memory: 1.1GB allocated, 1.1GB reserved / 15.8GB total\n",
            "  After allocation 2: GPU Memory: 2.1GB allocated, 2.1GB reserved / 15.8GB total\n",
            "  After allocation 3: GPU Memory: 3.2GB allocated, 3.2GB reserved / 15.8GB total\n",
            "  After allocation 4: GPU Memory: 4.3GB allocated, 4.3GB reserved / 15.8GB total\n",
            "  After allocation 5: GPU Memory: 5.4GB allocated, 5.4GB reserved / 15.8GB total\n",
            "\n",
            "Loading Whisper large-v3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [01:57<00:00, 26.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After model load: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "Modified n_frames: 1500 ‚Üí 5000\n",
            "\n",
            "‚öôÔ∏è  GPU OPTIMIZATION TIPS:\n",
            "1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\n",
            "2. Set USE_FLOAT32=True to double memory usage\n",
            "3. Increase BEAM_SIZE and BEST_OF for more memory use\n",
            "4. Set PRELOAD_TO_GPU=True to cache audio in GPU\n",
            "\n",
            "‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\n",
            "============================================================\n",
            "\n",
            "Files to process: 5\n",
            "\n",
            "üîí SEQUENTIAL MODE: Processing files one at a time\n",
            "\n",
            "[Worker 0] Processing: Call Recording - 19Mar2025 0800 JD.wav\n",
            "[Worker 0] Pre-loading to GPU...\n",
            "[Worker 0] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 1] Processing: Call Recording - 19Mar25 0900 - AJ.wav\n",
            "[Worker 1] Pre-loading to GPU...\n",
            "[Worker 1] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 2] Processing: Call Recording - 19Mar25 1730 - MO.wav\n",
            "[Worker 2] Pre-loading to GPU...\n",
            "[Worker 2] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 3] Processing: Call Recording - 20Mar2025 1200 LN.wav\n",
            "[Worker 3] Pre-loading to GPU...\n",
            "[Worker 3] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 4] Processing: Call Recording - 26Mar2025 0830 SA.wav\n",
            "[Worker 4] Pre-loading to GPU...\n",
            "[Worker 4] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "============================================================\n",
            "‚úÖ SESSION COMPLETE\n",
            "Final: GPU Memory: 4.2GB allocated, 5.1GB reserved / 15.8GB total\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\\n\\nGPU_OPTIONS = {\\n    'PARALLEL_FILES': 3,      # Process 3 files at once\\n    'PRELOAD_TO_GPU': True,   # Cache in GPU\\n    'MEL_CACHE_SIZE': 20,     # Large cache\\n    'USE_FLOAT32': True,      # Double precision (2x memory)\\n    'BEAM_SIZE': 20,          # Very large beam\\n    'BEST_OF': 20,            # Very large sampling\\n    'N_FRAMES': 6000,         # Process huge chunks\\n}\\n\\nThis will likely crash but will definitely use all GPU memory!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xlTXAAphOPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fg3kk7mXfEeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CHlcQADZ-uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Odc4_SxZUN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Drive File Storage\n"
      ],
      "metadata": {
        "id": "tvVwNwhHXQNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE WORKING TRANSCRIPTION SYSTEM\n",
        "# Based on the approach that was working\n",
        "\n",
        "# ============================================\n",
        "# CELL 1: Complete Setup and Processing\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "# if not os.path.exists('/content/drive'):\n",
        "#    drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MUIMLVU4Wtzj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 2: File Discovery and Status\n",
        "# ============================================\n",
        "\"\"\"\n",
        "Run this to see what files need processing\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths - adjust these to your actual locations\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "\n",
        "# Get list of audio files\n",
        "mp4_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.mp4\")))\n",
        "wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "all_audio_files = mp4_files + wav_files\n",
        "\n",
        "print(f\"\\nüìÅ Found {len(all_audio_files)} audio files:\")\n",
        "for i, f in enumerate(all_audio_files, 1):\n",
        "    print(f\"  {i}. {os.path.basename(f)}\")\n",
        "\n",
        "# Check what's already been transcribed\n",
        "completed_files = []\n",
        "remaining_files = []\n",
        "\n",
        "for audio_file in all_audio_files:\n",
        "    base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
        "    transcript_path = os.path.join(OUTPUT_PATH, f\"{base_name}_transcript.txt\")\n",
        "\n",
        "    if os.path.exists(transcript_path):\n",
        "        completed_files.append(audio_file)\n",
        "    else:\n",
        "        remaining_files.append(audio_file)\n",
        "\n",
        "print(f\"\\nüìä Status:\")\n",
        "print(f\"  ‚úì Completed: {len(completed_files)}\")\n",
        "print(f\"  ‚è≥ Remaining: {len(remaining_files)}\")\n",
        "\n",
        "if remaining_files:\n",
        "    print(f\"\\nüéØ Next file to process: {os.path.basename(remaining_files[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7h4HOEbWvno",
        "outputId": "a36521ba-b00a-444e-9cbf-3d3b48d42780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "üìÅ Found 8 audio files:\n",
            "  1. Call Recording - 13Mar2025 1200 BPA.wav\n",
            "  2. Call Recording - 13Mar25 1130 BK.wav\n",
            "  3. Call Recording - 13Mar25 1300 HB.wav\n",
            "  4. Call Recording - 19Mar2025 0800 JD.wav\n",
            "  5. Call Recording - 19Mar25 0900 - AJ.wav\n",
            "  6. Call Recording - 19Mar25 1730 - MO.wav\n",
            "  7. Call Recording - 20Mar2025 1200 LN.wav\n",
            "  8. Call Recording - 26Mar2025 0830 SA.wav\n",
            "\n",
            "üìä Status:\n",
            "  ‚úì Completed: 0\n",
            "  ‚è≥ Remaining: 8\n",
            "\n",
            "üéØ Next file to process: Call Recording - 13Mar2025 1200 BPA.wav\n"
          ]
        }
      ]
    }
  ]
}