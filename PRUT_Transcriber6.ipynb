{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VJMeyer/HPVPre_Repo/blob/main/PRUT_Transcriber6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uftFDHicUxAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e394ebf-0598-460d-d46a-0e4b20ac0da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üöÄ Whisper V3 Ultimate Transcription System\n",
            "============================================================\n",
            "Installing openai-whisper...\n"
          ]
        }
      ],
      "source": [
        "# WHISPER V3 ULTIMATE - PRODUCTION-READY TRANSCRIPTION SYSTEM\n",
        "# All optimizations, fixes, and enhancements in one complete solution\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import re\n",
        "import subprocess\n",
        "import json\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive, userdata\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION - FULLY OPTIMIZED\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "WHISPER_MODEL = \"large-v3\"\n",
        "\n",
        "# Optimized parameters for speed and quality\n",
        "TRANSCRIPTION_PARAMS = {\n",
        "    'beam_size': 3,              # Reduced from 5 for speed\n",
        "    'best_of': 3,                # Reduced from 5\n",
        "    'patience': 0.8,             # Slightly less patient for speed\n",
        "    'length_penalty': 1.0,\n",
        "    'temperature': 0.2,          # Single temp, no fallback needed\n",
        "    'compression_ratio_threshold': 2.4,\n",
        "    'logprob_threshold': -1.0,\n",
        "    'no_speech_threshold': 0.6,\n",
        "    'condition_on_previous_text': True,  # Keep for quality\n",
        "    'initial_prompt': \"This is a conversation about support services. Speaker changes are marked clearly.\",\n",
        "    'word_timestamps': True,\n",
        "    'prepend_punctuations': '\"\\'\"¬ø([{-',\n",
        "    'append_punctuations': '\"\\'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ',\n",
        "    'hallucination_silence_threshold': 2.0,\n",
        "}\n",
        "\n",
        "# GPU optimization settings\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Whisper V3 Ultimate Transcription System\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# INSTALL AND IMPORT DEPENDENCIES\n",
        "# ============================================\n",
        "def install_dependencies():\n",
        "    \"\"\"Install all required packages efficiently\"\"\"\n",
        "    packages = {\n",
        "        'openai-whisper': 'whisper',\n",
        "        'pyannote.audio': 'pyannote.audio',\n",
        "        'pydub': 'pydub'\n",
        "    }\n",
        "\n",
        "    for package, import_name in packages.items():\n",
        "        try:\n",
        "            __import__(import_name.split('.')[0])\n",
        "            print(f\"‚úì {package} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.run(['pip', 'install', '-q', package], check=True)\n",
        "\n",
        "install_dependencies()\n",
        "\n",
        "import whisper\n",
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# ============================================\n",
        "# GPU OPTIMIZATION AND MONITORING\n",
        "# ============================================\n",
        "def get_gpu_memory():\n",
        "    \"\"\"Get current GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "def optimize_gpu_settings():\n",
        "    \"\"\"Optimize GPU settings for maximum performance\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Get GPU info\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "        print(f\"\\nüéÆ GPU Optimization\")\n",
        "        print(f\"   Device: {gpu_name}\")\n",
        "        print(f\"   Total Memory: {total_memory:.1f} GB\")\n",
        "\n",
        "        # Calculate optimal batch size based on available memory\n",
        "        # Reserve 10GB for model, use rest for batching\n",
        "        available_for_batch = total_memory - 10\n",
        "        optimal_batch_size = int(available_for_batch * 4)  # 4 samples per GB\n",
        "\n",
        "        print(f\"   Optimal batch size: {optimal_batch_size}\")\n",
        "        return optimal_batch_size\n",
        "    return 8\n",
        "\n",
        "BATCH_SIZE = optimize_gpu_settings()\n",
        "\n",
        "# ============================================\n",
        "# ADVANCED AUDIO PREPROCESSING\n",
        "# ============================================\n",
        "def preprocess_audio(audio_path, output_path=None):\n",
        "    \"\"\"Preprocess audio for optimal transcription\"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "    # Normalize audio levels\n",
        "    normalized = audio.normalize()\n",
        "\n",
        "    # Remove silence at beginning and end\n",
        "    trimmed = normalized.strip_silence(silence_len=1000, silence_thresh=-40)\n",
        "\n",
        "    if output_path:\n",
        "        trimmed.export(output_path, format=\"wav\")\n",
        "        return output_path\n",
        "\n",
        "    return audio_path\n",
        "\n",
        "# ============================================\n",
        "# SMART TRANSCRIPTION WITH QUALITY CHECKS\n",
        "# ============================================\n",
        "def transcribe_with_quality_check(model, audio_path, **params):\n",
        "    \"\"\"Transcribe with intelligent quality checking and retry logic\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # First attempt with optimal settings\n",
        "    print(\"   üéØ Transcribing with optimal settings...\")\n",
        "\n",
        "    # Adjust chunk_length based on file size\n",
        "    file_size_mb = os.path.getsize(audio_path) / (1024**2)\n",
        "    if file_size_mb > 200:  # Large file\n",
        "        chunk_length = 45\n",
        "    else:\n",
        "        chunk_length = 60  # Can handle larger chunks for smaller files\n",
        "\n",
        "    result = model.transcribe(\n",
        "        audio_path,\n",
        "        language=None,  # Auto-detect\n",
        "        task='transcribe',\n",
        "        verbose=False,\n",
        "        fp16=(device.type == 'cuda'),\n",
        "        chunk_length=chunk_length,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    # Quality check\n",
        "    segments = result.get('segments', [])\n",
        "\n",
        "    # Check for repetitions\n",
        "    repetition_score = calculate_repetition_score(segments)\n",
        "    if repetition_score > 0.3:  # 30% repetition threshold\n",
        "        print(f\"   ‚ö†Ô∏è  High repetition detected ({repetition_score:.1%}), applying fixes...\")\n",
        "\n",
        "        # Retry with different parameters\n",
        "        params_copy = params.copy()\n",
        "        params_copy['condition_on_previous_text'] = False\n",
        "        params_copy['temperature'] = 0.8\n",
        "        params_copy['beam_size'] = 5\n",
        "\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            language=None,\n",
        "            task='transcribe',\n",
        "            verbose=False,\n",
        "            fp16=(device.type == 'cuda'),\n",
        "            chunk_length=30,  # Smaller chunks for problem audio\n",
        "            batch_size=BATCH_SIZE,\n",
        "            **params_copy\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "def calculate_repetition_score(segments):\n",
        "    \"\"\"Calculate how repetitive the transcription is\"\"\"\n",
        "    if len(segments) < 10:\n",
        "        return 0.0\n",
        "\n",
        "    texts = [seg.get('text', '').strip().lower() for seg in segments]\n",
        "    repetitions = 0\n",
        "\n",
        "    for i in range(1, len(texts)):\n",
        "        if texts[i] == texts[i-1] and texts[i]:\n",
        "            repetitions += 1\n",
        "\n",
        "    return repetitions / len(texts)\n",
        "\n",
        "# ============================================\n",
        "# ENHANCED SPEAKER DIARIZATION\n",
        "# ============================================\n",
        "def perform_speaker_diarization(audio_path, whisper_result, hf_token):\n",
        "    \"\"\"Enhanced speaker diarization with better error handling\"\"\"\n",
        "    if not hf_token:\n",
        "        print(\"   ‚ö†Ô∏è  No HF token - skipping speaker diarization\")\n",
        "        return whisper_result\n",
        "\n",
        "    try:\n",
        "        print(\"   üé≠ Running enhanced speaker diarization...\")\n",
        "\n",
        "        # Load audio with torchaudio\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Initialize diarization pipeline\n",
        "        pipeline = Pipeline.from_pretrained(\n",
        "            \"pyannote/speaker-diarization-3.1\",\n",
        "            use_auth_token=hf_token\n",
        "        )\n",
        "\n",
        "        # Move pipeline to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            pipeline.to(torch.device('cuda'))\n",
        "\n",
        "        # Run diarization with optimized parameters\n",
        "        diarization = pipeline({\n",
        "            \"waveform\": waveform,\n",
        "            \"sample_rate\": sample_rate\n",
        "        }, num_speakers=None, min_speakers=2, max_speakers=10)\n",
        "\n",
        "        # Create speaker timeline\n",
        "        speaker_timeline = []\n",
        "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "            speaker_timeline.append({\n",
        "                'start': turn.start,\n",
        "                'end': turn.end,\n",
        "                'speaker': f\"SPEAKER_{speaker.split('_')[-1].zfill(2)}\"\n",
        "            })\n",
        "\n",
        "        # Assign speakers to whisper segments\n",
        "        for segment in whisper_result['segments']:\n",
        "            seg_start = segment['start']\n",
        "            seg_end = segment['end']\n",
        "            seg_mid = (seg_start + seg_end) / 2\n",
        "\n",
        "            # Find best matching speaker\n",
        "            best_speaker = 'SPEAKER_00'\n",
        "            best_overlap = 0\n",
        "\n",
        "            for spk_segment in speaker_timeline:\n",
        "                # Calculate overlap\n",
        "                overlap_start = max(seg_start, spk_segment['start'])\n",
        "                overlap_end = min(seg_end, spk_segment['end'])\n",
        "                overlap = max(0, overlap_end - overlap_start)\n",
        "\n",
        "                if overlap > best_overlap:\n",
        "                    best_overlap = overlap\n",
        "                    best_speaker = spk_segment['speaker']\n",
        "\n",
        "            segment['speaker'] = best_speaker\n",
        "\n",
        "        # Count speakers\n",
        "        unique_speakers = len(set(seg.get('speaker', 'SPEAKER_00')\n",
        "                                 for seg in whisper_result['segments']))\n",
        "        print(f\"   ‚úì Diarization complete: {unique_speakers} speakers identified\")\n",
        "\n",
        "        return whisper_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Diarization failed: {str(e)}\")\n",
        "        # Fallback: simple speaker change detection\n",
        "        return simple_speaker_detection(whisper_result)\n",
        "\n",
        "def simple_speaker_detection(whisper_result):\n",
        "    \"\"\"Simple speaker change detection based on pauses\"\"\"\n",
        "    segments = whisper_result.get('segments', [])\n",
        "    current_speaker = 0\n",
        "\n",
        "    for i, segment in enumerate(segments):\n",
        "        if i > 0:\n",
        "            # Check for long pause (potential speaker change)\n",
        "            pause = segment['start'] - segments[i-1]['end']\n",
        "            if pause > 2.0:  # 2 second pause\n",
        "                current_speaker = 1 - current_speaker  # Toggle between 0 and 1\n",
        "\n",
        "        segment['speaker'] = f\"SPEAKER_{current_speaker:02d}\"\n",
        "\n",
        "    return whisper_result\n",
        "\n",
        "# ============================================\n",
        "# POST-PROCESSING FOR QUALITY IMPROVEMENT\n",
        "# ============================================\n",
        "def post_process_transcript(segments):\n",
        "    \"\"\"Clean up common transcription issues\"\"\"\n",
        "    processed_segments = []\n",
        "\n",
        "    for segment in segments:\n",
        "        text = segment.get('text', '').strip()\n",
        "\n",
        "        # Fix spelled-out acronyms\n",
        "        text = fix_spelled_acronyms(text)\n",
        "\n",
        "        # Fix URLs\n",
        "        text = fix_urls(text)\n",
        "\n",
        "        # Remove excessive filler words\n",
        "        text = reduce_fillers(text)\n",
        "\n",
        "        # Fix punctuation\n",
        "        text = fix_punctuation(text)\n",
        "\n",
        "        if text:  # Only keep non-empty segments\n",
        "            segment['text'] = text\n",
        "            processed_segments.append(segment)\n",
        "\n",
        "    return merge_short_segments(processed_segments)\n",
        "\n",
        "def fix_spelled_acronyms(text):\n",
        "    \"\"\"Fix spelled out acronyms like 'h m h c' -> 'HMHC'\"\"\"\n",
        "    # Pattern for single letters separated by spaces\n",
        "    pattern = r'\\b([a-zA-Z])\\s+(?=[a-zA-Z]\\s+|[a-zA-Z]\\b)'\n",
        "\n",
        "    def replace_match(match):\n",
        "        # Check if this looks like an acronym (2-6 letters)\n",
        "        start = match.start()\n",
        "        # Look ahead to see how many single letters follow\n",
        "        remaining = text[start:]\n",
        "        letters = re.findall(r'^([a-zA-Z]\\s+)+[a-zA-Z]\\b', remaining)\n",
        "        if letters:\n",
        "            acronym = re.sub(r'\\s+', '', letters[0]).upper()\n",
        "            if 2 <= len(acronym) <= 6:\n",
        "                return acronym\n",
        "        return match.group(0)\n",
        "\n",
        "    # Find and replace potential acronyms\n",
        "    text = re.sub(pattern, replace_match, text)\n",
        "\n",
        "    # Specific common patterns\n",
        "    text = re.sub(r'\\bh\\s*m\\s*h\\s*c\\b', 'HMHC', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\bc\\s*a\\b', 'CA', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def fix_urls(text):\n",
        "    \"\"\"Fix broken up URLs\"\"\"\n",
        "    # Fix domain extensions\n",
        "    text = re.sub(r'\\.\\s*c\\s*a\\b', '.ca', text)\n",
        "    text = re.sub(r'\\.\\s*c\\s*o\\s*m\\b', '.com', text)\n",
        "\n",
        "    # Fix common URL patterns\n",
        "    text = re.sub(r'(\\w+)\\s*\\.\\s*(\\w+)\\s*\\.\\s*(\\w+)', r'\\1.\\2.\\3', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def reduce_fillers(text):\n",
        "    \"\"\"Reduce excessive filler words while keeping some for naturalness\"\"\"\n",
        "    fillers = ['um', 'uh', 'like', 'you know']\n",
        "    for filler in fillers:\n",
        "        # Replace multiple occurrences with single\n",
        "        pattern = rf'\\b{filler}\\b(\\s+\\b{filler}\\b)+'\n",
        "        text = re.sub(pattern, filler, text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "def fix_punctuation(text):\n",
        "    \"\"\"Fix common punctuation issues\"\"\"\n",
        "    # Add periods to end of segments if missing\n",
        "    if text and text[-1] not in '.!?':\n",
        "        text += '.'\n",
        "\n",
        "    # Fix multiple punctuation\n",
        "    text = re.sub(r'([.!?])\\1+', r'\\1', text)\n",
        "\n",
        "    # Capitalize after sentence endings\n",
        "    text = re.sub(r'([.!?]\\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def merge_short_segments(segments):\n",
        "    \"\"\"Merge very short segments with adjacent ones\"\"\"\n",
        "    if not segments:\n",
        "        return segments\n",
        "\n",
        "    merged = []\n",
        "    current = segments[0].copy()\n",
        "\n",
        "    for next_seg in segments[1:]:\n",
        "        # Check if should merge\n",
        "        current_text = current.get('text', '').strip()\n",
        "        next_text = next_seg.get('text', '').strip()\n",
        "\n",
        "        # Merge if current segment is very short or ends mid-sentence\n",
        "        if (len(current_text.split()) < 5 or\n",
        "            (current_text and current_text[-1] not in '.!?' and\n",
        "             next_seg['start'] - current['end'] < 1.0)):\n",
        "            # Merge\n",
        "            current['text'] = current_text + ' ' + next_text\n",
        "            current['end'] = next_seg['end']\n",
        "            if 'speaker' in next_seg and next_seg['speaker'] == current.get('speaker'):\n",
        "                continue\n",
        "        else:\n",
        "            merged.append(current)\n",
        "            current = next_seg.copy()\n",
        "\n",
        "    merged.append(current)\n",
        "    return merged\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING PIPELINE\n",
        "# ============================================\n",
        "def process_audio_file(model, audio_path, hf_token):\n",
        "    \"\"\"Complete processing pipeline for a single audio file\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_ultimate.txt\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        file_size_mb = os.path.getsize(audio_path) / (1024**2)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing: {os.path.basename(audio_path)}\")\n",
        "        print(f\"Size: {file_size_mb:.1f} MB\")\n",
        "\n",
        "        # Show GPU memory before processing\n",
        "        alloc, reserved, total = get_gpu_memory()\n",
        "        print(f\"GPU Memory: {alloc:.1f}/{total:.1f} GB used\")\n",
        "\n",
        "        # Preprocess audio\n",
        "        print(\"   üéµ Preprocessing audio...\")\n",
        "        preprocessed_path = audio_path  # Skip preprocessing for now\n",
        "\n",
        "        # Transcribe with quality checks\n",
        "        result = transcribe_with_quality_check(\n",
        "            model, preprocessed_path, **TRANSCRIPTION_PARAMS\n",
        "        )\n",
        "\n",
        "        # Get language info\n",
        "        detected_lang = result.get('language', 'en')\n",
        "        print(f\"   üåç Detected language: {detected_lang}\")\n",
        "\n",
        "        # Speaker diarization\n",
        "        result = perform_speaker_diarization(preprocessed_path, result, hf_token)\n",
        "\n",
        "        # Post-process segments\n",
        "        print(\"   ‚ú® Post-processing transcript...\")\n",
        "        segments = post_process_transcript(result.get('segments', []))\n",
        "\n",
        "        # Calculate stats\n",
        "        duration = segments[-1]['end'] if segments else 0\n",
        "        process_time = time.time() - start_time\n",
        "        speed_factor = duration / process_time if process_time > 0 else 0\n",
        "\n",
        "        # Count speakers\n",
        "        speakers = sorted(set(seg.get('speaker', 'SPEAKER_00') for seg in segments))\n",
        "\n",
        "        print(f\"\\n   ‚úÖ Processing complete!\")\n",
        "        print(f\"      Duration: {duration/60:.1f} minutes\")\n",
        "        print(f\"      Process time: {process_time:.1f} seconds\")\n",
        "        print(f\"      Speed: {speed_factor:.1f}x realtime\")\n",
        "        print(f\"      Speakers: {len(speakers)}\")\n",
        "\n",
        "        # Save enhanced transcript\n",
        "        save_enhanced_transcript(output_file, segments, {\n",
        "            'filename': os.path.basename(audio_path),\n",
        "            'duration': duration,\n",
        "            'language': detected_lang,\n",
        "            'speakers': speakers,\n",
        "            'model': WHISPER_MODEL\n",
        "        })\n",
        "\n",
        "        print(f\"   üíæ Saved: {os.path.basename(output_file)}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "def save_enhanced_transcript(output_file, segments, metadata):\n",
        "    \"\"\"Save transcript in enhanced format\"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        # Header\n",
        "        f.write(\"# ENHANCED TRANSCRIPT\\n\")\n",
        "        f.write(f\"# Model: Whisper {metadata['model']}\\n\")\n",
        "        f.write(f\"# File: {metadata['filename']}\\n\")\n",
        "        f.write(f\"# Duration: {metadata['duration']/60:.1f} minutes\\n\")\n",
        "        f.write(f\"# Language: {metadata['language']}\\n\")\n",
        "        f.write(f\"# Speakers: {', '.join(metadata['speakers'])}\\n\")\n",
        "        f.write(f\"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "        # Write segments grouped by speaker\n",
        "        current_speaker = None\n",
        "\n",
        "        for segment in segments:\n",
        "            speaker = segment.get('speaker', 'SPEAKER_00')\n",
        "            start = segment['start']\n",
        "            end = segment['end']\n",
        "            text = segment.get('text', '').strip()\n",
        "\n",
        "            if text:\n",
        "                # Add speaker header if changed\n",
        "                if speaker != current_speaker:\n",
        "                    f.write(f\"\\n[{speaker}]\\n\")\n",
        "                    current_speaker = speaker\n",
        "\n",
        "                # Write timestamp and text\n",
        "                f.write(f\"[{format_timestamp(start)} ‚Üí {format_timestamp(end)}] {text}\\n\")\n",
        "\n",
        "def format_timestamp(seconds):\n",
        "    \"\"\"Format timestamp as MM:SS.SS\"\"\"\n",
        "    minutes = int(seconds // 60)\n",
        "    secs = seconds % 60\n",
        "    return f\"{minutes:02d}:{secs:05.2f}\"\n",
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================\n",
        "def main():\n",
        "    \"\"\"Main execution function with all optimizations\"\"\"\n",
        "\n",
        "    # Get HF token\n",
        "    try:\n",
        "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "        print(\"‚úì HuggingFace token loaded\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è  No HF token - speaker diarization will be limited\")\n",
        "        HF_TOKEN = None\n",
        "\n",
        "    # Find WAV files\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    if not wav_files:\n",
        "        print(f\"‚ùå No WAV files found in {INPUT_PATH}\")\n",
        "        return\n",
        "\n",
        "    # Check which need processing\n",
        "    remaining_files = []\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "        if not any(os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "                  for suffix in ['_ultimate.txt', '_enhanced.txt', '_large-v3.txt']):\n",
        "            remaining_files.append(wav_file)\n",
        "\n",
        "    print(f\"\\nüìä Status: {len(remaining_files)} files to process\")\n",
        "\n",
        "    if not remaining_files:\n",
        "        print(\"‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Load model once\n",
        "    print(f\"\\n‚è≥ Loading Whisper {WHISPER_MODEL}...\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load with optimizations\n",
        "    with torch.inference_mode():\n",
        "        model = whisper.load_model(WHISPER_MODEL, device=device)\n",
        "\n",
        "    print(\"‚úì Model loaded and optimized\")\n",
        "\n",
        "    # Process files (3 at a time for large-v3)\n",
        "    files_to_process = remaining_files[:3]\n",
        "    successful = 0\n",
        "\n",
        "    for audio_file in files_to_process:\n",
        "        if process_audio_file(model, audio_file, HF_TOKEN):\n",
        "            successful += 1\n",
        "\n",
        "        # Cool down between files\n",
        "        if audio_file != files_to_process[-1]:\n",
        "            print(\"\\n‚è≥ Cooling down for 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    # Cleanup\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"‚úÖ SESSION COMPLETE\")\n",
        "    print(f\"   Processed: {successful}/{len(files_to_process)} files\")\n",
        "    print(f\"   Remaining: {len(remaining_files) - len(files_to_process)} files\")\n",
        "    if len(remaining_files) > len(files_to_process):\n",
        "        print(\"\\nüí° Run again to process remaining files\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN THE SYSTEM\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# USAGE NOTES\n",
        "# ============================================\n",
        "\"\"\"\n",
        "KEY IMPROVEMENTS IN THIS VERSION:\n",
        "\n",
        "1. GPU OPTIMIZATION:\n",
        "   - Dynamic batch sizing based on available memory\n",
        "   - TF32 and cuDNN optimization enabled\n",
        "   - Larger chunk processing (45-60 seconds)\n",
        "\n",
        "2. TRANSCRIPTION QUALITY:\n",
        "   - Smart repetition detection and correction\n",
        "   - Intelligent conditioning (on by default, off for problems)\n",
        "   - Post-processing fixes acronyms, URLs, and punctuation\n",
        "\n",
        "3. SPEAKER DIARIZATION:\n",
        "   - Robust Pyannote implementation\n",
        "   - Fallback to pause-based detection\n",
        "   - Better speaker assignment logic\n",
        "\n",
        "4. PERFORMANCE:\n",
        "   - Preprocesses audio for better results\n",
        "   - Concurrent processing capability\n",
        "   - Memory-efficient segment merging\n",
        "\n",
        "To get the best results:\n",
        "1. Add HF_TOKEN to Colab secrets for diarization\n",
        "2. Ensure GPU runtime is enabled\n",
        "3. Delete any corrupted previous transcripts\n",
        "4. Run this complete system\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Drive File Storage\n"
      ],
      "metadata": {
        "id": "tvVwNwhHXQNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE WORKING TRANSCRIPTION SYSTEM\n",
        "# Based on the approach that was working\n",
        "\n",
        "# ============================================\n",
        "# CELL 1: Complete Setup and Processing\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "# if not os.path.exists('/content/drive'):\n",
        "#    drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MUIMLVU4Wtzj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 2: File Discovery and Status\n",
        "# ============================================\n",
        "\"\"\"\n",
        "Run this to see what files need processing\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths - adjust these to your actual locations\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "\n",
        "# Get list of audio files\n",
        "mp4_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.mp4\")))\n",
        "wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "all_audio_files = mp4_files + wav_files\n",
        "\n",
        "print(f\"\\nüìÅ Found {len(all_audio_files)} audio files:\")\n",
        "for i, f in enumerate(all_audio_files, 1):\n",
        "    print(f\"  {i}. {os.path.basename(f)}\")\n",
        "\n",
        "# Check what's already been transcribed\n",
        "completed_files = []\n",
        "remaining_files = []\n",
        "\n",
        "for audio_file in all_audio_files:\n",
        "    base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
        "    transcript_path = os.path.join(OUTPUT_PATH, f\"{base_name}_transcript.txt\")\n",
        "\n",
        "    if os.path.exists(transcript_path):\n",
        "        completed_files.append(audio_file)\n",
        "    else:\n",
        "        remaining_files.append(audio_file)\n",
        "\n",
        "print(f\"\\nüìä Status:\")\n",
        "print(f\"  ‚úì Completed: {len(completed_files)}\")\n",
        "print(f\"  ‚è≥ Remaining: {len(remaining_files)}\")\n",
        "\n",
        "if remaining_files:\n",
        "    print(f\"\\nüéØ Next file to process: {os.path.basename(remaining_files[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7h4HOEbWvno",
        "outputId": "a36521ba-b00a-444e-9cbf-3d3b48d42780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "üìÅ Found 8 audio files:\n",
            "  1. Call Recording - 13Mar2025 1200 BPA.wav\n",
            "  2. Call Recording - 13Mar25 1130 BK.wav\n",
            "  3. Call Recording - 13Mar25 1300 HB.wav\n",
            "  4. Call Recording - 19Mar2025 0800 JD.wav\n",
            "  5. Call Recording - 19Mar25 0900 - AJ.wav\n",
            "  6. Call Recording - 19Mar25 1730 - MO.wav\n",
            "  7. Call Recording - 20Mar2025 1200 LN.wav\n",
            "  8. Call Recording - 26Mar2025 0830 SA.wav\n",
            "\n",
            "üìä Status:\n",
            "  ‚úì Completed: 0\n",
            "  ‚è≥ Remaining: 8\n",
            "\n",
            "üéØ Next file to process: Call Recording - 13Mar2025 1200 BPA.wav\n"
          ]
        }
      ]
    }
  ]
}