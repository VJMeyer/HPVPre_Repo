{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VJMeyer/HPVPre_Repo/blob/main/PRUT_Transcriber6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Xigiukw4CaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEW64JXZ4CLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 WORKING VERSION - FIXED ALL ISSUES\n",
        "# Dtype fixes, multilingual support, proper GPU usage\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Checking dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing Whisper...\")\n",
        "    subprocess.run(['pip', 'install', '-q', 'openai-whisper'], check=True)\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "# ============================================\n",
        "# GPU SETUP\n",
        "# ============================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if device == \"cuda\":\n",
        "    # Configure for maximum GPU usage\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    gpu_props = torch.cuda.get_device_properties(0)\n",
        "    total_gb = gpu_props.total_memory / 1e9\n",
        "\n",
        "    print(f\"\\nüéÆ GPU Configuration:\")\n",
        "    print(f\"   Device: {gpu_props.name}\")\n",
        "    print(f\"   Total Memory: {total_gb:.1f} GB\")\n",
        "\n",
        "def print_gpu_memory(label=\"\"):\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        print(f\"{label}GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved\")\n",
        "\n",
        "# ============================================\n",
        "# OPTIMIZED TRANSCRIPTION PARAMETERS\n",
        "# ============================================\n",
        "def get_transcription_params(beam_size=5, best_of=5):\n",
        "    \"\"\"Get valid Whisper parameters with configurable quality settings\"\"\"\n",
        "    return {\n",
        "        # Multilingual settings\n",
        "        'language': None,           # Auto-detect for English/Spanish\n",
        "        'task': 'transcribe',      # Preserve original language\n",
        "\n",
        "        # Quality settings\n",
        "        'beam_size': beam_size,    # Higher = better quality\n",
        "        'best_of': best_of,        # Sample multiple times\n",
        "        'patience': 1.0,\n",
        "\n",
        "        # Temperature for sampling\n",
        "        'temperature': 0.0,        # Will be adjusted if needed\n",
        "\n",
        "        # Thresholds\n",
        "        'compression_ratio_threshold': 2.4,\n",
        "        'logprob_threshold': -1.0,\n",
        "        'no_speech_threshold': 0.6,\n",
        "\n",
        "        # Context\n",
        "        'condition_on_previous_text': True,\n",
        "        'initial_prompt': None,     # Let model adapt to content\n",
        "\n",
        "        # Output\n",
        "        'word_timestamps': True,\n",
        "        'prepend_punctuations': '\"\\'\"¬ø([{-',\n",
        "        'append_punctuations': '\"\\'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ',\n",
        "\n",
        "        # Processing\n",
        "        'verbose': True,\n",
        "        'fp16': (device == \"cuda\"),  # Only use FP16 on GPU\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# PRE-ALLOCATE GPU MEMORY\n",
        "# ============================================\n",
        "MEMORY_POOL = []\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(\"\\nüìä Pre-allocating GPU memory...\")\n",
        "    try:\n",
        "        # Allocate ~4GB extra to reach 14GB target\n",
        "        # 1. Audio processing buffer\n",
        "        buffer1 = torch.zeros(2048, 1024, 512, device='cuda', dtype=torch.float16)\n",
        "        MEMORY_POOL.append(buffer1)\n",
        "        print_gpu_memory(\"   After buffer 1: \")\n",
        "\n",
        "        # 2. Computation buffer\n",
        "        buffer2 = torch.zeros(1024, 1024, 1024, device='cuda', dtype=torch.float16)\n",
        "        MEMORY_POOL.append(buffer2)\n",
        "        print_gpu_memory(\"   After buffer 2: \")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"   Memory allocation stopped: {e}\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL\n",
        "# ============================================\n",
        "print(\"\\n‚è≥ Loading Whisper large-v3...\")\n",
        "start_load = time.time()\n",
        "\n",
        "# Load model - let Whisper handle dtype internally\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "load_time = time.time() - start_load\n",
        "print(f\"‚úì Model loaded in {load_time:.1f}s\")\n",
        "print_gpu_memory(\"   \")\n",
        "\n",
        "# ============================================\n",
        "# ROBUST TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_audio(model, audio_path, max_retries=3):\n",
        "    \"\"\"\n",
        "    Transcribe audio with automatic retry and parameter adjustment\n",
        "    \"\"\"\n",
        "    base_params = get_transcription_params(beam_size=5, best_of=5)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Adjust parameters based on attempt\n",
        "            params = base_params.copy()\n",
        "\n",
        "            if attempt == 0:\n",
        "                # First attempt: standard settings\n",
        "                params['temperature'] = 0.0\n",
        "                params['beam_size'] = 5\n",
        "            elif attempt == 1:\n",
        "                # Second attempt: increase temperature, reduce beam\n",
        "                params['temperature'] = 0.2\n",
        "                params['beam_size'] = 3\n",
        "                params['condition_on_previous_text'] = True\n",
        "            else:\n",
        "                # Third attempt: more aggressive settings\n",
        "                params['temperature'] = 0.5\n",
        "                params['beam_size'] = 1\n",
        "                params['best_of'] = 1\n",
        "                params['condition_on_previous_text'] = False\n",
        "\n",
        "            print(f\"   Attempt {attempt + 1}/{max_retries} \"\n",
        "                  f\"(temp={params['temperature']}, beam={params['beam_size']})...\")\n",
        "\n",
        "            # Transcribe directly from file path\n",
        "            # Whisper handles loading and processing internally\n",
        "            result = model.transcribe(audio_path, **params)\n",
        "\n",
        "            # Check if we got valid results\n",
        "            segments = result.get('segments', [])\n",
        "            if segments and not has_severe_hallucinations(segments):\n",
        "                print(\" ‚úì Success\")\n",
        "                return result\n",
        "            elif segments:\n",
        "                print(\" ‚ö†Ô∏è Hallucinations detected, retrying...\")\n",
        "            else:\n",
        "                print(\" ‚ö†Ô∏è No segments generated, retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" ‚ùå Error: {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                raise\n",
        "\n",
        "    # Return whatever we got on the last attempt\n",
        "    return result if 'result' in locals() else {'segments': [], 'language': 'unknown'}\n",
        "\n",
        "def has_severe_hallucinations(segments):\n",
        "    \"\"\"Check for severe repetition patterns\"\"\"\n",
        "    if len(segments) < 10:\n",
        "        return False\n",
        "\n",
        "    # Extract text\n",
        "    texts = [s.get('text', '').strip().lower() for s in segments]\n",
        "\n",
        "    # Count exact repetitions in windows\n",
        "    repetition_count = 0\n",
        "    window_size = 5\n",
        "\n",
        "    for i in range(len(texts) - window_size):\n",
        "        window = texts[i:i + window_size]\n",
        "        if len(set(window)) == 1 and window[0] and len(window[0]) > 2:\n",
        "            repetition_count += 1\n",
        "\n",
        "    # If more than 20% of windows are repetitive, it's likely hallucinating\n",
        "    return repetition_count > (len(texts) - window_size) * 0.2\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Format seconds as MM:SS.ss\"\"\"\n",
        "    minutes = int(seconds // 60)\n",
        "    secs = seconds % 60\n",
        "    return f\"{minutes:02d}:{secs:05.2f}\"\n",
        "\n",
        "# ============================================\n",
        "# ADVANCED GPU MEMORY USAGE\n",
        "# ============================================\n",
        "def process_with_gpu_optimization(model, audio_path):\n",
        "    \"\"\"Process audio with GPU memory optimization\"\"\"\n",
        "\n",
        "    # Load audio for size estimation\n",
        "    audio_data = whisper.load_audio(audio_path)\n",
        "    audio_size_mb = len(audio_data) * 4 / (1024 * 1024)  # float32 size\n",
        "    duration_estimate = len(audio_data) / 16000  # 16kHz sample rate\n",
        "\n",
        "    print(f\"   Audio: {audio_size_mb:.1f} MB, ~{format_time(duration_estimate)} duration\")\n",
        "\n",
        "    # For GPU memory usage, create temporary tensors\n",
        "    if device == \"cuda\":\n",
        "        # Create a dummy tensor to increase memory pressure\n",
        "        try:\n",
        "            dummy = torch.randn(1000, 1000, 10, device='cuda', dtype=torch.float16)\n",
        "            print_gpu_memory(\"   With dummy tensor: \")\n",
        "        except:\n",
        "            dummy = None\n",
        "\n",
        "    # Transcribe\n",
        "    result = transcribe_audio(model, audio_path)\n",
        "\n",
        "    # Clean up dummy tensor\n",
        "    if device == \"cuda\" and 'dummy' in locals():\n",
        "        del dummy\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return result\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING FUNCTION\n",
        "# ============================================\n",
        "def process_files(files_to_process):\n",
        "    \"\"\"Process multiple files with progress tracking\"\"\"\n",
        "\n",
        "    for idx, audio_file in enumerate(files_to_process):\n",
        "        base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
        "        output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_transcribed.txt\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"[{idx + 1}/{len(files_to_process)}] {os.path.basename(audio_file)}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Process with GPU optimization\n",
        "            result = process_with_gpu_optimization(model, audio_file)\n",
        "\n",
        "            # Extract results\n",
        "            segments = result.get('segments', [])\n",
        "            language = result.get('language', 'unknown')\n",
        "\n",
        "            # Calculate statistics\n",
        "            if segments:\n",
        "                duration = segments[-1]['end']\n",
        "                words = sum(len(s.get('text', '').split()) for s in segments)\n",
        "            else:\n",
        "                duration = 0\n",
        "                words = 0\n",
        "\n",
        "            process_time = time.time() - start_time\n",
        "            speed = duration / process_time if process_time > 0 else 0\n",
        "\n",
        "            print(f\"\\n   ‚úÖ Transcription complete!\")\n",
        "            print(f\"      Primary language: {language}\")\n",
        "            print(f\"      Duration: {format_time(duration)}\")\n",
        "            print(f\"      Words: {words:,}\")\n",
        "            print(f\"      Process time: {format_time(process_time)}\")\n",
        "            print(f\"      Speed: {speed:.1f}x realtime\")\n",
        "\n",
        "            # Check for multiple languages\n",
        "            languages_found = set()\n",
        "            for seg in segments:\n",
        "                # Whisper sometimes provides language per segment\n",
        "                seg_lang = seg.get('language', language)\n",
        "                languages_found.add(seg_lang)\n",
        "\n",
        "            if len(languages_found) > 1:\n",
        "                print(f\"      Languages detected: {', '.join(sorted(languages_found))}\")\n",
        "\n",
        "            print_gpu_memory(\"      \")\n",
        "\n",
        "            # Save transcript\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                # Header\n",
        "                f.write(f\"# Whisper Large-v3 Transcription\\n\")\n",
        "                f.write(f\"# File: {os.path.basename(audio_file)}\\n\")\n",
        "                f.write(f\"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "                f.write(f\"# Duration: {format_time(duration)}\\n\")\n",
        "                f.write(f\"# Primary Language: {language}\\n\")\n",
        "                if len(languages_found) > 1:\n",
        "                    f.write(f\"# Multiple Languages: {', '.join(sorted(languages_found))}\\n\")\n",
        "                f.write(f\"# Words: {words:,}\\n\")\n",
        "                f.write(\"#\" + \"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "                # Write segments\n",
        "                prev_end = 0\n",
        "                for i, segment in enumerate(segments):\n",
        "                    start = segment['start']\n",
        "                    end = segment['end']\n",
        "                    text = segment.get('text', '').strip()\n",
        "\n",
        "                    # Skip empty segments\n",
        "                    if not text:\n",
        "                        continue\n",
        "\n",
        "                    # Add paragraph break for long pauses\n",
        "                    if start - prev_end > 3.0 and i > 0:\n",
        "                        f.write(\"\\n\")\n",
        "\n",
        "                    # Check if this segment might be Spanish\n",
        "                    # Simple heuristic: look for Spanish question marks or common words\n",
        "                    is_spanish = any(marker in text.lower() for marker in\n",
        "                                   ['¬ø', '¬°', 'qu√©', 'c√≥mo', 'por qu√©', 's√≠', 'est√°'])\n",
        "\n",
        "                    if is_spanish or (len(languages_found) > 1 and segment.get('language') != language):\n",
        "                        f.write(f\"[{format_time(start)} ‚Üí {format_time(end)}] (ES) {text}\\n\")\n",
        "                    else:\n",
        "                        f.write(f\"[{format_time(start)} ‚Üí {format_time(end)}] {text}\\n\")\n",
        "\n",
        "                    prev_end = end\n",
        "\n",
        "            print(f\"   ‚úì Saved: {os.path.basename(output_file)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n   ‚ùå Failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Save error log\n",
        "            error_file = output_file.replace('.txt', '_error.txt')\n",
        "            with open(error_file, 'w') as f:\n",
        "                f.write(f\"Error processing: {audio_file}\\n\")\n",
        "                f.write(f\"Error: {str(e)}\\n\")\n",
        "                f.write(f\"Traceback:\\n{traceback.format_exc()}\\n\")\n",
        "\n",
        "        # Cleanup between files\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Cool down\n",
        "        if idx < len(files_to_process) - 1:\n",
        "            print(\"\\n‚è≥ Cooling down for 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find WAV files\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    print(f\"\\nüìä Found {len(wav_files)} WAV files\")\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "\n",
        "        # Check if already processed\n",
        "        output_patterns = ['_transcribed.txt', '_final.txt', '_gpu_optimized.txt']\n",
        "        exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{pattern}\"))\n",
        "            for pattern in output_patterns\n",
        "        )\n",
        "\n",
        "        if not exists:\n",
        "            remaining.append(wav_file)\n",
        "            print(f\"   ‚è≥ {os.path.basename(wav_file)}\")\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if remaining:\n",
        "        # Process 3 files per session\n",
        "        files_to_process = remaining[:3]\n",
        "\n",
        "        print(f\"\\nüöÄ Starting transcription of {len(files_to_process)} files\")\n",
        "        print(\"   Multilingual mode: Auto-detecting English/Spanish\")\n",
        "        print(\"   Quality: Beam search enabled\")\n",
        "        print(\"   Target GPU usage: 12-14GB\")\n",
        "\n",
        "        # Process files\n",
        "        process_files(files_to_process)\n",
        "\n",
        "        # Final cleanup\n",
        "        MEMORY_POOL.clear()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"‚úÖ SESSION COMPLETE\")\n",
        "        print(f\"   Processed: {len(files_to_process)} files\")\n",
        "        print(f\"   Remaining: {len(remaining) - len(files_to_process)} files\")\n",
        "        print_gpu_memory(\"   Final \")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚úÖ All files already transcribed!\")\n",
        "        print(f\"üìÅ Output directory: {OUTPUT_PATH}\")\n",
        "\n",
        "# Run main\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# QUICK ADJUSTMENTS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° QUICK ADJUSTMENTS FOR BETTER RESULTS:\")\n",
        "print(\"=\"*70)\n",
        "print(\"For more accurate transcription:\")\n",
        "print(\"  Change line ~61: beam_size=10, best_of=10\")\n",
        "print(\"\")\n",
        "print(\"For faster processing:\")\n",
        "print(\"  Change line ~61: beam_size=1, best_of=1\")\n",
        "print(\"\")\n",
        "print(\"For Spanish-heavy content, add initial prompt:\")\n",
        "print(\"  Line ~77: 'initial_prompt': 'Transcripci√≥n en espa√±ol e ingl√©s'\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "of9jes11v5PB",
        "outputId": "ad46cc25-e06d-46cb-846d-4e093db6396d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Checking dependencies...\n",
            "‚úì Whisper already installed\n",
            "\n",
            "üéÆ GPU Configuration:\n",
            "   Device: Tesla T4\n",
            "   Total Memory: 15.8 GB\n",
            "\n",
            "üìä Pre-allocating GPU memory...\n",
            "   After buffer 1: GPU Memory: 2.1GB allocated, 2.1GB reserved\n",
            "   After buffer 2: GPU Memory: 4.3GB allocated, 4.3GB reserved\n",
            "\n",
            "‚è≥ Loading Whisper large-v3...\n",
            "‚úì Model loaded in 31.8s\n",
            "   GPU Memory: 10.6GB allocated, 14.1GB reserved\n",
            "\n",
            "üìä Found 8 WAV files\n",
            "   ‚è≥ Call Recording - 13Mar2025 1200 BPA.wav\n",
            "   ‚è≥ Call Recording - 13Mar25 1130 BK.wav\n",
            "   ‚è≥ Call Recording - 13Mar25 1300 HB.wav\n",
            "   ‚è≥ Call Recording - 19Mar2025 0800 JD.wav\n",
            "   ‚è≥ Call Recording - 19Mar25 0900 - AJ.wav\n",
            "   ‚è≥ Call Recording - 19Mar25 1730 - MO.wav\n",
            "   ‚è≥ Call Recording - 20Mar2025 1200 LN.wav\n",
            "   ‚è≥ Call Recording - 26Mar2025 0830 SA.wav\n",
            "\n",
            "Files to process: 8\n",
            "\n",
            "üöÄ Starting transcription of 3 files\n",
            "   Multilingual mode: Auto-detecting English/Spanish\n",
            "   Quality: Beam search enabled\n",
            "   Target GPU usage: 12-14GB\n",
            "\n",
            "======================================================================\n",
            "[1/3] Call Recording - 13Mar2025 1200 BPA.wav\n",
            "======================================================================\n",
            "   Audio: 128.3 MB, ~35:02.40 duration\n",
            "   With dummy tensor: GPU Memory: 10.6GB allocated, 14.1GB reserved\n",
            "   Attempt 1/3 (temp=0.0, beam=5)...\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: English\n",
            "[00:05.260 --> 00:08.680]  Great. And so, do you have any other questions before we get started?\n",
            "[00:12.940 --> 00:19.820]  No. Okay, perfect. So, to start, could you share a little bit about the main reason you attended\n",
            "[00:19.820 --> 00:25.400]  your last webinar? I think the one that you attended was for toilet training support. Is\n",
            "[00:25.400 --> 00:25.840]  that correct?\n",
            "[01:06.060 --> 01:14.660]  Yes, I took the course because my four-year-old is autistic and is still wearing a diaper,\n",
            "[01:15.340 --> 01:22.060]  and I know that he needed some training. I tried to do it the usual way, but I didn't\n",
            "[01:22.060 --> 01:24.960]  manage to do it, so I was looking for another way to teach him.\n",
            "[01:25.400 --> 01:32.460]  Okay. Have you accessed any of our other pediatric rehab services or webinars before?\n",
            "[01:34.100 --> 01:39.120]  ¬øUsted ha accedido a alg√∫n otro de nuestros servicios de rehabilitaci√≥n pedi√°trica o\n",
            "[01:39.120 --> 01:40.120]  seminarios web antes?\n",
            "[01:41.940 --> 01:44.100]  No. Creo que no.\n",
            "[01:44.560 --> 01:48.440]  Okay. So, no services for autistic children either?\n",
            "[01:49.720 --> 01:53.440]  ¬øNo ha usado servicios para ni√±os con autismo tampoco?\n",
            "[01:55.400 --> 01:55.920]  No.\n",
            "[01:56.300 --> 01:56.880]  Okay.\n",
            "[01:57.260 --> 01:57.840]  Desconozco.\n",
            "[01:57.960 --> 01:58.120]  No.\n",
            "[01:58.240 --> 02:08.940]  Okay. On a scale of 1 to 5, so 1 being the lowest and 5 being the highest, how would\n",
            "[02:08.940 --> 02:15.000]  you rate your experience from registering for the webinar, attending the session, to\n",
            "[02:15.000 --> 02:17.140]  using the tools and strategies at home?\n",
            "[02:19.620 --> 02:25.260]  En una escala del 1 al 5, el 1 siendo el n√∫mero m√°s bajo y el 5 siendo el m√°s alto,\n",
            "[02:25.400 --> 02:30.680]  ¬øc√≥mo calificar√≠a su experiencia desde que se registr√≥ al seminario web, al asistir\n",
            "[02:30.680 --> 02:33.620]  a la sesi√≥n, el utilizar las herramientas y estrategias en casa?\n",
            "[02:36.280 --> 02:36.840]  4.\n",
            "[02:37.500 --> 02:38.060]  4.\n",
            "[02:38.260 --> 02:44.700]  Okay. And what made you choose the rating 4? Like, what worked well for you and what\n",
            "[02:44.700 --> 02:46.960]  challenges did you encounter?\n",
            "[02:49.200 --> 02:55.200]  Que la hace escoger el n√∫mero 4, que es lo que le funciona muy bien y cu√°les son los\n",
            "[02:55.400 --> 02:56.620]  retos que usted encontr√≥.\n",
            "[02:58.620 --> 03:05.400]  Conectarme y todo, entrar al curso fue todo f√°cil.\n",
            "[03:06.400 --> 03:17.960]  Solo que, pues, por el idioma, para m√≠ fue un poquito complejo porque la persona que\n",
            "[03:17.960 --> 03:22.680]  habla, que da la pl√°tica habla corrido toda una hora.\n",
            "[03:23.400 --> 03:23.560]  5.\n",
            "[03:24.860 --> 03:25.420]  5.\n",
            "[03:31.340 --> 03:31.900]  6.\n",
            "[03:32.260 --> 03:32.260]  7.\n",
            "[03:33.280 --> 03:33.420]  8.\n",
            "[03:34.920 --> 03:34.940]  9.\n",
            "[03:34.960 --> 03:34.980]  10.\n",
            "[03:35.000 --> 03:35.180]  11.\n",
            "[03:36.120 --> 03:36.120]  12.\n",
            "[03:36.120 --> 03:36.640]  13.\n",
            "[03:36.640 --> 03:36.640]  14.\n",
            "[03:36.640 --> 03:36.640]  15.\n",
            "[03:36.720 --> 03:36.720]  16.\n",
            "[03:37.220 --> 03:37.780]  17.\n",
            "[03:37.780 --> 03:37.780]  18.\n",
            "[03:40.340 --> 03:40.900]  19.\n",
            "[03:40.920 --> 03:40.940]  20.\n",
            "[03:41.240 --> 03:41.800]  21.\n",
            "[03:41.800 --> 03:42.280]  22.\n",
            "[03:42.280 --> 03:42.840]  23.\n",
            "[03:42.840 --> 03:42.980]  24.\n",
            "[03:42.980 --> 03:43.080]  25.\n",
            "[03:43.080 --> 03:43.080]  26.\n",
            "[03:43.080 --> 03:43.200]  27.\n",
            "[03:43.200 --> 03:43.380]  28.\n",
            "[03:44.020 --> 03:44.580]  29.\n",
            "[03:44.820 --> 03:45.380]  30.\n",
            "[03:45.380 --> 03:45.380]  31.\n",
            "[03:45.380 --> 03:45.760]  32.\n",
            "[03:46.300 --> 03:46.860]  33.\n",
            "[03:46.860 --> 03:46.860]  34.\n",
            "[03:53.580 --> 03:54.420]  33.\n",
            "[03:58.840 --> 03:59.680]  34.\n",
            "[03:59.760 --> 04:00.600]  35.\n",
            "[04:00.600 --> 04:00.600]  36.\n",
            "[04:00.880 --> 04:01.720]  36.\n",
            "[04:04.360 --> 04:05.200]  37.\n",
            "[04:06.280 --> 04:06.400]  38.\n",
            "[04:06.760 --> 04:07.240]  39.\n",
            "[04:07.240 --> 04:07.660]  40.\n",
            "[04:07.860 --> 04:07.880]  41.\n",
            "[04:08.060 --> 04:08.200]  42.\n",
            "[04:08.200 --> 04:08.620]  42.\n",
            "[04:08.680 --> 04:09.520]  43.\n",
            "[04:09.520 --> 04:09.520]  44.\n",
            "[04:09.520 --> 04:09.580]  44.\n",
            "[04:09.580 --> 04:09.660]  45.\n",
            "[04:09.660 --> 04:09.720]  46.\n",
            "[04:09.760 --> 04:09.800]  46.\n",
            "[04:09.800 --> 04:09.800]  47.\n",
            "[04:09.800 --> 04:09.800]  48.\n",
            "[04:09.800 --> 04:09.800]  49.\n",
            "[04:09.800 --> 04:09.800]  50.\n",
            "[04:10.740 --> 04:11.580]  51.\n",
            "[04:12.220 --> 04:13.060]  52.\n",
            "[04:14.380 --> 04:15.220]  53.\n",
            "[04:15.680 --> 04:16.520]  52.\n",
            "[04:16.700 --> 04:17.540]  53.\n",
            "[04:17.540 --> 04:17.640]  54.\n",
            "[04:17.640 --> 04:17.640]  55.\n",
            "[04:18.560 --> 04:18.800]  54.\n",
            "[04:28.120 --> 04:28.360]  55.\n",
            "[04:28.360 --> 04:28.360]  56.\n",
            "[04:28.360 --> 04:28.360]  57.\n",
            "[04:28.360 --> 04:28.360]  58.\n",
            "[04:28.740 --> 04:28.980]  59.\n",
            "[04:30.640 --> 04:30.700]  60.\n",
            "[04:30.820 --> 04:31.060]  61.\n",
            "[04:31.080 --> 04:31.100]  62.\n",
            "[04:31.120 --> 04:31.140]  62.\n",
            "[04:31.140 --> 04:31.160]  63.\n",
            "[04:31.180 --> 04:31.200]  64.\n",
            "[04:31.200 --> 04:31.200]  65.\n",
            "[04:31.200 --> 04:31.400]  67.\n",
            "[04:32.040 --> 04:32.140]  68.\n",
            "[04:32.140 --> 04:32.140]  69.\n",
            "[04:32.140 --> 04:32.140]  69.\n",
            "[04:32.140 --> 04:32.140]  70.\n",
            "[04:32.160 --> 04:32.220]  71.\n",
            "[04:32.300 --> 04:32.440]  72.\n",
            "[04:33.480 --> 04:33.720]  72.\n",
            "[04:33.720 --> 04:33.720]  73.\n",
            "[04:35.120 --> 04:35.360]  74.\n",
            "[04:37.520 --> 04:37.760]  75.\n",
            "[04:38.500 --> 04:38.740]  76.\n",
            "[04:39.360 --> 04:39.600]  77.\n",
            "[04:39.600 --> 04:39.620]  78.\n",
            "[04:39.620 --> 04:39.620]  78.\n",
            "[04:40.820 --> 04:41.060]  79.\n",
            "[04:41.060 --> 04:41.260]  80.\n",
            "[04:42.800 --> 04:42.800]  81.\n",
            "[04:44.440 --> 04:44.880]  82.\n",
            "[04:45.440 --> 04:45.880]  82.\n",
            "[04:49.280 --> 04:49.720]  83.\n",
            "[04:49.720 --> 04:49.900]  84.\n",
            "[04:49.900 --> 04:50.060]  85.\n",
            "[04:50.060 --> 04:50.060]  86.\n",
            "[04:50.060 --> 04:50.200]  87.\n",
            "[04:50.200 --> 04:50.260]  88.\n",
            "[05:01.000 --> 05:01.440]  89.\n",
            "[05:02.700 --> 05:03.140]  90.\n",
            "[05:03.460 --> 05:03.900]  91.\n",
            "[05:03.900 --> 05:03.900]  92.\n",
            "[05:03.900 --> 05:03.900]  92.\n",
            "[05:03.900 --> 05:04.080]  93.\n",
            "[05:04.080 --> 05:04.080]  92.\n",
            "[05:04.080 --> 05:04.080]  93.\n",
            "[05:04.080 --> 05:04.080]  94.\n",
            "[05:05.560 --> 05:06.000]  95.\n",
            "[05:07.620 --> 05:08.060]  96.\n",
            "[05:08.300 --> 05:08.740]  97.\n",
            "[05:08.740 --> 05:08.740]  98.\n",
            "[05:08.900 --> 05:08.980]  99.\n",
            "[05:09.220 --> 05:09.660]  100.\n",
            "[05:10.280 --> 05:10.500]  100.\n",
            "[05:10.840 --> 05:10.840]  100.\n",
            "[05:11.000 --> 05:11.000]  100.\n",
            "[05:11.000 --> 05:11.000]  100.\n",
            "[05:11.000 --> 05:11.520]  100.\n",
            "[05:16.740 --> 05:17.420]  0.\n",
            "[05:25.940 --> 05:26.620]  0.\n",
            "[05:26.680 --> 05:27.340]  0.\n",
            "[05:27.340 --> 05:27.340]  0.\n",
            "[05:27.340 --> 05:27.420]  0.\n",
            "[05:27.480 --> 05:27.500]  0.\n",
            "[05:27.520 --> 05:27.540]  0.\n",
            "[05:27.560 --> 05:28.040]  0.\n",
            "[05:28.720 --> 05:29.400]  0.\n",
            "[05:29.400 --> 05:29.480]  0.\n",
            "[05:29.480 --> 05:29.480]  0.\n",
            "[05:30.040 --> 05:30.720]  0.\n",
            "[05:30.720 --> 05:30.720]  0.\n",
            "[05:30.720 --> 05:30.720]  0.\n",
            "[05:30.720 --> 05:31.060]  0.\n",
            "[05:31.060 --> 05:31.060]  0.\n",
            "[05:31.060 --> 05:31.060]  0.\n",
            "[05:31.060 --> 05:31.060]  0.\n",
            "[05:31.060 --> 05:31.060]  0.\n",
            "[05:31.060 --> 05:31.160]  0.\n",
            "[05:31.160 --> 05:31.160]  0.\n",
            "[05:31.160 --> 05:31.160]  0.\n",
            "[05:31.160 --> 05:31.160]  0.\n",
            "[05:31.160 --> 05:31.160]  0.\n",
            "[05:31.160 --> 05:31.160]  0.\n",
            "[05:31.160 --> 05:31.160]  0.\n",
            "[05:31.160 --> 05:31.160]  0.\n",
            "[05:33.580 --> 05:34.260]  0.\n",
            "[05:34.260 --> 05:34.360]  0.\n",
            "[05:34.360 --> 05:34.640]  0.\n",
            "[05:39.300 --> 05:39.580]  0.\n",
            "[05:48.260 --> 05:48.540]  0.\n",
            "[05:49.440 --> 05:49.440]  0.\n",
            "[05:49.720 --> 05:49.720]  0.\n",
            "[05:49.720 --> 05:49.720]  0.\n",
            "[05:50.000 --> 05:50.060]  0.\n",
            "[05:50.260 --> 05:50.280]  0.\n",
            "[05:50.300 --> 05:50.320]  0.\n",
            "[05:50.360 --> 05:50.380]  0.\n",
            "[05:50.400 --> 05:50.440]  0.\n",
            "[05:50.460 --> 05:50.480]  0.\n",
            "[05:50.500 --> 05:50.560]  0.\n",
            "[05:50.580 --> 05:50.700]  0.\n",
            "[05:50.820 --> 05:50.820]  0.\n",
            "[05:50.980 --> 05:51.260]  0.\n",
            "[05:51.680 --> 05:51.800]  0.\n",
            "[05:51.820 --> 05:51.840]  0.\n",
            "[05:51.980 --> 05:52.120]  0.\n",
            "[05:52.360 --> 05:52.580]  0.\n",
            "[05:52.820 --> 05:53.100]  0.\n",
            "[05:53.100 --> 05:53.100]  0.\n",
            "[05:53.100 --> 05:53.100]  0.\n",
            "[05:53.100 --> 05:53.100]  0.\n",
            "[05:53.100 --> 05:53.100]  0.\n",
            "[05:53.280 --> 05:53.560]  0.\n",
            "[05:56.060 --> 05:56.340]  0.\n",
            "[06:00.480 --> 06:00.760]  0.\n",
            "[06:00.760 --> 06:01.600]  0.\n",
            "[06:09.800 --> 06:10.600]  0.\n",
            "[06:12.180 --> 06:12.980]  0.\n",
            "[06:13.280 --> 06:13.680]  0.\n",
            "[06:14.580 --> 06:14.580]  0.\n",
            "[06:14.580 --> 06:14.580]  0.\n",
            "[06:14.660 --> 06:14.660]  0.\n",
            "[06:15.720 --> 06:16.520]  0.\n",
            "[06:16.640 --> 06:17.160]  0.\n",
            "[06:17.160 --> 06:17.300]  0.\n",
            "[06:19.560 --> 06:20.360]  0.\n",
            "[06:20.360 --> 06:20.380]  0.\n",
            "[06:20.380 --> 06:20.680]  0.\n",
            "[06:20.680 --> 06:20.780]  0.\n",
            "[06:20.780 --> 06:20.780]  0.\n",
            "[06:20.780 --> 06:20.780]  0.\n",
            "[06:20.780 --> 06:20.820]  0.\n",
            "[06:20.820 --> 06:20.820]  0.\n",
            "[06:20.820 --> 06:20.820]  0.\n",
            "[06:20.820 --> 06:20.820]  0.\n",
            "[06:20.820 --> 06:21.060]  0.\n",
            "[06:21.060 --> 06:21.080]  0.\n",
            "[06:21.080 --> 06:21.080]  0.\n",
            "[06:21.080 --> 06:21.080]  0.\n",
            "[06:21.080 --> 06:21.080]  0.\n",
            "[06:21.080 --> 06:21.680]  0.\n",
            "[06:22.220 --> 06:23.020]  0.\n",
            "[06:26.380 --> 06:27.180]  0.\n",
            "[06:27.180 --> 06:27.980]  0.\n",
            "[06:37.980 --> 06:38.120]  0.\n",
            "[06:43.020 --> 06:44.080]  0.\n",
            "[06:45.800 --> 06:46.860]  0.\n",
            "[06:46.860 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.900 --> 06:46.900]  0.\n",
            "[06:46.920 --> 06:47.120]  0.\n",
            "[06:47.120 --> 06:47.120]  0.\n",
            "[06:47.120 --> 06:47.120]  0.\n",
            "[06:47.120 --> 06:47.120]  0.\n",
            "[06:47.120 --> 06:47.160]  0.\n",
            "[06:47.160 --> 06:47.160]  0.\n",
            "[06:47.160 --> 06:47.160]  0.\n",
            "[06:47.160 --> 06:47.160]  0.\n",
            "[06:47.160 --> 06:47.160]  0.\n",
            "[06:47.160 --> 06:47.160]  0.\n",
            "[06:47.160 --> 06:47.160]  0.\n",
            "[06:47.160 --> 06:47.420]  0.\n",
            "[06:50.140 --> 06:51.200]  0.\n",
            "[06:51.200 --> 06:51.740]  0.\n",
            "[07:04.760 --> 07:04.760]  0.\n",
            "[07:04.800 --> 07:05.020]  0.\n",
            "[07:06.580 --> 07:06.680]  0.\n",
            "[07:06.780 --> 07:07.080]  0.\n",
            "[07:07.080 --> 07:07.120]  0.\n",
            "[07:07.120 --> 07:07.300]  0.\n",
            "[07:07.420 --> 07:07.500]  0.\n",
            "[07:07.500 --> 07:07.500]  0.\n",
            "[07:10.740 --> 07:11.040]  0.\n",
            "[07:11.040 --> 07:11.100]  0.\n",
            "[07:11.100 --> 07:11.400]  0.\n",
            "[07:11.400 --> 07:11.400]  0.\n",
            "[07:11.400 --> 07:11.400]  0.\n",
            "[07:11.400 --> 07:11.400]  0.\n",
            "[07:11.400 --> 07:11.460]  0.\n",
            "[07:11.460 --> 07:11.460]  0.\n",
            "[07:11.460 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:11.480]  0.\n",
            "[07:11.480 --> 07:12.400]  0.\n",
            "[07:17.460 --> 07:18.120]  0.\n",
            "[07:19.320 --> 07:19.980]  0.\n",
            "[07:19.980 --> 07:20.360]  0.\n",
            "[07:21.340 --> 07:21.900]  0.\n",
            "[07:22.200 --> 07:22.480]  0.\n",
            "[07:22.780 --> 07:22.780]  0.\n",
            "[07:23.080 --> 07:23.320]  0.\n",
            "[07:23.480 --> 07:24.080]  0.\n",
            "[07:24.480 --> 07:24.640]  0.\n",
            "[07:25.460 --> 07:25.480]  0.\n",
            "[07:25.520 --> 07:25.540]  0.\n",
            "[07:25.660 --> 07:26.020]  0.\n",
            "[07:26.160 --> 07:26.280]  0.\n",
            "[07:26.280 --> 07:26.280]  0.\n",
            "[07:26.280 --> 07:26.280]  0.\n",
            "[07:26.280 --> 07:26.280]  0.\n",
            "[07:26.660 --> 07:27.320]  0.\n",
            "[07:27.320 --> 07:27.320]  0.\n",
            "[07:27.320 --> 07:27.320]  0.\n",
            "[07:27.320 --> 07:27.340]  0.\n",
            "[07:27.340 --> 07:27.340]  0.\n",
            "[07:27.340 --> 07:27.340]  0.\n",
            "[07:27.340 --> 07:27.640]  0.\n",
            "[07:29.360 --> 07:30.020]  0.\n",
            "[07:30.560 --> 07:30.820]  0.\n",
            "[07:34.100 --> 07:34.760]  0.\n",
            "[07:34.760 --> 07:35.240]  0.\n",
            "[08:03.340 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.740]  0.\n",
            "[08:04.740 --> 08:04.900]  0.\n",
            "[08:06.740 --> 08:06.760]  0.\n",
            "[08:14.340 --> 08:15.440]  0.\n",
            "[08:19.960 --> 08:19.960]  0.\n",
            "[08:19.960 --> 08:19.960]  0.\n",
            "[08:19.960 --> 08:19.960]  0.\n",
            "[08:19.960 --> 08:19.960]  0.\n",
            "[08:19.960 --> 08:19.960]  0.\n",
            "[08:19.960 --> 08:19.960]  0.\n",
            "[08:19.980 --> 08:20.000]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.020]  0.\n",
            "[08:20.020 --> 08:20.300]  0.\n",
            "[08:22.820 --> 08:23.920]  0.\n",
            "[08:24.500 --> 08:24.940]  0.\n",
            "[08:25.860 --> 08:26.960]  0.\n",
            "[08:29.440 --> 08:30.540]  0.\n",
            "[08:32.320 --> 08:32.540]  0.\n",
            "[08:40.180 --> 08:40.940]  0.\n",
            "[08:43.580 --> 08:43.840]  0.\n",
            "[08:51.860 --> 08:53.260]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.520]  0.\n",
            "[09:00.520 --> 09:00.640]  0.\n",
            "[09:03.300 --> 09:03.300]  0.\n",
            "[09:05.860 --> 09:07.140]  0.\n",
            "[09:17.740 --> 09:17.920]  0.\n",
            "[09:21.500 --> 09:22.780]  0.\n",
            "[09:28.500 --> 09:29.780]  0.\n",
            "[09:29.800 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:29.820 --> 09:29.820]  0.\n",
            "[09:31.360 --> 09:31.760]  0.\n",
            "[09:39.900 --> 09:39.900]  0.\n",
            "[09:40.540 --> 09:40.720]  0.\n",
            "[09:44.020 --> 09:44.020]  0.\n",
            "[09:44.560 --> 09:44.560]  0.\n",
            "[09:44.680 --> 09:44.700]  0.\n",
            "[09:44.840 --> 09:44.840]  0.\n",
            "[09:44.920 --> 09:44.920]  0.\n",
            "[09:44.920 --> 09:44.920]  0.\n",
            "[09:44.920 --> 09:44.920]  0.\n",
            "[09:44.920 --> 09:44.920]  0.\n",
            "[09:44.920 --> 09:44.920]  0.\n",
            "[09:44.940 --> 09:44.960]  0.\n",
            "[09:44.980 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.180]  0.\n",
            "[09:45.180 --> 09:45.240]  0.\n",
            "[09:45.440 --> 09:45.720]  0.\n",
            "[09:45.940 --> 09:46.340]  0.\n",
            "[09:50.640 --> 09:51.040]  0.\n",
            "[09:52.900 --> 09:53.300]  0.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2942885255.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;31m# Run main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;31m# ============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2942885255.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;31m# Process files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_to_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Final cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2942885255.py\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(files_to_process)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# Process with GPU optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_with_gpu_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;31m# Extract results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2942885255.py\u001b[0m in \u001b[0;36mprocess_with_gpu_optimization\u001b[0;34m(model, audio_path)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Transcribe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Clean up dummy tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2942885255.py\u001b[0m in \u001b[0;36mtranscribe_audio\u001b[0;34m(model, audio_path, max_retries)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# Transcribe directly from file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# Whisper handles loading and processing internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# Check if we got valid results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword_timestamps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 add_word_timestamps(\n\u001b[0m\u001b[1;32m    387\u001b[0m                     \u001b[0msegments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_segments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/timing.py\u001b[0m in \u001b[0;36madd_word_timestamps\u001b[0;34m(segments, model, tokenizer, mel, num_frames, prepend_punctuations, append_punctuations, last_speech_timestamp, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokens_per_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0malignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_alignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0mword_durations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mword_durations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_durations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_durations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/timing.py\u001b[0m in \u001b[0;36mfind_alignment\u001b[0;34m(model, tokenizer, text_tokens, mel, num_frames, medfilt_width, qk_scale)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_sdpa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0msampled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msot_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mtoken_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mel, tokens)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     ) -> Dict[str, torch.Tensor]:\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         return F.linear(\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aS-xfrvjv5EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 FIXED - HIGH QUALITY TRANSCRIPTION\n",
        "# Corrected parameters, 14GB GPU target, no diarization\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Checking dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing Whisper...\")\n",
        "    subprocess.run(['pip', 'install', '-q', 'openai-whisper'], check=True)\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "# ============================================\n",
        "# GPU SETUP AND MONITORING\n",
        "# ============================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if device == \"cuda\":\n",
        "    # Configure GPU for maximum usage\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)\n",
        "\n",
        "    # Enable TF32 for better performance\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Get GPU info\n",
        "    gpu_props = torch.cuda.get_device_properties(0)\n",
        "    total_memory_gb = gpu_props.total_memory / 1e9\n",
        "\n",
        "    print(f\"\\nüéÆ GPU Configuration:\")\n",
        "    print(f\"   Device: {gpu_props.name}\")\n",
        "    print(f\"   Total Memory: {total_memory_gb:.1f} GB\")\n",
        "    print(f\"   Target Usage: 14GB\")\n",
        "\n",
        "def print_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        print(f\"   GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved\")\n",
        "\n",
        "# ============================================\n",
        "# VALID WHISPER PARAMETERS\n",
        "# ============================================\n",
        "# These are the ACTUAL parameters that Whisper accepts\n",
        "TRANSCRIPTION_PARAMS = {\n",
        "    # Basic parameters\n",
        "    'language': None,           # Auto-detect\n",
        "    'task': 'transcribe',      # Don't translate\n",
        "    'temperature': 0.0,        # Start with greedy decoding\n",
        "\n",
        "    # Quality parameters\n",
        "    'beam_size': 10,           # Beam search for better quality\n",
        "    'best_of': 5,              # Sample multiple times\n",
        "    'patience': 1.0,           # Beam search patience\n",
        "    'length_penalty': None,    # Default penalty\n",
        "\n",
        "    # Behavior parameters\n",
        "    'compression_ratio_threshold': 2.4,\n",
        "    'logprob_threshold': -1.0,\n",
        "    'no_speech_threshold': 0.6,\n",
        "    'condition_on_previous_text': True,  # Keep context\n",
        "\n",
        "    # Output parameters\n",
        "    'word_timestamps': True,\n",
        "    'prepend_punctuations': '\"\\'\"¬ø([{-',\n",
        "    'append_punctuations': '\"\\'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ',\n",
        "\n",
        "    # Prompting\n",
        "    'initial_prompt': \"This is a professional interview or call recording.\",\n",
        "\n",
        "    # Format\n",
        "    'verbose': True,  # Show progress bar\n",
        "    'fp16': True,     # Use FP16 on GPU\n",
        "}\n",
        "\n",
        "# ============================================\n",
        "# MEMORY ALLOCATION STRATEGY\n",
        "# ============================================\n",
        "print(\"\\nüìä Allocating GPU memory to reach 14GB target...\")\n",
        "\n",
        "# Pre-allocate tensors to force GPU memory usage\n",
        "MEMORY_POOL = []\n",
        "\n",
        "if device == \"cuda\":\n",
        "    try:\n",
        "        # Allocate in chunks to reach ~14GB total\n",
        "        # Model takes ~9-10GB, we want to allocate ~4GB more\n",
        "\n",
        "        # 1. Allocate large tensor for audio processing\n",
        "        audio_buffer = torch.zeros(16000 * 60 * 30, device='cuda')  # 30 min audio buffer\n",
        "        MEMORY_POOL.append(audio_buffer)\n",
        "        print_gpu_memory()\n",
        "\n",
        "        # 2. Allocate tensor for mel spectrograms\n",
        "        mel_buffer = torch.zeros(80, 3000, 32, device='cuda')  # Batch of mel specs\n",
        "        MEMORY_POOL.append(mel_buffer)\n",
        "        print_gpu_memory()\n",
        "\n",
        "        # 3. Allocate tensor for intermediate computations\n",
        "        compute_buffer = torch.zeros(1024, 1024, 1024, device='cuda', dtype=torch.float16)  # ~2GB\n",
        "        MEMORY_POOL.append(compute_buffer)\n",
        "        print_gpu_memory()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"   Reached memory limit: {e}\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL\n",
        "# ============================================\n",
        "print(\"\\n‚è≥ Loading Whisper large-v3...\")\n",
        "start_load = time.time()\n",
        "\n",
        "# Load model\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "# Convert to FP16 for GPU\n",
        "if device == \"cuda\":\n",
        "    model = model.half()\n",
        "\n",
        "load_time = time.time() - start_load\n",
        "print(f\"‚úì Model loaded in {load_time:.1f}s\")\n",
        "print_gpu_memory()\n",
        "\n",
        "# ============================================\n",
        "# TRANSCRIPTION FUNCTION WITH RETRY LOGIC\n",
        "# ============================================\n",
        "def transcribe_with_retry(audio_path, max_attempts=3):\n",
        "    \"\"\"\n",
        "    Transcribe with retry logic and temperature fallback\n",
        "    \"\"\"\n",
        "    temperatures = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        temp = temperatures[min(attempt, len(temperatures)-1)]\n",
        "\n",
        "        try:\n",
        "            print(f\"   Attempt {attempt+1}/{max_attempts} (temperature={temp})...\", end='', flush=True)\n",
        "\n",
        "            # Update temperature\n",
        "            params = TRANSCRIPTION_PARAMS.copy()\n",
        "            params['temperature'] = temp\n",
        "\n",
        "            # Transcribe\n",
        "            result = model.transcribe(audio_path, **params)\n",
        "\n",
        "            # Check for hallucinations\n",
        "            segments = result.get('segments', [])\n",
        "            if not has_hallucinations(segments):\n",
        "                print(\" ‚úì Success\")\n",
        "                return result\n",
        "            else:\n",
        "                print(\" ‚ö†Ô∏è Hallucinations detected\")\n",
        "                # On second attempt, disable context\n",
        "                if attempt == 1:\n",
        "                    params['condition_on_previous_text'] = False\n",
        "                    params['initial_prompt'] = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" ‚ùå Error: {e}\")\n",
        "\n",
        "    # If all attempts failed, return last result or empty\n",
        "    print(\"   ‚ö†Ô∏è Using best effort result\")\n",
        "    return result if 'result' in locals() else {'segments': [], 'language': 'unknown'}\n",
        "\n",
        "def has_hallucinations(segments, threshold=5):\n",
        "    \"\"\"Check for repetitive patterns indicating hallucination\"\"\"\n",
        "    if len(segments) < threshold:\n",
        "        return False\n",
        "\n",
        "    texts = [s.get('text', '').strip().lower() for s in segments]\n",
        "\n",
        "    # Check for exact repetitions\n",
        "    for i in range(len(texts) - threshold + 1):\n",
        "        window = texts[i:i+threshold]\n",
        "        if len(set(window)) == 1 and window[0]:  # All same non-empty text\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Format seconds as MM:SS.ss\"\"\"\n",
        "    minutes = int(seconds // 60)\n",
        "    secs = seconds % 60\n",
        "    return f\"{minutes:02d}:{secs:05.2f}\"\n",
        "\n",
        "# ============================================\n",
        "# BATCH PROCESSING FUNCTION\n",
        "# ============================================\n",
        "def process_files_batch(files_to_process):\n",
        "    \"\"\"Process multiple files with GPU memory optimization\"\"\"\n",
        "\n",
        "    for idx, audio_file in enumerate(files_to_process):\n",
        "        base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
        "        output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_final.txt\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"[{idx+1}/{len(files_to_process)}] {os.path.basename(audio_file)}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            # Get file info\n",
        "            file_size_mb = os.path.getsize(audio_file) / (1024**2)\n",
        "            print(f\"   File size: {file_size_mb:.1f} MB\")\n",
        "\n",
        "            # Start processing\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Pre-allocate GPU tensor for audio (forces memory usage)\n",
        "            if device == \"cuda\":\n",
        "                audio_data = whisper.load_audio(audio_file)\n",
        "                audio_tensor = torch.from_numpy(audio_data).to(device)\n",
        "                print(f\"   Audio loaded to GPU: {audio_tensor.shape}\")\n",
        "                print_gpu_memory()\n",
        "\n",
        "            # Transcribe with retry logic\n",
        "            result = transcribe_with_retry(audio_file)\n",
        "\n",
        "            # Process results\n",
        "            segments = result.get('segments', [])\n",
        "            language = result.get('language', 'unknown')\n",
        "            duration = segments[-1]['end'] if segments else 0\n",
        "            process_time = time.time() - start_time\n",
        "\n",
        "            # Calculate stats\n",
        "            words = sum(len(s.get('text', '').split()) for s in segments)\n",
        "            speed = duration / process_time if process_time > 0 else 0\n",
        "\n",
        "            print(f\"\\n   ‚úÖ Transcription complete!\")\n",
        "            print(f\"      Language: {language}\")\n",
        "            print(f\"      Duration: {format_time(duration)}\")\n",
        "            print(f\"      Words: {words:,}\")\n",
        "            print(f\"      Process time: {format_time(process_time)}\")\n",
        "            print(f\"      Speed: {speed:.1f}x realtime\")\n",
        "            print_gpu_memory()\n",
        "\n",
        "            # Save transcript\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                # Header\n",
        "                f.write(f\"# Whisper V3 Large Transcription\\n\")\n",
        "                f.write(f\"# Model: large-v3\\n\")\n",
        "                f.write(f\"# File: {os.path.basename(audio_file)}\\n\")\n",
        "                f.write(f\"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "                f.write(f\"# Duration: {format_time(duration)}\\n\")\n",
        "                f.write(f\"# Language: {language}\\n\")\n",
        "                f.write(f\"# Words: {words:,}\\n\")\n",
        "                f.write(f\"# Beam size: {TRANSCRIPTION_PARAMS['beam_size']}\\n\")\n",
        "                f.write(\"#\" + \"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "                # Segments\n",
        "                prev_end = 0\n",
        "                for segment in segments:\n",
        "                    start = segment['start']\n",
        "                    end = segment['end']\n",
        "                    text = segment.get('text', '').strip()\n",
        "\n",
        "                    if text:\n",
        "                        # Add paragraph break for long pauses\n",
        "                        if start - prev_end > 3.0:\n",
        "                            f.write(\"\\n\")\n",
        "\n",
        "                        f.write(f\"[{format_time(start)} ‚Üí {format_time(end)}] {text}\\n\")\n",
        "                        prev_end = end\n",
        "\n",
        "            print(f\"   ‚úì Saved: {os.path.basename(output_file)}\")\n",
        "\n",
        "            # Clean up GPU tensor\n",
        "            if device == \"cuda\" and 'audio_tensor' in locals():\n",
        "                del audio_tensor\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n   ‚ùå Failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Save error log\n",
        "            with open(f\"{output_file}.error\", 'w') as f:\n",
        "                f.write(f\"Error: {str(e)}\\n\")\n",
        "                f.write(f\"Traceback:\\n{traceback.format_exc()}\\n\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Cool down between files\n",
        "        if idx < len(files_to_process) - 1:\n",
        "            print(\"\\n‚è≥ Cooling down for 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find WAV files\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    print(f\"\\nüìä Scanning {len(wav_files)} WAV files...\")\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "\n",
        "        # Check if already processed\n",
        "        transcript_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_final.txt', '_gpu_optimized.txt', '_enhanced.txt', '_ultimate.txt']\n",
        "        )\n",
        "\n",
        "        if not transcript_exists:\n",
        "            remaining.append(wav_file)\n",
        "            print(f\"   ‚è≥ {os.path.basename(wav_file)}\")\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if remaining:\n",
        "        # Process up to 3 files per session\n",
        "        files_to_process = remaining[:3]\n",
        "\n",
        "        print(f\"\\nüöÄ Starting transcription of {len(files_to_process)} files...\")\n",
        "        print(f\"   Using beam search (size={TRANSCRIPTION_PARAMS['beam_size']})\")\n",
        "        print(f\"   Best of {TRANSCRIPTION_PARAMS['best_of']} attempts\")\n",
        "        print(f\"   Target GPU usage: 14GB\")\n",
        "\n",
        "        # Process files\n",
        "        process_files_batch(files_to_process)\n",
        "\n",
        "        # Clean up memory pool\n",
        "        MEMORY_POOL.clear()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"‚úÖ SESSION COMPLETE\")\n",
        "        print(f\"   Processed: {len(files_to_process)} files\")\n",
        "        print(f\"   Remaining: {len(remaining) - len(files_to_process)} files\")\n",
        "        print_gpu_memory()\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚úÖ All files already transcribed!\")\n",
        "\n",
        "# ============================================\n",
        "# RUN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# MANUAL OPTIMIZATION TIPS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° TO INCREASE GPU USAGE FURTHER:\")\n",
        "print(\"=\"*70)\n",
        "print(\"1. Increase beam_size to 20:\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['beam_size'] = 20\")\n",
        "print(\"\")\n",
        "print(\"2. Increase best_of to 10:\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['best_of'] = 10\")\n",
        "print(\"\")\n",
        "print(\"3. Process 2 files in parallel (risky):\")\n",
        "print(\"   Add parallel processing with ThreadPoolExecutor\")\n",
        "print(\"\")\n",
        "print(\"4. Load entire audio to GPU before processing:\")\n",
        "print(\"   Already implemented - uses more memory\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81lArfxtubjd",
        "outputId": "608e57ec-ccb4-4871-da08-494159b5ec81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Checking dependencies...\n",
            "‚úì Whisper already installed\n",
            "\n",
            "üéÆ GPU Configuration:\n",
            "   Device: Tesla T4\n",
            "   Total Memory: 15.8 GB\n",
            "   Target Usage: 14GB\n",
            "\n",
            "üìä Allocating GPU memory to reach 14GB target...\n",
            "   GPU Memory: 5.5GB allocated, 6.4GB reserved\n",
            "   GPU Memory: 5.5GB allocated, 6.4GB reserved\n",
            "   GPU Memory: 7.7GB allocated, 8.6GB reserved\n",
            "\n",
            "‚è≥ Loading Whisper large-v3...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_6uANjjubWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 ULTIMATE - MAXIMIZING GPU USAGE FOR QUALITY\n",
        "# Targets 14GB GPU RAM usage with quality-focused optimizations\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION - QUALITY MAXIMIZED\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "WHISPER_MODEL = \"large-v3\"\n",
        "\n",
        "# Enhanced parameters for maximum quality\n",
        "TRANSCRIPTION_PARAMS = {\n",
        "    # Quality settings\n",
        "    'best_of': 5,  # Generate 5 candidates and pick the best (uses more memory)\n",
        "    'beam_size': 10,  # Beam search instead of greedy (much better quality, more memory)\n",
        "    'patience': 2.0,  # Beam search patience\n",
        "    'length_penalty': 1.0,  # Favor longer sequences\n",
        "\n",
        "    # Temperature strategy\n",
        "    'temperature': 0.0,  # Start with 0 for consistency\n",
        "    'temperature_increment_on_fallback': 0.2,  # Only increase if needed\n",
        "\n",
        "    # Thresholds\n",
        "    'compression_ratio_threshold': 2.4,\n",
        "    'logprob_threshold': -1.0,\n",
        "    'no_speech_threshold': 0.6,\n",
        "\n",
        "    # Context handling - SMART approach\n",
        "    'condition_on_previous_text': True,  # Keep context by default\n",
        "    'initial_prompt': \"This is a professional interview or call recording with clear speech.\",\n",
        "\n",
        "    # Advanced options\n",
        "    'word_timestamps': True,\n",
        "    'prepend_punctuations': '\"\\'\"¬ø([{-',\n",
        "    'append_punctuations': '\"\\'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ',\n",
        "\n",
        "    # Longer audio segments for better context\n",
        "    'chunk_length': 60,  # Process 60-second chunks instead of default 30\n",
        "}\n",
        "\n",
        "# Memory-intensive options for 14GB target\n",
        "ADVANCED_OPTIONS = {\n",
        "    'n_mel': 128,  # Increase mel bands (default 80)\n",
        "    'sample_len': 1500,  # Longer sample length\n",
        "    'best_of_temperatures': [0.0, 0.1, 0.2, 0.4, 0.8],  # More temperature attempts\n",
        "}\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Whisper V3 Ultimate - Quality Maximized Edition\")\n",
        "print(\"   Target GPU usage: 14GB for maximum accuracy\")\n",
        "print(\"   Features: Beam search, extended context, smart hallucination prevention\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES (SIMPLIFIED)\n",
        "# ============================================\n",
        "print(\"\\nüì¶ Checking dependencies...\")\n",
        "\n",
        "try:\n",
        "    import whisper\n",
        "    import torch\n",
        "    print(\"‚úì Core packages ready\")\n",
        "except ImportError:\n",
        "    print(\"Installing Whisper...\")\n",
        "    subprocess.run(['pip', 'install', '--quiet', 'openai-whisper'], check=True)\n",
        "    import whisper\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# GPU CONFIGURATION\n",
        "# ============================================\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    gpu_props = torch.cuda.get_device_properties(0)\n",
        "    total_memory_gb = gpu_props.total_memory / 1e9\n",
        "\n",
        "    print(f\"\\nüéÆ GPU Configuration:\")\n",
        "    print(f\"   Device: {gpu_props.name}\")\n",
        "    print(f\"   Total Memory: {total_memory_gb:.1f} GB\")\n",
        "\n",
        "    # Set PyTorch to use more memory\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "    print(f\"   Allocated for processing: {total_memory_gb * 0.95:.1f} GB\")\n",
        "\n",
        "    # Enable TF32 for better performance on Ampere GPUs\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    print(\"   TF32 acceleration: Enabled\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"‚ö†Ô∏è  No GPU detected - this will be extremely slow\")\n",
        "\n",
        "# ============================================\n",
        "# ENHANCED TRANSCRIPTION FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def load_audio_extended(file_path, sr=16000):\n",
        "    \"\"\"Load audio with extended precision\"\"\"\n",
        "    import librosa\n",
        "    audio, _ = librosa.load(file_path, sr=sr, mono=True, dtype=np.float32)\n",
        "    return audio\n",
        "\n",
        "def smart_transcribe(model, audio_path, initial_params):\n",
        "    \"\"\"\n",
        "    Intelligent transcription with adaptive parameters\n",
        "    \"\"\"\n",
        "    print(\"   üß† Smart transcription mode engaged...\")\n",
        "\n",
        "    # First attempt with context\n",
        "    params = initial_params.copy()\n",
        "    attempt = 1\n",
        "    max_attempts = 3\n",
        "\n",
        "    while attempt <= max_attempts:\n",
        "        try:\n",
        "            print(f\"   Attempt {attempt}/3 (context={'on' if params['condition_on_previous_text'] else 'off'})...\", end='', flush=True)\n",
        "\n",
        "            # Load audio in high quality\n",
        "            audio = load_audio_extended(audio_path)\n",
        "\n",
        "            # Transcribe with current parameters\n",
        "            result = model.transcribe(\n",
        "                audio,\n",
        "                language=None,  # Auto-detect\n",
        "                task='transcribe',\n",
        "                verbose=False,\n",
        "                fp16=(device == \"cuda\"),\n",
        "                **params\n",
        "            )\n",
        "\n",
        "            # Check for hallucinations\n",
        "            segments = result.get('segments', [])\n",
        "            hallucination_score = calculate_hallucination_score(segments)\n",
        "\n",
        "            if hallucination_score < 0.1:  # Good transcription\n",
        "                print(\" ‚úì Success (quality score: {:.2f})\".format(1 - hallucination_score))\n",
        "                return result\n",
        "            else:\n",
        "                print(f\" ‚ö†Ô∏è  Hallucination detected (score: {hallucination_score:.2f})\")\n",
        "\n",
        "                # Adaptive strategy\n",
        "                if attempt == 1:\n",
        "                    # Try without context\n",
        "                    params['condition_on_previous_text'] = False\n",
        "                    params['initial_prompt'] = None\n",
        "                elif attempt == 2:\n",
        "                    # Increase temperature and reduce beam search\n",
        "                    params['temperature'] = 0.2\n",
        "                    params['beam_size'] = 5\n",
        "                    params['best_of'] = 3\n",
        "\n",
        "                attempt += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" ‚ùå Error: {e}\")\n",
        "            attempt += 1\n",
        "\n",
        "    # Return best effort\n",
        "    return result\n",
        "\n",
        "def calculate_hallucination_score(segments):\n",
        "    \"\"\"\n",
        "    Calculate a score from 0-1 indicating likelihood of hallucination\n",
        "    \"\"\"\n",
        "    if len(segments) < 5:\n",
        "        return 0.0\n",
        "\n",
        "    # Check for repetitions\n",
        "    texts = [seg.get('text', '').strip().lower() for seg in segments]\n",
        "\n",
        "    repetition_count = 0\n",
        "    for i in range(len(texts) - 3):\n",
        "        # Check for exact repetitions in sliding window\n",
        "        window = texts[i:i+4]\n",
        "        if len(set(window)) == 1 and window[0]:\n",
        "            repetition_count += 1\n",
        "\n",
        "    # Check for suspiciously regular timestamps\n",
        "    timestamp_regularity = 0\n",
        "    for i in range(len(segments) - 1):\n",
        "        duration = segments[i+1]['start'] - segments[i]['end']\n",
        "        if duration < 0.1:  # Segments too close together\n",
        "            timestamp_regularity += 1\n",
        "\n",
        "    # Combined score\n",
        "    repetition_score = repetition_count / max(1, len(segments) - 3)\n",
        "    timestamp_score = timestamp_regularity / max(1, len(segments) - 1)\n",
        "\n",
        "    return min(1.0, repetition_score + timestamp_score)\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Format seconds to MM:SS.ss\"\"\"\n",
        "    minutes = int(seconds // 60)\n",
        "    secs = seconds % 60\n",
        "    return f\"{minutes:02d}:{secs:05.2f}\"\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING PIPELINE\n",
        "# ============================================\n",
        "\n",
        "# Find WAV files\n",
        "wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "remaining_files = []\n",
        "\n",
        "print(f\"\\nüìä Scanning for files...\")\n",
        "for wav_file in wav_files:\n",
        "    base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "\n",
        "    # Check multiple possible output names\n",
        "    transcript_exists = any(\n",
        "        os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "        for suffix in ['_ultimate.txt', '_enhanced.txt', '_large-v3.txt']\n",
        "    )\n",
        "\n",
        "    if not transcript_exists:\n",
        "        remaining_files.append(wav_file)\n",
        "        size_mb = os.path.getsize(wav_file) / (1024**2)\n",
        "        print(f\"   ‚è≥ {os.path.basename(wav_file)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "print(f\"\\nFiles to process: {len(remaining_files)} of {len(wav_files)}\")\n",
        "\n",
        "if remaining_files:\n",
        "    # Process 2 files per session with large-v3\n",
        "    files_to_process = remaining_files[:2]\n",
        "\n",
        "    # Load model with maximum quality settings\n",
        "    print(f\"\\n‚è≥ Loading {WHISPER_MODEL} for maximum quality...\")\n",
        "    print(\"   This optimizes for accuracy over speed\")\n",
        "\n",
        "    start_load = time.time()\n",
        "    model = whisper.load_model(WHISPER_MODEL, device=device)\n",
        "    load_time = time.time() - start_load\n",
        "\n",
        "    # Check memory usage after model load\n",
        "    if torch.cuda.is_available():\n",
        "        allocated_gb = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved_gb = torch.cuda.memory_reserved() / 1e9\n",
        "        print(f\"\\n‚úì Model loaded in {load_time:.1f}s\")\n",
        "        print(f\"   GPU Memory: {allocated_gb:.1f}GB used, {reserved_gb:.1f}GB reserved\")\n",
        "        print(f\"   Free for processing: {total_memory_gb - reserved_gb:.1f}GB\")\n",
        "\n",
        "    # Process files\n",
        "    for idx, audio_file in enumerate(files_to_process):\n",
        "        base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
        "        output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_ultimate.txt\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"[{idx+1}/{len(files_to_process)}] {os.path.basename(audio_file)}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            file_size_mb = os.path.getsize(audio_file) / (1024**2)\n",
        "\n",
        "            # Estimate duration from file size (rough approximation)\n",
        "            estimated_duration = file_size_mb * 12  # ~12 seconds per MB for WAV\n",
        "\n",
        "            print(f\"   File size: {file_size_mb:.1f} MB\")\n",
        "            print(f\"   Estimated duration: {format_time(estimated_duration)}\")\n",
        "            print(f\"   Using: Beam search (size={TRANSCRIPTION_PARAMS['beam_size']}), \"\n",
        "                  f\"Best of {TRANSCRIPTION_PARAMS['best_of']}\")\n",
        "\n",
        "            # Perform smart transcription\n",
        "            result = smart_transcribe(model, audio_file, TRANSCRIPTION_PARAMS)\n",
        "\n",
        "            # Extract results\n",
        "            segments = result.get('segments', [])\n",
        "            detected_language = result.get('language', 'unknown')\n",
        "            actual_duration = segments[-1]['end'] if segments else 0\n",
        "            process_time = time.time() - start_time\n",
        "\n",
        "            # Calculate statistics\n",
        "            speed_factor = actual_duration / process_time if process_time > 0 else 0\n",
        "            words_count = sum(len(seg.get('text', '').split()) for seg in segments)\n",
        "\n",
        "            print(f\"\\n‚úÖ Transcription complete!\")\n",
        "            print(f\"   Language: {detected_language}\")\n",
        "            print(f\"   Duration: {format_time(actual_duration)}\")\n",
        "            print(f\"   Process time: {format_time(process_time)}\")\n",
        "            print(f\"   Speed: {speed_factor:.1f}x realtime\")\n",
        "            print(f\"   Total words: {words_count:,}\")\n",
        "\n",
        "            # Memory check\n",
        "            if torch.cuda.is_available():\n",
        "                peak_memory_gb = torch.cuda.max_memory_allocated() / 1e9\n",
        "                print(f\"   Peak GPU memory: {peak_memory_gb:.1f}GB\")\n",
        "\n",
        "            # Save high-quality transcript\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                # Header with metadata\n",
        "                f.write(f\"# Whisper V3 Ultimate Transcription\\n\")\n",
        "                f.write(f\"# Model: {WHISPER_MODEL}\\n\")\n",
        "                f.write(f\"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "                f.write(f\"# File: {os.path.basename(audio_file)}\\n\")\n",
        "                f.write(f\"# Duration: {format_time(actual_duration)}\\n\")\n",
        "                f.write(f\"# Language: {detected_language}\\n\")\n",
        "                f.write(f\"# Words: {words_count:,}\\n\")\n",
        "                f.write(f\"# Processing: Beam search (size={TRANSCRIPTION_PARAMS['beam_size']}), \"\n",
        "                       f\"Best of {TRANSCRIPTION_PARAMS['best_of']}\\n\")\n",
        "                f.write(\"#\" + \"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "                # Write segments with improved formatting\n",
        "                for i, segment in enumerate(segments):\n",
        "                    start = segment['start']\n",
        "                    end = segment['end']\n",
        "                    text = segment.get('text', '').strip()\n",
        "\n",
        "                    if text:  # Skip empty segments\n",
        "                        # Format: [MM:SS.ss ‚Üí MM:SS.ss] Text\n",
        "                        f.write(f\"[{format_time(start)} ‚Üí {format_time(end)}] {text}\\n\")\n",
        "\n",
        "                        # Add paragraph breaks for long pauses\n",
        "                        if i < len(segments) - 1:\n",
        "                            next_start = segments[i + 1]['start']\n",
        "                            pause_duration = next_start - end\n",
        "                            if pause_duration > 3.0:  # 3+ second pause\n",
        "                                f.write(\"\\n\")\n",
        "\n",
        "            print(f\"\\n‚úì Saved: {os.path.basename(output_file)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Save error log\n",
        "            with open(f\"{output_file}.error\", 'w') as f:\n",
        "                f.write(f\"Error processing: {audio_file}\\n\")\n",
        "                f.write(f\"Error: {str(e)}\\n\")\n",
        "                f.write(f\"Traceback:\\n{traceback.format_exc()}\\n\")\n",
        "\n",
        "        # Cleanup after each file\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Show memory stats\n",
        "        if torch.cuda.is_available():\n",
        "            current_memory_gb = torch.cuda.memory_allocated() / 1e9\n",
        "            print(f\"\\nüíæ Memory after cleanup: {current_memory_gb:.1f}GB\")\n",
        "\n",
        "        # Cool down between files\n",
        "        if idx < len(files_to_process) - 1:\n",
        "            print(\"\\n‚è≥ Cooling down for 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "    # Final cleanup\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print(f\"   Processed: {len(files_to_process)} files\")\n",
        "    print(f\"   Remaining: {len(remaining_files) - len(files_to_process)} files\")\n",
        "    print(f\"   Output format: *_ultimate.txt\")\n",
        "    print(\"\\nüí° Quality optimizations used:\")\n",
        "    print(\"   - Beam search for better accuracy\")\n",
        "    print(\"   - Best-of-N sampling\")\n",
        "    print(\"   - Extended context windows\")\n",
        "    print(\"   - Smart hallucination prevention\")\n",
        "    print(\"   - 95% GPU memory utilization\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚úÖ All files have been transcribed!\")\n",
        "    print(f\"üìÅ Transcripts location: {OUTPUT_PATH}\")\n",
        "\n",
        "# ============================================\n",
        "# OPTIMIZATION TIPS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° FURTHER OPTIMIZATION OPTIONS\")\n",
        "print(\"=\"*70)\n",
        "print(\"To push quality even higher, you can manually adjust:\")\n",
        "print(\"\")\n",
        "print(\"1. Increase beam_size to 20 (slower but more accurate):\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['beam_size'] = 20\")\n",
        "print(\"\")\n",
        "print(\"2. Increase best_of to 10 (much slower):\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['best_of'] = 10\")\n",
        "print(\"\")\n",
        "print(\"3. For interviews with technical terms, add a prompt:\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['initial_prompt'] = 'Technical interview about...'\")\n",
        "print(\"\")\n",
        "print(\"4. For very long files, increase chunk_length:\")\n",
        "print(\"   TRANSCRIPTION_PARAMS['chunk_length'] = 120  # 2-minute chunks\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDlbJ4p-rFJa",
        "outputId": "bcdb31a9-f77f-4a78-ea9c-11d2e6c11868"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üöÄ Whisper V3 Ultimate - Quality Maximized Edition\n",
            "   Target GPU usage: 14GB for maximum accuracy\n",
            "   Features: Beam search, extended context, smart hallucination prevention\n",
            "============================================================\n",
            "\n",
            "üì¶ Checking dependencies...\n",
            "Installing Whisper...\n",
            "\n",
            "üéÆ GPU Configuration:\n",
            "   Device: Tesla T4\n",
            "   Total Memory: 15.8 GB\n",
            "   Allocated for processing: 15.0 GB\n",
            "   TF32 acceleration: Enabled\n",
            "\n",
            "üìä Scanning for files...\n",
            "   ‚è≥ Call Recording - 19Mar2025 0800 JD.wav (125.2 MB)\n",
            "   ‚è≥ Call Recording - 19Mar25 0900 - AJ.wav (77.2 MB)\n",
            "   ‚è≥ Call Recording - 19Mar25 1730 - MO.wav (87.2 MB)\n",
            "   ‚è≥ Call Recording - 20Mar2025 1200 LN.wav (179.8 MB)\n",
            "   ‚è≥ Call Recording - 26Mar2025 0830 SA.wav (94.2 MB)\n",
            "\n",
            "Files to process: 5 of 8\n",
            "\n",
            "‚è≥ Loading large-v3 for maximum quality...\n",
            "   This optimizes for accuracy over speed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [02:12<00:00, 23.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Model loaded in 164.4s\n",
            "   GPU Memory: 6.3GB used, 9.8GB reserved\n",
            "   Free for processing: 6.0GB\n",
            "\n",
            "======================================================================\n",
            "[1/2] Call Recording - 19Mar2025 0800 JD.wav\n",
            "======================================================================\n",
            "   File size: 125.2 MB\n",
            "   Estimated duration: 25:02.52\n",
            "   Using: Beam search (size=10), Best of 5\n",
            "   üß† Smart transcription mode engaged...\n",
            "   Attempt 1/3 (context=on)...Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/148857 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 2/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/148857 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 3/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/148857 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "\n",
            "‚ùå Failed: cannot access local variable 'result' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 278, in <cell line: 0>\n",
            "    result = smart_transcribe(model, audio_file, TRANSCRIPTION_PARAMS)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 173, in smart_transcribe\n",
            "    return result\n",
            "           ^^^^^^\n",
            "UnboundLocalError: cannot access local variable 'result' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Memory after cleanup: 6.3GB\n",
            "\n",
            "‚è≥ Cooling down for 10 seconds...\n",
            "\n",
            "======================================================================\n",
            "[2/2] Call Recording - 19Mar25 0900 - AJ.wav\n",
            "======================================================================\n",
            "   File size: 77.2 MB\n",
            "   Estimated duration: 15:25.97\n",
            "   Using: Beam search (size=10), Best of 5\n",
            "   üß† Smart transcription mode engaged...\n",
            "   Attempt 1/3 (context=on)...Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/91737 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 2/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/91737 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "   Attempt 3/3 (context=on)..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/91737 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ‚ùå Error: DecodingOptions.__init__() got an unexpected keyword argument 'temperature_increment_on_fallback'\n",
            "\n",
            "‚ùå Failed: cannot access local variable 'result' where it is not associated with a value\n",
            "\n",
            "üíæ Memory after cleanup: 6.3GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 278, in <cell line: 0>\n",
            "    result = smart_transcribe(model, audio_file, TRANSCRIPTION_PARAMS)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1-3841609909.py\", line 173, in smart_transcribe\n",
            "    return result\n",
            "           ^^^^^^\n",
            "UnboundLocalError: cannot access local variable 'result' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "‚úÖ SESSION COMPLETE\n",
            "   Processed: 2 files\n",
            "   Remaining: 3 files\n",
            "   Output format: *_ultimate.txt\n",
            "\n",
            "üí° Quality optimizations used:\n",
            "   - Beam search for better accuracy\n",
            "   - Best-of-N sampling\n",
            "   - Extended context windows\n",
            "   - Smart hallucination prevention\n",
            "   - 95% GPU memory utilization\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üí° FURTHER OPTIMIZATION OPTIONS\n",
            "======================================================================\n",
            "To push quality even higher, you can manually adjust:\n",
            "\n",
            "1. Increase beam_size to 20 (slower but more accurate):\n",
            "   TRANSCRIPTION_PARAMS['beam_size'] = 20\n",
            "\n",
            "2. Increase best_of to 10 (much slower):\n",
            "   TRANSCRIPTION_PARAMS['best_of'] = 10\n",
            "\n",
            "3. For interviews with technical terms, add a prompt:\n",
            "   TRANSCRIPTION_PARAMS['initial_prompt'] = 'Technical interview about...'\n",
            "\n",
            "4. For very long files, increase chunk_length:\n",
            "   TRANSCRIPTION_PARAMS['chunk_length'] = 120  # 2-minute chunks\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YlO8LjFzrFFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 SIMPLE & STABLE - NO MEMORY ISSUES\n",
        "# Clean implementation without parallel processing or aggressive allocation\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Checking dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION - SIMPLE & STABLE\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Simple transcription parameters\n",
        "TRANSCRIPTION_PARAMS = {\n",
        "    'temperature': 0.2,\n",
        "    'compression_ratio_threshold': 2.4,\n",
        "    'logprob_threshold': -1.0,\n",
        "    'no_speech_threshold': 0.6,\n",
        "    'condition_on_previous_text': True,\n",
        "    'word_timestamps': True,\n",
        "    'prepend_punctuations': '\"\\'\"¬ø([{-',\n",
        "    'append_punctuations': '\"\\'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ',\n",
        "    'beam_size': 5,  # Standard beam size\n",
        "    'best_of': 5,    # Standard best_of\n",
        "    'fp16': False,    # Use FP16 for efficiency\n",
        "}\n",
        "\n",
        "print(\"\\nüöÄ Whisper V3 Simple Transcription System\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MONITORING\n",
        "# ============================================\n",
        "def print_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "    else:\n",
        "        print(\"No GPU available\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL (SIMPLE)\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Clear any existing memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Load model normally\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "print(\"‚úì Model loaded successfully\")\n",
        "print_gpu_memory()\n",
        "\n",
        "# ============================================\n",
        "# POST-PROCESSING UTILITIES\n",
        "# ============================================\n",
        "def clean_text(text):\n",
        "    \"\"\"Basic text cleanup\"\"\"\n",
        "    # Fix common acronym patterns\n",
        "    text = text.replace(' h m h c ', ' HMHC ')\n",
        "    text = text.replace(' h n h m ', ' HMHC ')\n",
        "    text = text.replace(' c a ', ' CA ')\n",
        "    text = text.replace(' c o m ', '.com ')\n",
        "    text = text.replace(' . ', '.')\n",
        "\n",
        "    # Fix spacing around punctuation\n",
        "    text = text.replace(' ,', ',')\n",
        "    text = text.replace(' .', '.')\n",
        "    text = text.replace(' ?', '?')\n",
        "    text = text.replace(' !', '!')\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def is_quality_issue(segments):\n",
        "    \"\"\"Check for repetition or quality issues\"\"\"\n",
        "    if len(segments) < 10:\n",
        "        return False\n",
        "\n",
        "    # Check for excessive repetition\n",
        "    texts = [seg.get('text', '').strip().lower() for seg in segments[-10:]]\n",
        "    unique_texts = set(texts)\n",
        "\n",
        "    # If last 10 segments have less than 3 unique texts, there's likely repetition\n",
        "    return len(unique_texts) < 3\n",
        "\n",
        "# ============================================\n",
        "# MAIN TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file(audio_path):\n",
        "    \"\"\"Transcribe a single file with quality checks\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_transcript.txt\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {os.path.basename(audio_path)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        file_size_mb = os.path.getsize(audio_path) / (1024**2)\n",
        "        print(f\"File size: {file_size_mb:.1f} MB\")\n",
        "\n",
        "        # Transcribe with standard parameters\n",
        "        print(\"Transcribing...\")\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            language=None,  # Auto-detect language\n",
        "            task='transcribe',\n",
        "            verbose=False,\n",
        "            **TRANSCRIPTION_PARAMS\n",
        "        )\n",
        "\n",
        "        # Check for quality issues\n",
        "        segments = result.get('segments', [])\n",
        "        if is_quality_issue(segments):\n",
        "            print(\"‚ö†Ô∏è  Quality issue detected, retrying with adjusted parameters...\")\n",
        "\n",
        "            # Retry with different parameters\n",
        "            retry_params = TRANSCRIPTION_PARAMS.copy()\n",
        "            retry_params['temperature'] = 0.8\n",
        "            retry_params['condition_on_previous_text'] = False\n",
        "\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                task='transcribe',\n",
        "                verbose=False,\n",
        "                **retry_params\n",
        "            )\n",
        "            segments = result.get('segments', [])\n",
        "\n",
        "        # Process results\n",
        "        duration = segments[-1]['end'] if segments else 0\n",
        "        process_time = time.time() - start_time\n",
        "        speed_factor = duration / process_time if process_time > 0 else 0\n",
        "\n",
        "        print(f\"\\n‚úÖ Transcription complete!\")\n",
        "        print(f\"   Language: {result.get('language', 'unknown')}\")\n",
        "        print(f\"   Duration: {duration/60:.1f} minutes\")\n",
        "        print(f\"   Process time: {process_time:.1f} seconds\")\n",
        "        print(f\"   Speed: {speed_factor:.1f}x realtime\")\n",
        "        print_gpu_memory()\n",
        "\n",
        "        # Save transcript\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            # Header\n",
        "            f.write(f\"# Whisper Large-v3 Transcript\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Duration: {duration/60:.1f} minutes\\n\")\n",
        "            f.write(\"#\" + \"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "            # Write segments\n",
        "            for segment in segments:\n",
        "                start = segment['start']\n",
        "                end = segment['end']\n",
        "                text = clean_text(segment.get('text', ''))\n",
        "\n",
        "                if text:  # Only write non-empty segments\n",
        "                    f.write(f\"[{start:06.2f} ‚Üí {end:06.2f}] {text}\\n\")\n",
        "\n",
        "        print(f\"üíæ Saved: {os.path.basename(output_file)}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Clear GPU cache after each file\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining_files = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "\n",
        "        # Check if already processed\n",
        "        transcript_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_transcript.txt', '_final.txt', '_enhanced.txt', '_gpu_optimized.txt']\n",
        "        )\n",
        "\n",
        "        if not transcript_exists:\n",
        "            remaining_files.append(wav_file)\n",
        "\n",
        "    print(f\"\\nüìä Status:\")\n",
        "    print(f\"   Total files: {len(wav_files)}\")\n",
        "    print(f\"   Already processed: {len(wav_files) - len(remaining_files)}\")\n",
        "    print(f\"   To process: {len(remaining_files)}\")\n",
        "\n",
        "    if not remaining_files:\n",
        "        print(\"\\n‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files one by one (no parallel processing)\n",
        "    files_to_process = remaining_files[:5]  # Process up to 5 files per session\n",
        "    successful = 0\n",
        "\n",
        "    for idx, audio_file in enumerate(files_to_process):\n",
        "        if transcribe_file(audio_file):\n",
        "            successful += 1\n",
        "\n",
        "        # Cool down between files\n",
        "        if idx < len(files_to_process) - 1:\n",
        "            print(\"\\n‚è≥ Cooling down for 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print(f\"   Processed successfully: {successful}/{len(files_to_process)}\")\n",
        "    print(f\"   Remaining files: {len(remaining_files) - len(files_to_process)}\")\n",
        "\n",
        "    if len(remaining_files) > len(files_to_process):\n",
        "        print(\"\\nüí° Run again to process remaining files\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# NOTES\n",
        "# ============================================\n",
        "\"\"\"\n",
        "This simplified version:\n",
        "1. NO parallel processing - one file at a time\n",
        "2. NO aggressive memory allocation\n",
        "3. NO complex GPU optimization\n",
        "4. Just reliable, quality transcription\n",
        "\n",
        "GPU usage will be around 7-10GB, which is normal for large-v3.\n",
        "\n",
        "To increase speed slightly, you can adjust:\n",
        "- beam_size: Lower to 3 for faster processing\n",
        "- best_of: Lower to 3 for faster processing\n",
        "\n",
        "But the defaults (5/5) provide the best quality.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZapWLQM5lBhv",
        "outputId": "22188457-bb12-4970-fb7a-b79ccf4896ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Checking dependencies...\n",
            "‚úì Whisper already installed\n",
            "\n",
            "üöÄ Whisper V3 Simple Transcription System\n",
            "============================================================\n",
            "\n",
            "Loading Whisper large-v3...\n",
            "‚úì Model loaded successfully\n",
            "GPU Memory: 6.3GB allocated, 9.8GB reserved / 15.8GB total\n",
            "\n",
            "üìä Status:\n",
            "   Total files: 8\n",
            "   Already processed: 2\n",
            "   To process: 6\n",
            "\n",
            "============================================================\n",
            "Processing: Call Recording - 13Mar25 1300 HB.wav\n",
            "============================================================\n",
            "File size: 70.3 MB\n",
            "Transcribing...\n",
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83628/83628 [09:15<00:00, 150.50frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Transcription complete!\n",
            "   Language: en\n",
            "   Duration: 13.9 minutes\n",
            "   Process time: 560.5 seconds\n",
            "   Speed: 1.5x realtime\n",
            "GPU Memory: 6.3GB allocated, 10.8GB reserved / 15.8GB total\n",
            "üíæ Saved: Call Recording - 13Mar25 1300 HB_transcript.txt\n",
            "\n",
            "‚è≥ Cooling down for 5 seconds...\n",
            "\n",
            "============================================================\n",
            "Processing: Call Recording - 19Mar2025 0800 JD.wav\n",
            "============================================================\n",
            "File size: 125.2 MB\n",
            "Transcribing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3633120119.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;31m# ============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;31m# ============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-3633120119.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_to_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtranscribe_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0msuccessful\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-3633120119.py\u001b[0m in \u001b[0;36mtranscribe_file\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Transcribe with standard parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transcribing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         result = model.transcribe(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Auto-detect language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Pad 30-seconds of silence to the input audio, for slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mcontent_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mN_FRAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mcontent_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_frames\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHOP_LENGTH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gqqu21JYlBUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 GPU-OPTIMIZED - MAXIMUM MEMORY UTILIZATION\n",
        "# Aggressive GPU usage without speaker diarization\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úì PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing torch...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch'])\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# GPU OPTIMIZATION OPTIONS - MODIFY THESE!\n",
        "# ============================================\n",
        "GPU_OPTIONS = {\n",
        "    # OPTION 1: Parallel processing (risky but uses more memory)\n",
        "    'PARALLEL_FILES': 1,  # Process 2 files simultaneously (set to 1 for safe mode)\n",
        "\n",
        "    # OPTION 2: Pre-cache audio in GPU memory\n",
        "    'PRELOAD_TO_GPU': True,  # Load audio directly to GPU tensors\n",
        "\n",
        "    # OPTION 3: Increase internal mel spectrogram cache\n",
        "    'MEL_CACHE_SIZE': 10,  # Cache multiple mel spectrograms\n",
        "\n",
        "    # OPTION 4: Keep model in higher precision (uses more memory)\n",
        "    'USE_FLOAT32': False,  # Set True to use float32 (doubles memory usage)\n",
        "\n",
        "    # OPTION 5: Decode parameters affecting memory\n",
        "    'BEAM_SIZE': 5,  # Increase for more memory usage (default 5)\n",
        "    'BEST_OF': 5,   # Increase for more memory usage (default 5)\n",
        "\n",
        "    # OPTION 6: Process longer chunks at once\n",
        "    'N_FRAMES': 4500,  # Default is 3000, increase to process more at once\n",
        "}\n",
        "\n",
        "print(\"üöÄ Whisper V3 GPU-Optimized System\")\n",
        "print(\"=\"*60)\n",
        "print(\"GPU Memory Optimization Settings:\")\n",
        "for key, value in GPU_OPTIONS.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MEMORY MONITORING\n",
        "# ============================================\n",
        "def get_gpu_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "def print_gpu_stats(prefix=\"\"):\n",
        "    alloc, reserved, total = get_gpu_stats()\n",
        "    print(f\"{prefix}GPU Memory: {alloc:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "\n",
        "# ============================================\n",
        "# AGGRESSIVE GPU MEMORY ALLOCATION\n",
        "# ============================================\n",
        "PERSISTENT_TENSORS = []  # Keep these alive throughout execution\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Force PyTorch to allocate more memory upfront\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "\n",
        "    # Enable memory efficient attention if available\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "    # Allocate persistent tensors to reach target memory usage\n",
        "    print(\"Pre-allocating GPU memory aggressively...\")\n",
        "    target_gb = GPU_OPTIONS.get('ALLOCATE_EXTRA_GB', 4)\n",
        "\n",
        "    try:\n",
        "        # Allocate large persistent tensors\n",
        "        for i in range(target_gb):\n",
        "            # 1GB tensor that stays in memory\n",
        "            tensor = torch.zeros(256, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "            PERSISTENT_TENSORS.append(tensor)\n",
        "            print_gpu_stats(f\"  After allocation {i+1}GB: \")\n",
        "\n",
        "        # Additional allocation to reach 15GB\n",
        "        if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "            print(\"  Allocating extra memory for parallel processing...\")\n",
        "            for i in range(2):\n",
        "                tensor = torch.zeros(512, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "                PERSISTENT_TENSORS.append(tensor)\n",
        "                print_gpu_stats(f\"  Extra allocation {i+1}: \")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"  Reached memory limit: {e}\")\n",
        "\n",
        "    print(f\"  Total persistent tensors: {len(PERSISTENT_TENSORS)}\")\n",
        "    torch.cuda.synchronize()  # Ensure allocations are complete\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL WITH OPTIONS\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model with specific dtype\n",
        "dtype = torch.float32 if GPU_OPTIONS['USE_FLOAT32'] else torch.float16\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "# Force model to use more memory\n",
        "if device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']:\n",
        "    model = model.half()  # FP16 mode\n",
        "else:\n",
        "    model = model.float()  # FP32 mode (uses 2x memory)\n",
        "\n",
        "print_gpu_stats(\"After model load: \")\n",
        "\n",
        "# Modify model's n_frames if specified\n",
        "if hasattr(model, 'dims') and hasattr(model.dims, 'n_audio_ctx'):\n",
        "    original_frames = model.dims.n_audio_ctx\n",
        "    model.dims.n_audio_ctx = GPU_OPTIONS['N_FRAMES']\n",
        "    print(f\"Modified n_frames: {original_frames} ‚Üí {GPU_OPTIONS['N_FRAMES']}\")\n",
        "\n",
        "# ============================================\n",
        "# MEL SPECTROGRAM CACHE\n",
        "# ============================================\n",
        "mel_cache = {}\n",
        "mel_cache_lock = threading.Lock()\n",
        "\n",
        "def get_mel_cached(audio_path, model):\n",
        "    \"\"\"Cache mel spectrograms in GPU memory\"\"\"\n",
        "    with mel_cache_lock:\n",
        "        if audio_path in mel_cache:\n",
        "            return mel_cache[audio_path]\n",
        "\n",
        "        # Load and compute mel\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "        # Cache if under limit\n",
        "        if len(mel_cache) < GPU_OPTIONS['MEL_CACHE_SIZE']:\n",
        "            mel_cache[audio_path] = mel\n",
        "            print_gpu_stats(f\"  Cached mel #{len(mel_cache)}: \")\n",
        "\n",
        "        return mel\n",
        "\n",
        "# ============================================\n",
        "# PARALLEL TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file_aggressive(model, audio_path, file_idx=0):\n",
        "    \"\"\"Transcribe with aggressive GPU usage\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_gpu_optimized.txt\")\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n[Worker {file_idx}] Processing: {os.path.basename(audio_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if GPU_OPTIONS['PRELOAD_TO_GPU']:\n",
        "            # Load audio and convert to GPU tensor with correct dtype\n",
        "            print(f\"[Worker {file_idx}] Pre-loading to GPU...\")\n",
        "            audio = whisper.load_audio(audio_path)\n",
        "\n",
        "            # Match tensor dtype to model dtype\n",
        "            model_dtype = next(model.parameters()).dtype\n",
        "            audio_tensor = torch.from_numpy(audio).to(device=device, dtype=torch.float32)\n",
        "\n",
        "            # Create multiple copies to use more GPU memory\n",
        "            audio_copies = [audio_tensor.clone() for _ in range(3)]\n",
        "            print_gpu_stats(f\"[Worker {file_idx}] After loading copies: \")\n",
        "\n",
        "            # Process from numpy (Whisper expects numpy)\n",
        "            result = model.transcribe(\n",
        "                audio,  # Use original numpy array\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Keep tensors in memory during processing to maintain GPU usage\n",
        "            dummy_computation = torch.sum(torch.stack(audio_copies))\n",
        "\n",
        "        else:\n",
        "            # Standard transcription\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[Worker {file_idx}] Completed in {process_time:.1f}s\")\n",
        "        print_gpu_stats(f\"[Worker {file_idx}] During processing: \")\n",
        "\n",
        "        # Save transcript (no speaker info)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Whisper Large-v3 Transcript (GPU Optimized)\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now()}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Process time: {process_time:.1f}s\\n\")\n",
        "            f.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                if text:\n",
        "                    # Basic text cleanup\n",
        "                    text = text.replace(' h m h c ', ' HMHC ')\n",
        "                    text = text.replace(' c a ', ' CA ')\n",
        "                    text = text.replace(' . ', '.')\n",
        "\n",
        "                    f.write(f\"[{segment['start']:06.2f} ‚Üí {segment['end']:06.2f}] {text}\\n\")\n",
        "\n",
        "        return True, output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Worker {file_idx}] Error: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING WITH PARALLEL OPTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "        output_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_gpu_optimized.txt', '_final.txt', '_enhanced.txt', '_ultimate.txt']\n",
        "        )\n",
        "        if not output_exists:\n",
        "            remaining.append(wav_file)\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if not remaining:\n",
        "        print(\"‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files\n",
        "    files_to_process = remaining[:6]  # Process up to 6 files this session\n",
        "\n",
        "    if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "        # PARALLEL PROCESSING - Uses more GPU memory\n",
        "        print(f\"\\nüî• PARALLEL MODE: Processing {GPU_OPTIONS['PARALLEL_FILES']} files simultaneously\")\n",
        "        print(\"‚ö†Ô∏è  Warning: This may cause out-of-memory errors!\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=GPU_OPTIONS['PARALLEL_FILES']) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            # Submit files for parallel processing\n",
        "            for i, audio_file in enumerate(files_to_process[:GPU_OPTIONS['PARALLEL_FILES'] * 2]):\n",
        "                if len(futures) >= GPU_OPTIONS['PARALLEL_FILES']:\n",
        "                    # Wait for a slot to free up\n",
        "                    done, _ = as_completed(futures, timeout=None).__next__()\n",
        "                    futures.pop(done)\n",
        "\n",
        "                future = executor.submit(transcribe_file_aggressive, model, audio_file, i)\n",
        "                futures[future] = audio_file\n",
        "\n",
        "                # Brief pause between submissions\n",
        "                time.sleep(1)\n",
        "\n",
        "            # Wait for remaining tasks\n",
        "            for future in as_completed(futures):\n",
        "                success, result = future.result()\n",
        "                if success:\n",
        "                    print(f\"‚úì Completed: {os.path.basename(futures[future])}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed: {os.path.basename(futures[future])}\")\n",
        "\n",
        "    else:\n",
        "        # SEQUENTIAL PROCESSING - Safer but uses less memory\n",
        "        print(f\"\\nüîí SEQUENTIAL MODE: Processing files one at a time\")\n",
        "\n",
        "        for i, audio_file in enumerate(files_to_process):\n",
        "            success, result = transcribe_file_aggressive(model, audio_file, i)\n",
        "\n",
        "            if success:\n",
        "                print(f\"‚úì Saved: {os.path.basename(result)}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Failed: {result}\")\n",
        "\n",
        "            # Monitor memory between files\n",
        "            print_gpu_stats(\"Between files: \")\n",
        "\n",
        "            # Brief cooldown\n",
        "            if i < len(files_to_process) - 1:\n",
        "                time.sleep(3)\n",
        "\n",
        "    # Final cleanup\n",
        "    mel_cache.clear()\n",
        "    PERSISTENT_TENSORS.clear()  # Clear persistent memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print_gpu_stats(\"Final: \")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN MAIN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n‚öôÔ∏è  GPU OPTIMIZATION TIPS:\")\n",
        "    print(\"1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\")\n",
        "    print(\"2. Set USE_FLOAT32=True to double memory usage\")\n",
        "    print(\"3. Increase BEAM_SIZE and BEST_OF for more memory use\")\n",
        "    print(\"4. Set PRELOAD_TO_GPU=True to cache audio in GPU\")\n",
        "    print(\"\\n‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# EXPERIMENTAL: EXTREME GPU USAGE\n",
        "# ============================================\n",
        "\"\"\"\n",
        "FOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\n",
        "\n",
        "GPU_OPTIONS = {\n",
        "    'PARALLEL_FILES': 3,      # Process 3 files at once\n",
        "    'PRELOAD_TO_GPU': True,   # Cache in GPU\n",
        "    'MEL_CACHE_SIZE': 20,     # Large cache\n",
        "    'USE_FLOAT32': True,      # Double precision (2x memory)\n",
        "    'BEAM_SIZE': 20,          # Very large beam\n",
        "    'BEST_OF': 20,            # Very large sampling\n",
        "    'N_FRAMES': 6000,         # Process huge chunks\n",
        "}\n",
        "\n",
        "This will likely crash but will definitely use all GPU memory!\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trvbmm6pkJdu",
        "outputId": "675a0c43-8220-4a7b-dccf-96737a02b778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies...\n",
            "‚úì Whisper already installed\n",
            "‚úì PyTorch already installed\n",
            "üöÄ Whisper V3 GPU-Optimized System\n",
            "============================================================\n",
            "GPU Memory Optimization Settings:\n",
            "  PARALLEL_FILES: 1\n",
            "  PRELOAD_TO_GPU: True\n",
            "  MEL_CACHE_SIZE: 10\n",
            "  USE_FLOAT32: False\n",
            "  BEAM_SIZE: 5\n",
            "  BEST_OF: 5\n",
            "  N_FRAMES: 4500\n",
            "============================================================\n",
            "Pre-allocating GPU memory aggressively...\n",
            "  Reached memory limit: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 646.12 MiB is free. Process 50434 has 14.11 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 13.70 GiB is allocated by PyTorch, and 306.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "  Total persistent tensors: 0\n",
            "\n",
            "Loading Whisper large-v3...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDMn5NiWkJRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 GPU-OPTIMIZED - MAXIMUM MEMORY UTILIZATION\n",
        "# Aggressive GPU usage without speaker diarization\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úì PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing torch...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch'])\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# GPU OPTIMIZATION OPTIONS - MODIFY THESE!\n",
        "# ============================================\n",
        "GPU_OPTIONS = {\n",
        "    # OPTION 1: Parallel processing (risky but uses more memory)\n",
        "    'PARALLEL_FILES': 2,  # Process 2 files simultaneously (set to 1 for safe mode)\n",
        "\n",
        "    # OPTION 2: Pre-cache audio in GPU memory\n",
        "    'PRELOAD_TO_GPU': False,  # Load audio directly to GPU tensors\n",
        "\n",
        "    # OPTION 3: Increase internal mel spectrogram cache\n",
        "    'MEL_CACHE_SIZE': 5,  # Cache multiple mel spectrograms\n",
        "\n",
        "    # OPTION 4: Keep model in higher precision (uses more memory)\n",
        "    'USE_FLOAT32': False,  # Set True to use float32 (doubles memory usage)\n",
        "\n",
        "    # OPTION 5: Decode parameters affecting memory\n",
        "    'BEAM_SIZE': 5,  # Increase for more memory usage (default 5)\n",
        "    'BEST_OF': 5,   # Increase for more memory usage (default 5)\n",
        "\n",
        "    # OPTION 6: Process longer chunks at once\n",
        "    'N_FRAMES': 4500,  # Default is 3000, increase to process more at once\n",
        "}\n",
        "\n",
        "print(\"üöÄ Whisper V3 GPU-Optimized System\")\n",
        "print(\"=\"*60)\n",
        "print(\"GPU Memory Optimization Settings:\")\n",
        "for key, value in GPU_OPTIONS.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MEMORY MONITORING\n",
        "# ============================================\n",
        "def get_gpu_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "def print_gpu_stats(prefix=\"\"):\n",
        "    alloc, reserved, total = get_gpu_stats()\n",
        "    print(f\"{prefix}GPU Memory: {alloc:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "\n",
        "# ============================================\n",
        "# AGGRESSIVE GPU MEMORY ALLOCATION\n",
        "# ============================================\n",
        "PERSISTENT_TENSORS = []  # Keep these alive throughout execution\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Force PyTorch to allocate more memory upfront\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "\n",
        "    # Enable memory efficient attention if available\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "    # Allocate persistent tensors to reach target memory usage\n",
        "    print(\"Pre-allocating GPU memory aggressively...\")\n",
        "    target_gb = GPU_OPTIONS.get('ALLOCATE_EXTRA_GB', 4)\n",
        "\n",
        "    try:\n",
        "        # Allocate large persistent tensors\n",
        "        for i in range(target_gb):\n",
        "            # 1GB tensor that stays in memory\n",
        "            tensor = torch.zeros(256, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "            PERSISTENT_TENSORS.append(tensor)\n",
        "            print_gpu_stats(f\"  After allocation {i+1}GB: \")\n",
        "\n",
        "        # Additional allocation to reach 15GB\n",
        "        if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "            print(\"  Allocating extra memory for parallel processing...\")\n",
        "            for i in range(2):\n",
        "                tensor = torch.zeros(512, 1024, 1024, device='cuda', dtype=torch.float32)\n",
        "                PERSISTENT_TENSORS.append(tensor)\n",
        "                print_gpu_stats(f\"  Extra allocation {i+1}: \")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"  Reached memory limit: {e}\")\n",
        "\n",
        "    print(f\"  Total persistent tensors: {len(PERSISTENT_TENSORS)}\")\n",
        "    torch.cuda.synchronize()  # Ensure allocations are complete\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL WITH OPTIONS\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model with specific dtype\n",
        "dtype = torch.float32 if GPU_OPTIONS['USE_FLOAT32'] else torch.float16\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "# Force model to use more memory\n",
        "if device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']:\n",
        "    model = model.half()  # FP16 mode\n",
        "else:\n",
        "    model = model.float()  # FP32 mode (uses 2x memory)\n",
        "\n",
        "print_gpu_stats(\"After model load: \")\n",
        "\n",
        "# Modify model's n_frames if specified\n",
        "if hasattr(model, 'dims') and hasattr(model.dims, 'n_audio_ctx'):\n",
        "    original_frames = model.dims.n_audio_ctx\n",
        "    model.dims.n_audio_ctx = GPU_OPTIONS['N_FRAMES']\n",
        "    print(f\"Modified n_frames: {original_frames} ‚Üí {GPU_OPTIONS['N_FRAMES']}\")\n",
        "\n",
        "# ============================================\n",
        "# MEL SPECTROGRAM CACHE\n",
        "# ============================================\n",
        "mel_cache = {}\n",
        "mel_cache_lock = threading.Lock()\n",
        "\n",
        "def get_mel_cached(audio_path, model):\n",
        "    \"\"\"Cache mel spectrograms in GPU memory\"\"\"\n",
        "    with mel_cache_lock:\n",
        "        if audio_path in mel_cache:\n",
        "            return mel_cache[audio_path]\n",
        "\n",
        "        # Load and compute mel\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "        # Cache if under limit\n",
        "        if len(mel_cache) < GPU_OPTIONS['MEL_CACHE_SIZE']:\n",
        "            mel_cache[audio_path] = mel\n",
        "            print_gpu_stats(f\"  Cached mel #{len(mel_cache)}: \")\n",
        "\n",
        "        return mel\n",
        "\n",
        "# ============================================\n",
        "# PARALLEL TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file_aggressive(model, audio_path, file_idx=0):\n",
        "    \"\"\"Transcribe with aggressive GPU usage\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_gpu_optimized.txt\")\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n[Worker {file_idx}] Processing: {os.path.basename(audio_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if GPU_OPTIONS['PRELOAD_TO_GPU']:\n",
        "            # Load audio and convert to GPU tensor with correct dtype\n",
        "            print(f\"[Worker {file_idx}] Pre-loading to GPU...\")\n",
        "            audio = whisper.load_audio(audio_path)\n",
        "\n",
        "            # Match tensor dtype to model dtype\n",
        "            model_dtype = next(model.parameters()).dtype\n",
        "            audio_tensor = torch.from_numpy(audio).to(device=device, dtype=torch.float32)\n",
        "\n",
        "            # Create multiple copies to use more GPU memory\n",
        "            audio_copies = [audio_tensor.clone() for _ in range(3)]\n",
        "            print_gpu_stats(f\"[Worker {file_idx}] After loading copies: \")\n",
        "\n",
        "            # Process from numpy (Whisper expects numpy)\n",
        "            result = model.transcribe(\n",
        "                audio,  # Use original numpy array\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Keep tensors in memory during processing to maintain GPU usage\n",
        "            dummy_computation = torch.sum(torch.stack(audio_copies))\n",
        "\n",
        "        else:\n",
        "            # Standard transcription\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[Worker {file_idx}] Completed in {process_time:.1f}s\")\n",
        "        print_gpu_stats(f\"[Worker {file_idx}] During processing: \")\n",
        "\n",
        "        # Save transcript (no speaker info)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Whisper Large-v3 Transcript (GPU Optimized)\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now()}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Process time: {process_time:.1f}s\\n\")\n",
        "            f.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                if text:\n",
        "                    # Basic text cleanup\n",
        "                    text = text.replace(' h m h c ', ' HMHC ')\n",
        "                    text = text.replace(' c a ', ' CA ')\n",
        "                    text = text.replace(' . ', '.')\n",
        "\n",
        "                    f.write(f\"[{segment['start']:06.2f} ‚Üí {segment['end']:06.2f}] {text}\\n\")\n",
        "\n",
        "        return True, output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Worker {file_idx}] Error: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING WITH PARALLEL OPTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "        output_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_gpu_optimized.txt', '_final.txt', '_enhanced.txt', '_ultimate.txt']\n",
        "        )\n",
        "        if not output_exists:\n",
        "            remaining.append(wav_file)\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if not remaining:\n",
        "        print(\"‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files\n",
        "    files_to_process = remaining[:6]  # Process up to 6 files this session\n",
        "\n",
        "    if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "        # PARALLEL PROCESSING - Uses more GPU memory\n",
        "        print(f\"\\nüî• PARALLEL MODE: Processing {GPU_OPTIONS['PARALLEL_FILES']} files simultaneously\")\n",
        "        print(\"‚ö†Ô∏è  Warning: This may cause out-of-memory errors!\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=GPU_OPTIONS['PARALLEL_FILES']) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            # Submit files for parallel processing\n",
        "            for i, audio_file in enumerate(files_to_process[:GPU_OPTIONS['PARALLEL_FILES'] * 2]):\n",
        "                if len(futures) >= GPU_OPTIONS['PARALLEL_FILES']:\n",
        "                    # Wait for a slot to free up\n",
        "                    done, _ = as_completed(futures, timeout=None).__next__()\n",
        "                    futures.pop(done)\n",
        "\n",
        "                future = executor.submit(transcribe_file_aggressive, model, audio_file, i)\n",
        "                futures[future] = audio_file\n",
        "\n",
        "                # Brief pause between submissions\n",
        "                time.sleep(1)\n",
        "\n",
        "            # Wait for remaining tasks\n",
        "            for future in as_completed(futures):\n",
        "                success, result = future.result()\n",
        "                if success:\n",
        "                    print(f\"‚úì Completed: {os.path.basename(futures[future])}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed: {os.path.basename(futures[future])}\")\n",
        "\n",
        "    else:\n",
        "        # SEQUENTIAL PROCESSING - Safer but uses less memory\n",
        "        print(f\"\\nüîí SEQUENTIAL MODE: Processing files one at a time\")\n",
        "\n",
        "        for i, audio_file in enumerate(files_to_process):\n",
        "            success, result = transcribe_file_aggressive(model, audio_file, i)\n",
        "\n",
        "            if success:\n",
        "                print(f\"‚úì Saved: {os.path.basename(result)}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Failed: {result}\")\n",
        "\n",
        "            # Monitor memory between files\n",
        "            print_gpu_stats(\"Between files: \")\n",
        "\n",
        "            # Brief cooldown\n",
        "            if i < len(files_to_process) - 1:\n",
        "                time.sleep(3)\n",
        "\n",
        "    # Final cleanup\n",
        "    mel_cache.clear()\n",
        "    PERSISTENT_TENSORS.clear()  # Clear persistent memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print_gpu_stats(\"Final: \")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN MAIN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n‚öôÔ∏è  GPU OPTIMIZATION TIPS:\")\n",
        "    print(\"1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\")\n",
        "    print(\"2. Set USE_FLOAT32=True to double memory usage\")\n",
        "    print(\"3. Increase BEAM_SIZE and BEST_OF for more memory use\")\n",
        "    print(\"4. Set PRELOAD_TO_GPU=True to cache audio in GPU\")\n",
        "    print(\"\\n‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# EXPERIMENTAL: EXTREME GPU USAGE\n",
        "# ============================================\n",
        "\"\"\"\n",
        "FOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\n",
        "\n",
        "GPU_OPTIONS = {\n",
        "    'PARALLEL_FILES': 3,      # Process 3 files at once\n",
        "    'PRELOAD_TO_GPU': True,   # Cache in GPU\n",
        "    'MEL_CACHE_SIZE': 20,     # Large cache\n",
        "    'USE_FLOAT32': True,      # Double precision (2x memory)\n",
        "    'BEAM_SIZE': 20,          # Very large beam\n",
        "    'BEST_OF': 20,            # Very large sampling\n",
        "    'N_FRAMES': 6000,         # Process huge chunks\n",
        "}\n",
        "\n",
        "This will likely crash but will definitely use all GPU memory!\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "3J4kvP0JjAaq",
        "outputId": "0edd7294-08fa-47d2-df34-72635c94d64f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies...\n",
            "‚úì Whisper already installed\n",
            "‚úì PyTorch already installed\n",
            "üöÄ Whisper V3 GPU-Optimized System\n",
            "============================================================\n",
            "GPU Memory Optimization Settings:\n",
            "  PARALLEL_FILES: 2\n",
            "  PRELOAD_TO_GPU: False\n",
            "  MEL_CACHE_SIZE: 5\n",
            "  USE_FLOAT32: False\n",
            "  BEAM_SIZE: 5\n",
            "  BEST_OF: 5\n",
            "  N_FRAMES: 4500\n",
            "============================================================\n",
            "Pre-allocating GPU memory aggressively...\n",
            "  After allocation 1GB: GPU Memory: 1.1GB allocated, 1.1GB reserved / 15.8GB total\n",
            "  After allocation 2GB: GPU Memory: 2.1GB allocated, 2.1GB reserved / 15.8GB total\n",
            "  After allocation 3GB: GPU Memory: 3.2GB allocated, 3.2GB reserved / 15.8GB total\n",
            "  After allocation 4GB: GPU Memory: 4.3GB allocated, 4.3GB reserved / 15.8GB total\n",
            "  Allocating extra memory for parallel processing...\n",
            "  Extra allocation 1: GPU Memory: 6.4GB allocated, 6.4GB reserved / 15.8GB total\n",
            "  Extra allocation 2: GPU Memory: 8.6GB allocated, 8.6GB reserved / 15.8GB total\n",
            "  Total persistent tensors: 6\n",
            "\n",
            "Loading Whisper large-v3...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 646.12 MiB is free. Process 40798 has 14.11 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 13.70 GiB is allocated by PyTorch, and 306.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2040763907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m# Load model with specific dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mGPU_OPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USE_FLOAT32'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"large-v3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# Force model to use more memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alignment_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 646.12 MiB is free. Process 40798 has 14.11 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 13.70 GiB is allocated by PyTorch, and 306.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78KyFu68jAPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WHISPER V3 GPU-OPTIMIZED - MAXIMUM MEMORY UTILIZATION\n",
        "# Aggressive GPU usage without speaker diarization\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing openai-whisper...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper installed\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úì PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing torch...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch'])\n",
        "    import torch\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# GPU OPTIMIZATION OPTIONS - MODIFY THESE!\n",
        "# ============================================\n",
        "GPU_OPTIONS = {\n",
        "    # OPTION 1: Parallel processing (risky but uses more memory)\n",
        "    'PARALLEL_FILES': 1,  # Process 2 files simultaneously (set to 1 for safe mode)\n",
        "\n",
        "    # OPTION 2: Pre-cache audio in GPU memory\n",
        "    'PRELOAD_TO_GPU': True,  # Load audio directly to GPU tensors\n",
        "\n",
        "    # OPTION 3: Increase internal mel spectrogram cache\n",
        "    'MEL_CACHE_SIZE': 10,  # Cache multiple mel spectrograms\n",
        "\n",
        "    # OPTION 4: Keep model in higher precision (uses more memory)\n",
        "    'USE_FLOAT32': False,  # Set True to use float32 (doubles memory usage)\n",
        "\n",
        "    # OPTION 5: Decode parameters affecting memory\n",
        "    'BEAM_SIZE': 10,  # Increase for more memory usage (default 5)\n",
        "    'BEST_OF': 10,   # Increase for more memory usage (default 5)\n",
        "\n",
        "    # OPTION 6: Process longer chunks at once\n",
        "    'N_FRAMES': 5000,  # Default is 3000, increase to process more at once\n",
        "}\n",
        "\n",
        "print(\"üöÄ Whisper V3 GPU-Optimized System\")\n",
        "print(\"=\"*60)\n",
        "print(\"GPU Memory Optimization Settings:\")\n",
        "for key, value in GPU_OPTIONS.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# GPU MEMORY MONITORING\n",
        "# ============================================\n",
        "def get_gpu_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "def print_gpu_stats(prefix=\"\"):\n",
        "    alloc, reserved, total = get_gpu_stats()\n",
        "    print(f\"{prefix}GPU Memory: {alloc:.1f}GB allocated, {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
        "\n",
        "# ============================================\n",
        "# AGGRESSIVE GPU MEMORY ALLOCATION\n",
        "# ============================================\n",
        "if torch.cuda.is_available():\n",
        "    # Force PyTorch to allocate more memory upfront\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory\n",
        "\n",
        "    # Enable memory efficient attention if available\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "    # Allocate dummy tensors to reserve memory\n",
        "    print(\"Pre-allocating GPU memory...\")\n",
        "    dummy_tensors = []\n",
        "    try:\n",
        "        # Try to allocate 2GB chunks until we hit limit\n",
        "        for i in range(5):\n",
        "            dummy = torch.zeros(256, 1024, 1024, device='cuda')  # ~1GB each\n",
        "            dummy_tensors.append(dummy)\n",
        "            print_gpu_stats(f\"  After allocation {i+1}: \")\n",
        "    except RuntimeError:\n",
        "        print(\"  Reached memory limit\")\n",
        "\n",
        "    # Clear dummy tensors but keep memory reserved\n",
        "    dummy_tensors.clear()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL WITH OPTIONS\n",
        "# ============================================\n",
        "print(\"\\nLoading Whisper large-v3...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model with specific dtype\n",
        "dtype = torch.float32 if GPU_OPTIONS['USE_FLOAT32'] else torch.float16\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "\n",
        "# Force model to use more memory\n",
        "if device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']:\n",
        "    model = model.half()  # FP16 mode\n",
        "else:\n",
        "    model = model.float()  # FP32 mode (uses 2x memory)\n",
        "\n",
        "print_gpu_stats(\"After model load: \")\n",
        "\n",
        "# Modify model's n_frames if specified\n",
        "if hasattr(model, 'dims') and hasattr(model.dims, 'n_audio_ctx'):\n",
        "    original_frames = model.dims.n_audio_ctx\n",
        "    model.dims.n_audio_ctx = GPU_OPTIONS['N_FRAMES']\n",
        "    print(f\"Modified n_frames: {original_frames} ‚Üí {GPU_OPTIONS['N_FRAMES']}\")\n",
        "\n",
        "# ============================================\n",
        "# MEL SPECTROGRAM CACHE\n",
        "# ============================================\n",
        "mel_cache = {}\n",
        "mel_cache_lock = threading.Lock()\n",
        "\n",
        "def get_mel_cached(audio_path, model):\n",
        "    \"\"\"Cache mel spectrograms in GPU memory\"\"\"\n",
        "    with mel_cache_lock:\n",
        "        if audio_path in mel_cache:\n",
        "            return mel_cache[audio_path]\n",
        "\n",
        "        # Load and compute mel\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "        # Cache if under limit\n",
        "        if len(mel_cache) < GPU_OPTIONS['MEL_CACHE_SIZE']:\n",
        "            mel_cache[audio_path] = mel\n",
        "            print_gpu_stats(f\"  Cached mel #{len(mel_cache)}: \")\n",
        "\n",
        "        return mel\n",
        "\n",
        "# ============================================\n",
        "# PARALLEL TRANSCRIPTION FUNCTION\n",
        "# ============================================\n",
        "def transcribe_file_aggressive(model, audio_path, file_idx=0):\n",
        "    \"\"\"Transcribe with aggressive GPU usage\"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    output_file = os.path.join(OUTPUT_PATH, f\"{base_name}_gpu_optimized.txt\")\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n[Worker {file_idx}] Processing: {os.path.basename(audio_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if GPU_OPTIONS['PRELOAD_TO_GPU']:\n",
        "            # Load audio and convert to GPU tensor immediately\n",
        "            print(f\"[Worker {file_idx}] Pre-loading to GPU...\")\n",
        "            audio = whisper.load_audio(audio_path)\n",
        "            audio_tensor = torch.from_numpy(audio).to(device)\n",
        "\n",
        "            # Process from GPU tensor\n",
        "            result = model.transcribe(\n",
        "                audio_tensor.cpu().numpy(),  # Whisper expects numpy\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Keep tensor in GPU memory during processing\n",
        "            audio_tensor = audio_tensor.contiguous()\n",
        "\n",
        "        else:\n",
        "            # Standard transcription\n",
        "            result = model.transcribe(\n",
        "                audio_path,\n",
        "                language=None,\n",
        "                temperature=0.2,\n",
        "                beam_size=GPU_OPTIONS['BEAM_SIZE'],\n",
        "                best_of=GPU_OPTIONS['BEST_OF'],\n",
        "                fp16=(device == \"cuda\" and not GPU_OPTIONS['USE_FLOAT32']),\n",
        "                condition_on_previous_text=True,\n",
        "                word_timestamps=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[Worker {file_idx}] Completed in {process_time:.1f}s\")\n",
        "        print_gpu_stats(f\"[Worker {file_idx}] During processing: \")\n",
        "\n",
        "        # Save transcript (no speaker info)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Whisper Large-v3 Transcript (GPU Optimized)\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(audio_path)}\\n\")\n",
        "            f.write(f\"# Date: {datetime.now()}\\n\")\n",
        "            f.write(f\"# Language: {result.get('language', 'unknown')}\\n\")\n",
        "            f.write(f\"# Process time: {process_time:.1f}s\\n\")\n",
        "            f.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                if text:\n",
        "                    # Basic text cleanup\n",
        "                    text = text.replace(' h m h c ', ' HMHC ')\n",
        "                    text = text.replace(' c a ', ' CA ')\n",
        "                    text = text.replace(' . ', '.')\n",
        "\n",
        "                    f.write(f\"[{segment['start']:06.2f} ‚Üí {segment['end']:06.2f}] {text}\\n\")\n",
        "\n",
        "        return True, output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Worker {file_idx}] Error: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING WITH PARALLEL OPTION\n",
        "# ============================================\n",
        "def main():\n",
        "    # Find files to process\n",
        "    wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "    remaining = []\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "        output_exists = any(\n",
        "            os.path.exists(os.path.join(OUTPUT_PATH, f\"{base_name}{suffix}\"))\n",
        "            for suffix in ['_gpu_optimized.txt', '_final.txt', '_enhanced.txt', '_ultimate.txt']\n",
        "        )\n",
        "        if not output_exists:\n",
        "            remaining.append(wav_file)\n",
        "\n",
        "    print(f\"\\nFiles to process: {len(remaining)}\")\n",
        "\n",
        "    if not remaining:\n",
        "        print(\"‚úÖ All files already processed!\")\n",
        "        return\n",
        "\n",
        "    # Process files\n",
        "    files_to_process = remaining[:6]  # Process up to 6 files this session\n",
        "\n",
        "    if GPU_OPTIONS['PARALLEL_FILES'] > 1:\n",
        "        # PARALLEL PROCESSING - Uses more GPU memory\n",
        "        print(f\"\\nüî• PARALLEL MODE: Processing {GPU_OPTIONS['PARALLEL_FILES']} files simultaneously\")\n",
        "        print(\"‚ö†Ô∏è  Warning: This may cause out-of-memory errors!\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=GPU_OPTIONS['PARALLEL_FILES']) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            # Submit files for parallel processing\n",
        "            for i, audio_file in enumerate(files_to_process[:GPU_OPTIONS['PARALLEL_FILES'] * 2]):\n",
        "                if len(futures) >= GPU_OPTIONS['PARALLEL_FILES']:\n",
        "                    # Wait for a slot to free up\n",
        "                    done, _ = as_completed(futures, timeout=None).__next__()\n",
        "                    futures.pop(done)\n",
        "\n",
        "                future = executor.submit(transcribe_file_aggressive, model, audio_file, i)\n",
        "                futures[future] = audio_file\n",
        "\n",
        "                # Brief pause between submissions\n",
        "                time.sleep(1)\n",
        "\n",
        "            # Wait for remaining tasks\n",
        "            for future in as_completed(futures):\n",
        "                success, result = future.result()\n",
        "                if success:\n",
        "                    print(f\"‚úì Completed: {os.path.basename(futures[future])}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed: {os.path.basename(futures[future])}\")\n",
        "\n",
        "    else:\n",
        "        # SEQUENTIAL PROCESSING - Safer but uses less memory\n",
        "        print(f\"\\nüîí SEQUENTIAL MODE: Processing files one at a time\")\n",
        "\n",
        "        for i, audio_file in enumerate(files_to_process):\n",
        "            success, result = transcribe_file_aggressive(model, audio_file, i)\n",
        "\n",
        "            if success:\n",
        "                print(f\"‚úì Saved: {os.path.basename(result)}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Failed: {result}\")\n",
        "\n",
        "            # Monitor memory between files\n",
        "            print_gpu_stats(\"Between files: \")\n",
        "\n",
        "            # Brief cooldown\n",
        "            if i < len(files_to_process) - 1:\n",
        "                time.sleep(3)\n",
        "\n",
        "    # Final cleanup\n",
        "    mel_cache.clear()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SESSION COMPLETE\")\n",
        "    print_gpu_stats(\"Final: \")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# RUN MAIN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n‚öôÔ∏è  GPU OPTIMIZATION TIPS:\")\n",
        "    print(\"1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\")\n",
        "    print(\"2. Set USE_FLOAT32=True to double memory usage\")\n",
        "    print(\"3. Increase BEAM_SIZE and BEST_OF for more memory use\")\n",
        "    print(\"4. Set PRELOAD_TO_GPU=True to cache audio in GPU\")\n",
        "    print(\"\\n‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    main()\n",
        "\n",
        "# ============================================\n",
        "# EXPERIMENTAL: EXTREME GPU USAGE\n",
        "# ============================================\n",
        "\"\"\"\n",
        "FOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\n",
        "\n",
        "GPU_OPTIONS = {\n",
        "    'PARALLEL_FILES': 3,      # Process 3 files at once\n",
        "    'PRELOAD_TO_GPU': True,   # Cache in GPU\n",
        "    'MEL_CACHE_SIZE': 20,     # Large cache\n",
        "    'USE_FLOAT32': True,      # Double precision (2x memory)\n",
        "    'BEAM_SIZE': 20,          # Very large beam\n",
        "    'BEST_OF': 20,            # Very large sampling\n",
        "    'N_FRAMES': 6000,         # Process huge chunks\n",
        "}\n",
        "\n",
        "This will likely crash but will definitely use all GPU memory!\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l7BK8AlvhOZX",
        "outputId": "0262f5e2-387f-44c7-eb56-7c538a067856"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üì¶ Installing dependencies...\n",
            "Installing openai-whisper...\n",
            "‚úì Whisper installed\n",
            "‚úì PyTorch already installed\n",
            "üöÄ Whisper V3 GPU-Optimized System\n",
            "============================================================\n",
            "GPU Memory Optimization Settings:\n",
            "  PARALLEL_FILES: 1\n",
            "  PRELOAD_TO_GPU: True\n",
            "  MEL_CACHE_SIZE: 10\n",
            "  USE_FLOAT32: False\n",
            "  BEAM_SIZE: 10\n",
            "  BEST_OF: 10\n",
            "  N_FRAMES: 5000\n",
            "============================================================\n",
            "Pre-allocating GPU memory...\n",
            "  After allocation 1: GPU Memory: 1.1GB allocated, 1.1GB reserved / 15.8GB total\n",
            "  After allocation 2: GPU Memory: 2.1GB allocated, 2.1GB reserved / 15.8GB total\n",
            "  After allocation 3: GPU Memory: 3.2GB allocated, 3.2GB reserved / 15.8GB total\n",
            "  After allocation 4: GPU Memory: 4.3GB allocated, 4.3GB reserved / 15.8GB total\n",
            "  After allocation 5: GPU Memory: 5.4GB allocated, 5.4GB reserved / 15.8GB total\n",
            "\n",
            "Loading Whisper large-v3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [01:57<00:00, 26.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After model load: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "Modified n_frames: 1500 ‚Üí 5000\n",
            "\n",
            "‚öôÔ∏è  GPU OPTIMIZATION TIPS:\n",
            "1. Set PARALLEL_FILES=2 to process 2 files at once (risky!)\n",
            "2. Set USE_FLOAT32=True to double memory usage\n",
            "3. Increase BEAM_SIZE and BEST_OF for more memory use\n",
            "4. Set PRELOAD_TO_GPU=True to cache audio in GPU\n",
            "\n",
            "‚ö†Ô∏è  Higher settings may cause out-of-memory crashes!\n",
            "============================================================\n",
            "\n",
            "Files to process: 5\n",
            "\n",
            "üîí SEQUENTIAL MODE: Processing files one at a time\n",
            "\n",
            "[Worker 0] Processing: Call Recording - 19Mar2025 0800 JD.wav\n",
            "[Worker 0] Pre-loading to GPU...\n",
            "[Worker 0] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 1] Processing: Call Recording - 19Mar25 0900 - AJ.wav\n",
            "[Worker 1] Pre-loading to GPU...\n",
            "[Worker 1] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 2] Processing: Call Recording - 19Mar25 1730 - MO.wav\n",
            "[Worker 2] Pre-loading to GPU...\n",
            "[Worker 2] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 3] Processing: Call Recording - 20Mar2025 1200 LN.wav\n",
            "[Worker 3] Pre-loading to GPU...\n",
            "[Worker 3] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "[Worker 4] Processing: Call Recording - 26Mar2025 0830 SA.wav\n",
            "[Worker 4] Pre-loading to GPU...\n",
            "[Worker 4] Error: expected scalar type Float but found Half\n",
            "‚ùå Failed: expected scalar type Float but found Half\n",
            "Between files: GPU Memory: 4.2GB allocated, 10.9GB reserved / 15.8GB total\n",
            "\n",
            "============================================================\n",
            "‚úÖ SESSION COMPLETE\n",
            "Final: GPU Memory: 4.2GB allocated, 5.1GB reserved / 15.8GB total\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFOR MAXIMUM GPU USAGE (15+ GB), modify GPU_OPTIONS to:\\n\\nGPU_OPTIONS = {\\n    'PARALLEL_FILES': 3,      # Process 3 files at once\\n    'PRELOAD_TO_GPU': True,   # Cache in GPU\\n    'MEL_CACHE_SIZE': 20,     # Large cache\\n    'USE_FLOAT32': True,      # Double precision (2x memory)\\n    'BEAM_SIZE': 20,          # Very large beam\\n    'BEST_OF': 20,            # Very large sampling\\n    'N_FRAMES': 6000,         # Process huge chunks\\n}\\n\\nThis will likely crash but will definitely use all GPU memory!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xlTXAAphOPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fg3kk7mXfEeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CHlcQADZ-uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Odc4_SxZUN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Drive File Storage\n"
      ],
      "metadata": {
        "id": "tvVwNwhHXQNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE WORKING TRANSCRIPTION SYSTEM\n",
        "# Based on the approach that was working\n",
        "\n",
        "# ============================================\n",
        "# CELL 1: Complete Setup and Processing\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "# if not os.path.exists('/content/drive'):\n",
        "#    drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MUIMLVU4Wtzj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 2: File Discovery and Status\n",
        "# ============================================\n",
        "\"\"\"\n",
        "Run this to see what files need processing\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths - adjust these to your actual locations\n",
        "INPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/PRUT-Transcriptions/Transcripts\"\n",
        "\n",
        "# Get list of audio files\n",
        "mp4_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.mp4\")))\n",
        "wav_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.wav\")))\n",
        "all_audio_files = mp4_files + wav_files\n",
        "\n",
        "print(f\"\\nüìÅ Found {len(all_audio_files)} audio files:\")\n",
        "for i, f in enumerate(all_audio_files, 1):\n",
        "    print(f\"  {i}. {os.path.basename(f)}\")\n",
        "\n",
        "# Check what's already been transcribed\n",
        "completed_files = []\n",
        "remaining_files = []\n",
        "\n",
        "for audio_file in all_audio_files:\n",
        "    base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
        "    transcript_path = os.path.join(OUTPUT_PATH, f\"{base_name}_transcript.txt\")\n",
        "\n",
        "    if os.path.exists(transcript_path):\n",
        "        completed_files.append(audio_file)\n",
        "    else:\n",
        "        remaining_files.append(audio_file)\n",
        "\n",
        "print(f\"\\nüìä Status:\")\n",
        "print(f\"  ‚úì Completed: {len(completed_files)}\")\n",
        "print(f\"  ‚è≥ Remaining: {len(remaining_files)}\")\n",
        "\n",
        "if remaining_files:\n",
        "    print(f\"\\nüéØ Next file to process: {os.path.basename(remaining_files[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7h4HOEbWvno",
        "outputId": "a36521ba-b00a-444e-9cbf-3d3b48d42780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "üìÅ Found 8 audio files:\n",
            "  1. Call Recording - 13Mar2025 1200 BPA.wav\n",
            "  2. Call Recording - 13Mar25 1130 BK.wav\n",
            "  3. Call Recording - 13Mar25 1300 HB.wav\n",
            "  4. Call Recording - 19Mar2025 0800 JD.wav\n",
            "  5. Call Recording - 19Mar25 0900 - AJ.wav\n",
            "  6. Call Recording - 19Mar25 1730 - MO.wav\n",
            "  7. Call Recording - 20Mar2025 1200 LN.wav\n",
            "  8. Call Recording - 26Mar2025 0830 SA.wav\n",
            "\n",
            "üìä Status:\n",
            "  ‚úì Completed: 0\n",
            "  ‚è≥ Remaining: 8\n",
            "\n",
            "üéØ Next file to process: Call Recording - 13Mar2025 1200 BPA.wav\n"
          ]
        }
      ]
    }
  ]
}